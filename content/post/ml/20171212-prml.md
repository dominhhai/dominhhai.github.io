---
title: "Pattern Recognition and Machine Learning"
slug: ml-prml
date: 2017-12-12T16:36:56+09:00
categories:
- Học Máy
- ML
tags:
- Học Máy
keywords:
- Học Máy
- Pattern Recognition and Machine Learning
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: https://res.cloudinary.com/dominhhai/image/upload/ml/prml.jpg
metaAlignment: center
---
Được coi là sách giáo khoa cho những người làm học máy, cuốn sách này viết về các giải thuật và lý thuyết xây dựng các giải thuật nhận dạng mẫu và học máy. Tuy nhiên lúc mới đọc thì thấy khá khó nhằn nên tôi đã tìm hiểu độ khó các phần đề biết đường mà đọc.
<!--more-->

Độ khó này không phải do tôi nghĩ ra mà là lấy từ trang <a href="http://ibisforest.org/index.php?PRML%2Fcourse" target="_blank"_ rel="noopener noreferrer">trợ giúp sách này của Nhật</a>. Tôi mới đọc hết 1 phần cuốn này và thấy đánh giá ở trang đó có vẻ đúng. Tôi thì ưu tiên đọc phần dễ và vừa trước rồi mới tới các phần nâng cao. Tuy nhiên không phải là không đọc tí nào phần nâng cao mà có đọc 1 chút dù không hiểu gì cũng mặc.

<!--toc-->

# 1. Các cấp độ
| Cấp độ | Giải thích |
|---|---|
| {{< hl-text cyan >}}Sơ cấp{{< /hl-text >}} | Lý thuyết và phương pháp cơ bản phù hợp với người nhập môn. Nên đọc những phần này trước để nắm được nội dung cơ bản. |
| {{< hl-text green >}}Trung cấp{{< /hl-text >}} | Cơ bản về suy luận Bayes và một số nội dung hơi nâng cao một chút. Ngoài ra, còn có một số phương pháp hữu dụng trong một số trường hợp đặc biệt. Phần này phù hợp với bậc học tiến sĩ với nội dung hơi huướng nâng cao. |
| {{< hl-text yellow >}}Nâng cao{{< /hl-text >}} | Nội dung nâng cao bao gồm cả giải thích lý thuyết. Phần này phù hợp với trình độ tiến sĩ, nhà nghiên và các kĩ sư học máy. |

# 2. Các chương
## 2.1. Chương 1: Introduction
| Mục | Tiêu đề |
|---|---|
| {{< hl-text cyan >}}1{{< /hl-text >}} | Introduction |
| {{< hl-text cyan >}}1.1{{< /hl-text >}} | Example: Polynomial Curve Fitting |
| {{< hl-text cyan >}}1.2{{< /hl-text >}} | Probability Theory |
| {{< hl-text cyan >}}1.2.1{{< /hl-text >}} | Probability densities |
| {{< hl-text cyan >}}1.2.2{{< /hl-text >}} | Expectations and covariances |
| {{< hl-text cyan >}}1.2.3{{< /hl-text >}} | Bayesian probabilities |
| {{< hl-text cyan >}}1.2.4{{< /hl-text >}} | The Gaussian distribution |
| {{< hl-text cyan >}}1.2.5{{< /hl-text >}} | Curve fitting re-visited |
| {{< hl-text green >}}1.2.6{{< /hl-text >}} | Bayesian curve fitting |
| {{< hl-text cyan >}}1.3{{< /hl-text >}} | Model Selection |
| {{< hl-text cyan >}}1.4{{< /hl-text >}} | The Curse of Dimensionality |
| {{< hl-text cyan >}}1.5{{< /hl-text >}} | Decision Theory |
| {{< hl-text cyan >}}1.5.1{{< /hl-text >}} | Minimizing the misclassification rate |
| {{< hl-text cyan >}}1.5.2{{< /hl-text >}} | Minimizing the expected loss |
| {{< hl-text yellow >}}1.5.3{{< /hl-text >}} | The reject option |
| {{< hl-text green >}}1.5.4{{< /hl-text >}} | Inference and decision |
| {{< hl-text cyan >}}1.5.5{{< /hl-text >}} | Loss functions for regression |
| {{< hl-text cyan >}}1.6{{< /hl-text >}} | Information Theory |
| {{< hl-text cyan >}}1.6.1{{< /hl-text >}} | Relative entropy and mutual information |

## 2.2. Chương 2: Probability Distributions
| Mục | Tiêu đề |
|---|---|
| {{< hl-text cyan >}}2{{< /hl-text >}} | Probability Distributions |
| {{< hl-text cyan >}}2.1{{< /hl-text >}} | Binary Variables |
| {{< hl-text cyan >}}2.1.1{{< /hl-text >}} | The beta distribution |
| {{< hl-text cyan >}}2.2{{< /hl-text >}} | Multinomial Variables |
| {{< hl-text cyan >}}2.2.1{{< /hl-text >}} | The Dirichlet distribution |
| {{< hl-text cyan >}}2.3{{< /hl-text >}} | The Gaussian Distribution |
| {{< hl-text cyan >}}2.3.1{{< /hl-text >}} | Conditional Gaussian distributions |
| {{< hl-text cyan >}}2.3.2{{< /hl-text >}} | Marginal Gaussian distributions |
| {{< hl-text cyan >}}2.3.3{{< /hl-text >}} | Bayes’ theorem for Gaussian variables |
| {{< hl-text cyan >}}2.3.4{{< /hl-text >}} | Maximum likelihood for the Gaussian |
| {{< hl-text green >}}2.3.5{{< /hl-text >}} | Sequential estimation |
| {{< hl-text green >}}2.3.6{{< /hl-text >}} | Bayesian inference for the Gaussian |
| {{< hl-text green >}}2.3.7{{< /hl-text >}} | Student’s t-distribution |
| {{< hl-text yellow >}}2.3.8{{< /hl-text >}} | Periodic variables |
| {{< hl-text cyan >}}2.3.9{{< /hl-text >}} | Mixtures of Gaussians |
| {{< hl-text cyan >}}2.4{{< /hl-text >}} | The Exponential Family |
| {{< hl-text cyan >}}2.4.1{{< /hl-text >}} | Maximum likelihood and sufficient statistics |
| {{< hl-text green >}}2.4.2{{< /hl-text >}} | Conjugate priors |
| {{< hl-text green >}}2.4.3{{< /hl-text >}} | Noninformative priors |
| {{< hl-text cyan >}}2.5{{< /hl-text >}} | Nonparametric Methods |
| {{< hl-text cyan >}}2.5.1{{< /hl-text >}} | Kernel density estimators |
| {{< hl-text cyan >}}2.5.2{{< /hl-text >}} | Nearest-neighbour methods |

## 2.3. Chương 3: Linear Models for Regression
| Mục | Tiêu đề |
|---|---|
| {{< hl-text cyan >}}3{{< /hl-text >}} | Linear Models for Regression |
| {{< hl-text cyan >}}3.1{{< /hl-text >}} | Linear Basis Function Models |
| {{< hl-text cyan >}}3.1.1{{< /hl-text >}} | Maximum likelihood and least squares |
| {{< hl-text yellow >}}3.1.2{{< /hl-text >}} | Geometry of least squares |
| {{< hl-text green >}}3.1.3{{< /hl-text >}} | Sequential learning |
| {{< hl-text green >}}3.1.4{{< /hl-text >}} | Regularized least squares |
| {{< hl-text yellow >}}3.1.5{{< /hl-text >}} | Multiple outputs |
| {{< hl-text cyan >}}3.2{{< /hl-text >}} | The Bias-Variance Decomposition |
| {{< hl-text green >}}3.3{{< /hl-text >}} | Bayesian Linear Regression |
| {{< hl-text green >}}3.3.1{{< /hl-text >}} | Parameter distribution |
| {{< hl-text green >}}3.3.2{{< /hl-text >}} | Predictive distribution |
| {{< hl-text green >}}3.3.3{{< /hl-text >}} | Equivalent kernel |
| {{< hl-text yellow >}}3.4{{< /hl-text >}} | Bayesian Model Comparison |
| {{< hl-text yellow >}}3.5{{< /hl-text >}} | The Evidence Approximation |
| {{< hl-text yellow >}}3.5.1{{< /hl-text >}} | Evaluation of the evidence function |
| {{< hl-text yellow >}}3.5.2{{< /hl-text >}} | Maximizing the evidence function |
| {{< hl-text yellow >}}3.5.3{{< /hl-text >}} | Effective number of parameters |
| {{< hl-text green >}}3.6{{< /hl-text >}} | Limitations of Fixed Basis Functions |

## 2.4. Chương 4: Linear Models for Classification
| Mục | Tiêu đề |
|---|---|
| {{< hl-text cyan >}}4{{< /hl-text >}} | Linear Models for Regression |
| {{< hl-text cyan >}}4.1{{< /hl-text >}} | Discriminant Functions |
| {{< hl-text cyan >}}4.1.1{{< /hl-text >}} | Two classes |
| {{< hl-text cyan >}}4.1.2{{< /hl-text >}} | Multiple classes |
| {{< hl-text cyan >}}4.1.3{{< /hl-text >}} | Lest squares for classification |
| {{< hl-text cyan >}}4.1.4{{< /hl-text >}} | Fisher's linear discriminant |
| {{< hl-text cyan >}}4.1.5{{< /hl-text >}} | Relation to least squares |
| {{< hl-text cyan >}}4.1.6{{< /hl-text >}} | Fisher's discriminant for multiple classes |
| {{< hl-text cyan >}}4.1.7{{< /hl-text >}} | The perceptron algorithm |
| {{< hl-text cyan >}}4.2{{< /hl-text >}} | Probabilistic Generative Models |
| {{< hl-text cyan >}}4.2.1{{< /hl-text >}} | Continuous inputs |
| {{< hl-text cyan >}}4.2.2{{< /hl-text >}} | Maximum likelihood solution |
| {{< hl-text cyan >}}4.2.3{{< /hl-text >}} | Discrete features |
| {{< hl-text cyan >}}4.2.4{{< /hl-text >}} | Exponential family |
| {{< hl-text cyan >}}4.3{{< /hl-text >}} | Probabilistic Discriminant Models |
| {{< hl-text cyan >}}4.3.1{{< /hl-text >}} | Fixed basis functions |
| {{< hl-text cyan >}}4.3.2{{< /hl-text >}} | Logistic regression |
| {{< hl-text cyan >}}4.3.3{{< /hl-text >}} | Interative reweighted least squares |
| {{< hl-text cyan >}}4.3.4{{< /hl-text >}} | Multiclass logistic regression |
| {{< hl-text yellow >}}4.3.5{{< /hl-text >}} | Probit regression |
| {{< hl-text yellow >}}4.3.6{{< /hl-text >}} | Canonical link functions |
| {{< hl-text green >}}4.4{{< /hl-text >}} | The Laplace Approximation |
| {{< hl-text green >}}4.4.1{{< /hl-text >}} | Model comparison and BIC |
| {{< hl-text green >}}4.5{{< /hl-text >}} | Bayesian Logistic Regression |
| {{< hl-text green >}}4.5.1{{< /hl-text >}} | Laplace approximation |
| {{< hl-text green >}}4.5.2{{< /hl-text >}} | Predictive distribution |

## 2.5. Chương 5: Neural Networks
| Mục | Tiêu đề |
|---|---|
| {{< hl-text cyan >}}5{{< /hl-text >}} | Neural Networks |
| {{< hl-text cyan >}}5.1{{< /hl-text >}} | Feed-forward Networks Functions |
| {{< hl-text cyan >}}5.1.1{{< /hl-text >}} | Weight-space symmetries |
| {{< hl-text cyan >}}5.2{{< /hl-text >}} | Network Training |
| {{< hl-text cyan >}}5.2.1{{< /hl-text >}} | Parameter optimization |
| {{< hl-text cyan >}}5.2.2{{< /hl-text >}} | Local quadratic approximation |
| {{< hl-text cyan >}}5.2.3{{< /hl-text >}} | Use of gradient information |
| {{< hl-text cyan >}}5.2.4{{< /hl-text >}} | Gradient descent optimization |
| {{< hl-text cyan >}}5.3{{< /hl-text >}} | Error Backpropagation |
| {{< hl-text cyan >}}5.3.1{{< /hl-text >}} | Evaluation of error-function derivatives |
| {{< hl-text cyan >}}5.3.2{{< /hl-text >}} | A simple example |
| {{< hl-text cyan >}}5.3.3{{< /hl-text >}} | Efficiency of backpropagation |
| {{< hl-text yellow >}}5.3.4{{< /hl-text >}} | The Jacobian matrix |
| {{< hl-text yellow >}}5.4{{< /hl-text >}} | The Hessian Matrix |
| {{< hl-text yellow >}}5.4.1{{< /hl-text >}} | Diagonal approximation |
| {{< hl-text yellow >}}5.4.2{{< /hl-text >}} | Outer product approximation |
| {{< hl-text yellow >}}5.4.3{{< /hl-text >}} | Inverse Hessian |
| {{< hl-text yellow >}}5.4.4{{< /hl-text >}} | Finite differences |
| {{< hl-text yellow >}}5.4.5{{< /hl-text >}} | Exact evaluation of the Hessian |
| {{< hl-text yellow >}}5.4.6{{< /hl-text >}} | Fast multiplication by the Hessian |
| {{< hl-text green >}}5.5{{< /hl-text >}} | Regularization in Neural Networks |
| {{< hl-text green >}}5.5.1{{< /hl-text >}} | Consistent Gaussian priors |
| {{< hl-text green >}}5.5.2{{< /hl-text >}} | Early stopping |
| {{< hl-text yellow >}}5.5.3{{< /hl-text >}} | Invariances |
| {{< hl-text yellow >}}5.5.4{{< /hl-text >}} | Tangent propagation |
| {{< hl-text yellow >}}5.5.5{{< /hl-text >}} | Training with transformed data |
| {{< hl-text yellow >}}5.5.6{{< /hl-text >}} | Convolutional networks |
| {{< hl-text yellow >}}5.5.7{{< /hl-text >}} | Soft weight sharing |
| {{< hl-text yellow >}}5.6{{< /hl-text >}} | Mixture Density Networks |
| {{< hl-text yellow >}}5.7{{< /hl-text >}} | Bayesian Neural Networks |
| {{< hl-text yellow >}}5.7.1{{< /hl-text >}} | Posterior parameter distribution |
| {{< hl-text yellow >}}5.7.2{{< /hl-text >}} | Hyperparameter optimization |
| {{< hl-text yellow >}}5.7.3{{< /hl-text >}} | Bayesian neural networks for classification |

## 2.6. Chương 6: Kernel Methods
| Mục | Tiêu đề |
|---|---|
| {{< hl-text cyan >}}6{{< /hl-text >}} | Kernel Methods |
| {{< hl-text cyan >}}6.1{{< /hl-text >}} | Dual Representaions |
| {{< hl-text cyan >}}6.3{{< /hl-text >}} | Constructing Kernels |
| {{< hl-text cyan >}}6.3{{< /hl-text >}} | Radial Basis Function Networks |
| {{< hl-text cyan >}}6.3.1{{< /hl-text >}} | Nadaraya-Watson model |
| {{< hl-text yellow >}}6.4{{< /hl-text >}} | Gaussian Processes |
| {{< hl-text yellow >}}6.4.1{{< /hl-text >}} | Linear regression revisited |
| {{< hl-text yellow >}}6.4.2{{< /hl-text >}} | Gaussian processes for regression |
| {{< hl-text yellow >}}6.4.3{{< /hl-text >}} | Learning the hyperparameter |
| {{< hl-text yellow >}}6.4.4{{< /hl-text >}} | Automatic relevance determination |
| {{< hl-text yellow >}}6.4.5{{< /hl-text >}} | Gaussian processes for classification |
| {{< hl-text yellow >}}6.4.6{{< /hl-text >}} | Laplace approximation |
| {{< hl-text yellow >}}6.4.7{{< /hl-text >}} | Connection to neural networks |

## 2.7. Chương 7: Sparse Kernel Machines
| Mục | Tiêu đề |
|---|---|
| {{< hl-text cyan >}}7{{< /hl-text >}} | Sparse Kernel Machines |
| {{< hl-text cyan >}}7.1{{< /hl-text >}} | Maximum Margin Classifiers |
| {{< hl-text cyan >}}7.1.1{{< /hl-text >}} | Overlapping class distributions |
| {{< hl-text green >}}7.1.2{{< /hl-text >}} | Relation to logistic regression |
| {{< hl-text green >}}7.1.3{{< /hl-text >}} | Multiclass SVMs |
| {{< hl-text green >}}7.1.4{{< /hl-text >}} | SVMs for regression |
| {{< hl-text green >}}7.1.5{{< /hl-text >}} | Computational learning theory |
| {{< hl-text yellow >}}7.2{{< /hl-text >}} | Relevance Vector Machines |
| {{< hl-text yellow >}}7.2.1{{< /hl-text >}} | RVM for regression |
| {{< hl-text yellow >}}7.2.2{{< /hl-text >}} | Analysis of sparsity |
| {{< hl-text yellow >}}7.2.3{{< /hl-text >}} | RVM for classification |

## 2.8. Chương 8: Graphical Models
| Mục | Tiêu đề |
|---|---|
| {{< hl-text green >}}8{{< /hl-text >}} | Graphical Models |
| {{< hl-text green >}}8.1{{< /hl-text >}} | Bayesian Networks |
| {{< hl-text green >}}8.1.1{{< /hl-text >}} | Example: Polynomial regression |
| {{< hl-text green >}}8.1.2{{< /hl-text >}} | Generative models |
| {{< hl-text green >}}8.1.3{{< /hl-text >}} | Discrete variables |
| {{< hl-text green >}}8.1.4{{< /hl-text >}} | Linear-Gaussian models |
| {{< hl-text green >}}8.2{{< /hl-text >}} | Conditional Independence |
| {{< hl-text green >}}8.2.1{{< /hl-text >}} | Three example graphs |
| {{< hl-text green >}}8.2.2{{< /hl-text >}} | D-separation |
| {{< hl-text green >}}8.3{{< /hl-text >}} | Markov Random Fields |
| {{< hl-text green >}}8.3.1{{< /hl-text >}} | Conditional independence properties |
| {{< hl-text green >}}8.3.2{{< /hl-text >}} | Factorization properties |
| {{< hl-text green >}}8.3.3{{< /hl-text >}} | Illustration: Image de-noising |
| {{< hl-text green >}}8.3.4{{< /hl-text >}} | Relation to directed graphs |
| {{< hl-text green >}}8.4{{< /hl-text >}} | inference in Graphical Models |
| {{< hl-text green >}}8.4.1{{< /hl-text >}} | Inference on a chain |
| {{< hl-text green >}}8.4.2{{< /hl-text >}} | Trees |
| {{< hl-text green >}}8.4.3{{< /hl-text >}} | Factor graphs |
| {{< hl-text green >}}8.4.4{{< /hl-text >}} | The sum-product algorithm |
| {{< hl-text green >}}8.4.5{{< /hl-text >}} | The max-sum algorithm |
| {{< hl-text yellow >}}8.4.6{{< /hl-text >}} | Exact inference in general graphs |
| {{< hl-text yellow >}}8.4.7{{< /hl-text >}} | Loopy belief propagation |
| {{< hl-text yellow >}}8.4.8{{< /hl-text >}} | Learning the graph structure |

## 2.9. Chương 9: Mixture Models and EM
| Mục | Tiêu đề |
|---|---|
| {{< hl-text cyan >}}9{{< /hl-text >}} | Mixture Models and EM |
| {{< hl-text cyan >}}9.1{{< /hl-text >}} | K-means Clustering |
| {{< hl-text cyan >}}9.1.1{{< /hl-text >}} | Image segmentation and compression |
| {{< hl-text cyan >}}9.2{{< /hl-text >}} | Mixtures of Gaussians |
| {{< hl-text cyan >}}9.2.1{{< /hl-text >}} | Maximum likelihood |
| {{< hl-text cyan >}}9.2.2{{< /hl-text >}} | EM for Gaussian mixtures |
| {{< hl-text cyan >}}9.3{{< /hl-text >}} | An Alternative View of EM |
| {{< hl-text cyan >}}9.3.1{{< /hl-text >}} | Gaussian mixtures revisited |
| {{< hl-text cyan >}}9.3.2{{< /hl-text >}} | Relation to K-means |
| {{< hl-text yellow >}}9.3.3{{< /hl-text >}} | Mixtures of Bernoulli distributions |
| {{< hl-text yellow >}}9.3.4{{< /hl-text >}} | EM for Bayesian linear regression |
| {{< hl-text green >}}9.4{{< /hl-text >}} | The EM Algorithm in General |

## 2.10. Chương 10: Approximate Inference
| Mục | Tiêu đề |
|---|---|
| {{< hl-text green >}}10{{< /hl-text >}} | Approximate Inference |
| {{< hl-text green >}}10.1{{< /hl-text >}} | Variational Inference |
| {{< hl-text green >}}10.1.1{{< /hl-text >}} | Factorized distributions |
| {{< hl-text green >}}10.1.2{{< /hl-text >}} | Properties of factorized approximations |
| {{< hl-text green >}}10.1.3{{< /hl-text >}} | Example: The univariate Gaussian |
| {{< hl-text green >}}10.1.4{{< /hl-text >}} | Model comparison |
| {{< hl-text green >}}10.2{{< /hl-text >}} | Illustration: Variational Mixture of Gaussians |
| {{< hl-text green >}}10.2.1{{< /hl-text >}} | Variational distribution |
| {{< hl-text green >}}10.2.2{{< /hl-text >}} | Variational lower bound |
| {{< hl-text green >}}10.2.3{{< /hl-text >}} | Predictive density |
| {{< hl-text yellow >}}10.2.4{{< /hl-text >}} | Determining the number of components |
| {{< hl-text yellow >}}10.2.5{{< /hl-text >}} | Induced factorizations |
| {{< hl-text yellow >}}10.3{{< /hl-text >}} | Variational Linear Regression |
| {{< hl-text yellow >}}10.3.1{{< /hl-text >}} | Variational distribution |
| {{< hl-text yellow >}}10.3.2{{< /hl-text >}} | Predictive distribution |
| {{< hl-text yellow >}}10.3.3{{< /hl-text >}} | Lower bound |
| {{< hl-text yellow >}}10.4{{< /hl-text >}} | Exponential Family Distributions |
| {{< hl-text yellow >}}10.4.1{{< /hl-text >}} | Variational message passing |
| {{< hl-text yellow >}}10.5{{< /hl-text >}} | Local Variational Methods |
| {{< hl-text yellow >}}10.6{{< /hl-text >}} | Variational Logistic Regression |
| {{< hl-text yellow >}}10.6.1{{< /hl-text >}} | Variational posterior distribution |
| {{< hl-text yellow >}}10.6.2{{< /hl-text >}} | Optimizing the variational parameters |
| {{< hl-text yellow >}}10.6.3{{< /hl-text >}} | Inference of hyperparameters |
| {{< hl-text yellow >}}10.7{{< /hl-text >}} | Expectation Propagation |
| {{< hl-text yellow >}}10.7.1{{< /hl-text >}} | Example: The clutter problem |
| {{< hl-text yellow >}}10.7.2{{< /hl-text >}} | Expectation propagation of graphs |

## 2.11. Chương 11: Sampling Methods
| Mục | Tiêu đề |
|---|---|
| {{< hl-text green >}}11{{< /hl-text >}} | Sampling Methods |
| {{< hl-text green >}}11.1{{< /hl-text >}} | Basis Sampling Algorithms |
| {{< hl-text green >}}11.1.1{{< /hl-text >}} | Standard distributions |
| {{< hl-text green >}}11.1.2{{< /hl-text >}} | Rejection sampling |
| {{< hl-text yellow >}}11.1.3{{< /hl-text >}} | Adaptive rejection sampling |
| {{< hl-text yellow >}}11.1.4{{< /hl-text >}} | Importance sampling |
| {{< hl-text yellow >}}11.1.5{{< /hl-text >}} | Sampling-importance-resampling |
| {{< hl-text yellow >}}11.1.6{{< /hl-text >}} | Sampling and EM algorithm |
| {{< hl-text green >}}11.2{{< /hl-text >}} | Markov Chain Monte Carlo |
| {{< hl-text green >}}11.2.1{{< /hl-text >}} | Markov chains |
| {{< hl-text green >}}11.2.2{{< /hl-text >}} | The Metropolis-Hastings algorithm |
| {{< hl-text green >}}11.3{{< /hl-text >}} | Gibbs Sampling |
| {{< hl-text yellow >}}11.4{{< /hl-text >}} | Slice Sampling |
| {{< hl-text yellow >}}11.5{{< /hl-text >}} | The Hybrid Monte Carlo Algorithm |
| {{< hl-text yellow >}}11.5.1{{< /hl-text >}} | Dynamical systems |
| {{< hl-text yellow >}}11.5.2{{< /hl-text >}} | Hybrid Monte Carlo |
| {{< hl-text yellow >}}11.6{{< /hl-text >}} | Estimating the Partition Function |

## 2.12. Chương 12: Continuous Latent Variables
| Mục | Tiêu đề |
|---|---|
| {{< hl-text cyan >}}12{{< /hl-text >}} | Continuous Latent Variables |
| {{< hl-text cyan >}}12.1{{< /hl-text >}} | Principal Component Analysis |
| {{< hl-text cyan >}}12.1.1{{< /hl-text >}} | Maximum variance formulation |
| {{< hl-text cyan >}}12.1.2{{< /hl-text >}} | Minimum-error formulation |
| {{< hl-text cyan >}}12.1.3{{< /hl-text >}} | Applications of peA |
| {{< hl-text cyan >}}12.1.4{{< /hl-text >}} | PCA for high-dimensional data |
| {{< hl-text yellow >}}12.2{{< /hl-text >}} | Probabilistic p e A |
| {{< hl-text yellow >}}12.2.1{{< /hl-text >}} | Maximum likelihood peA |
| {{< hl-text yellow >}}12.2.2{{< /hl-text >}} | EM algorithm for peA |
| {{< hl-text yellow >}}12.2.3{{< /hl-text >}} | Bayesian peA |
| {{< hl-text yellow >}}12.2.4{{< /hl-text >}} | Factor analysis |
| {{< hl-text green >}}12.3{{< /hl-text >}} | Kernel PCA |
| {{< hl-text yellow >}}12.4{{< /hl-text >}} | Nonliear Latent Variable Models |
| {{< hl-text yellow >}}12.4.1{{< /hl-text >}} | Independent component analysis |
| {{< hl-text yellow >}}12.4.2{{< /hl-text >}} | Autoassociative neural networks |
| {{< hl-text yellow >}}12.4.3{{< /hl-text >}} | Modelling nonlinear manifolds |

## 2.13. Chương 13: Sequential Data
| Mục | Tiêu đề |
|---|---|
| {{< hl-text green >}}13{{< /hl-text >}} | Sequential Data |
| {{< hl-text green >}}13.1{{< /hl-text >}} | Markov Models |
| {{< hl-text green >}}13.2{{< /hl-text >}} | Hidden Markov Models |
| {{< hl-text green >}}13.2.1{{< /hl-text >}} | Maximum likelihood for the HMM |
| {{< hl-text green >}}13.2.2{{< /hl-text >}} | The forward-backward algorithm |
| {{< hl-text yellow >}}13.2.3{{< /hl-text >}} | The sum-product algorithm for the HMM |
| {{< hl-text yellow >}}13.2.4{{< /hl-text >}} | Scaling factors |
| {{< hl-text green >}}13.2.5{{< /hl-text >}} | The Viterbi algorithm |
| {{< hl-text yellow >}}13.2.6{{< /hl-text >}} | Extensions of the hidden Markov model |
| {{< hl-text green >}}13.3{{< /hl-text >}} | Linear Dynamical Systems |
| {{< hl-text green >}}13.3.1{{< /hl-text >}} | Inference in LDS |
| {{< hl-text green >}}13.3.2{{< /hl-text >}} | Learning in LDS |
| {{< hl-text yellow >}}13.3.3{{< /hl-text >}} | Extensions of LDS |
| {{< hl-text yellow >}}13.3.4{{< /hl-text >}} | Particle filters |

## 2.14. Chương 14: Combining Models
| Mục | Tiêu đề |
|---|---|
| {{< hl-text green >}}14{{< /hl-text >}} | Combining Models |
| {{< hl-text green >}}14.1{{< /hl-text >}} | Bayesian Model Averaging |
| {{< hl-text green >}}14.2{{< /hl-text >}} | Committees |
| {{< hl-text green >}}14.3{{< /hl-text >}} | Boosting |
| {{< hl-text green >}}14.3.1{{< /hl-text >}} | Minimizing exponential error |
| {{< hl-text green >}}14.3.2{{< /hl-text >}} | Error functions for boosting |
| {{< hl-text cyan >}}14.4{{< /hl-text >}} | Tree-based Models |
| {{< hl-text yellow >}}14.5{{< /hl-text >}} | Conditional Mixture Models |
| {{< hl-text yellow >}}14.5.1{{< /hl-text >}} | Mixtures of linear regression models |
| {{< hl-text yellow >}}14.5.2{{< /hl-text >}} | Mixtures of logistic models |
| {{< hl-text yellow >}}14.5.3{{< /hl-text >}} | Mixtures of experts |
