---
title: "Pattern Recognition and Machine Learning"
slug: ml-prml
date: 2017-12-12T16:36:56+09:00
categories:
- Machine Learning
- ML
tags:
- Machine Learning
keywords:
- Pattern Recognition and Machine Learning
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: https://res.cloudinary.com/dominhhai/image/upload/ml/prml.jpg
metaAlignment: center
---
This book is known as the **textbook** for machine learning learners. It covers various algorithm and the theory underline. It's hard to learn too! So, I have to find the complexity of each part in order to study more productivity.
<!--more-->

I found the guideline and complexity reference from <a href="http://ibisforest.org/index.php?PRML%2Fcourse" target="_blank"_ rel="noopener noreferrer">this Japanese page</a>. So, I just translate & copy them to here. I have read a small parts of this book, these guideline is fair ok IMO.

<!--toc-->

# 1. Levels
| Level | Content |
|---|---|
| {{< hl-text cyan >}}Basic{{< /hl-text >}} | The basic theory and methods for beginers. These part should be read first. |
| {{< hl-text green >}}Intermediate{{< /hl-text >}} | Basic Bayes inference and a bit adavanced contents. Beside tha, it contents some useful methods for special cases also. These parts target are doctoral students. |
| {{< hl-text yellow >}}Advance{{< /hl-text >}} | About the advanced contents with deep theory underline. So, these will be fittable for doctoral studiers, researcher and machine learning's engineers. |

# 2. Chapters
## 2.1. Chapter 1: Introduction
| Index | Title |
|---|---|
| {{< hl-text cyan >}}1{{< /hl-text >}} | Introduction |
| {{< hl-text cyan >}}1.1{{< /hl-text >}} | Example: Polynomial Curve Fitting |
| {{< hl-text cyan >}}1.2{{< /hl-text >}} | Probability Theory |
| {{< hl-text cyan >}}1.2.1{{< /hl-text >}} | Probability densities |
| {{< hl-text cyan >}}1.2.2{{< /hl-text >}} | Expectations and covariances |
| {{< hl-text cyan >}}1.2.3{{< /hl-text >}} | Bayesian probabilities |
| {{< hl-text cyan >}}1.2.4{{< /hl-text >}} | The Gaussian distribution |
| {{< hl-text cyan >}}1.2.5{{< /hl-text >}} | Curve fitting re-visited |
| {{< hl-text green >}}1.2.6{{< /hl-text >}} | Bayesian curve fitting |
| {{< hl-text cyan >}}1.3{{< /hl-text >}} | Model Selection |
| {{< hl-text cyan >}}1.4{{< /hl-text >}} | The Curse of Dimensionality |
| {{< hl-text cyan >}}1.5{{< /hl-text >}} | Decision Theory |
| {{< hl-text cyan >}}1.5.1{{< /hl-text >}} | Minimizing the misclassification rate |
| {{< hl-text cyan >}}1.5.2{{< /hl-text >}} | Minimizing the expected loss |
| {{< hl-text yellow >}}1.5.3{{< /hl-text >}} | The reject option |
| {{< hl-text green >}}1.5.4{{< /hl-text >}} | Inference and decision |
| {{< hl-text cyan >}}1.5.5{{< /hl-text >}} | Loss functions for regression |
| {{< hl-text cyan >}}1.6{{< /hl-text >}} | Information Theory |
| {{< hl-text cyan >}}1.6.1{{< /hl-text >}} | Relative entropy and mutual information |

## 2.2. Chapter 2: Probability Distributions
| Index | Title |
|---|---|
| {{< hl-text cyan >}}2{{< /hl-text >}} | Probability Distributions |
| {{< hl-text cyan >}}2.1{{< /hl-text >}} | Binary Variables |
| {{< hl-text cyan >}}2.1.1{{< /hl-text >}} | The beta distribution |
| {{< hl-text cyan >}}2.2{{< /hl-text >}} | Multinomial Variables |
| {{< hl-text cyan >}}2.2.1{{< /hl-text >}} | The Dirichlet distribution |
| {{< hl-text cyan >}}2.3{{< /hl-text >}} | The Gaussian Distribution |
| {{< hl-text cyan >}}2.3.1{{< /hl-text >}} | Conditional Gaussian distributions |
| {{< hl-text cyan >}}2.3.2{{< /hl-text >}} | Marginal Gaussian distributions |
| {{< hl-text cyan >}}2.3.3{{< /hl-text >}} | Bayes’ theorem for Gaussian variables |
| {{< hl-text cyan >}}2.3.4{{< /hl-text >}} | Maximum likelihood for the Gaussian |
| {{< hl-text green >}}2.3.5{{< /hl-text >}} | Sequential estimation |
| {{< hl-text green >}}2.3.6{{< /hl-text >}} | Bayesian inference for the Gaussian |
| {{< hl-text green >}}2.3.7{{< /hl-text >}} | Student’s t-distribution |
| {{< hl-text yellow >}}2.3.8{{< /hl-text >}} | Periodic variables |
| {{< hl-text cyan >}}2.3.9{{< /hl-text >}} | Mixtures of Gaussians |
| {{< hl-text cyan >}}2.4{{< /hl-text >}} | The Exponential Family |
| {{< hl-text cyan >}}2.4.1{{< /hl-text >}} | Maximum likelihood and sufficient statistics |
| {{< hl-text green >}}2.4.2{{< /hl-text >}} | Conjugate priors |
| {{< hl-text green >}}2.4.3{{< /hl-text >}} | Noninformative priors |
| {{< hl-text cyan >}}2.5{{< /hl-text >}} | Nonparametric Methods |
| {{< hl-text cyan >}}2.5.1{{< /hl-text >}} | Kernel density estimators |
| {{< hl-text cyan >}}2.5.2{{< /hl-text >}} | Nearest-neighbour methods |

## 2.3. Chapter 3: Linear Models for Regression
| Index | Title |
|---|---|
| {{< hl-text cyan >}}3{{< /hl-text >}} | Linear Models for Regression |
| {{< hl-text cyan >}}3.1{{< /hl-text >}} | Linear Basis Function Models |
| {{< hl-text cyan >}}3.1.1{{< /hl-text >}} | Maximum likelihood and least squares |
| {{< hl-text yellow >}}3.1.2{{< /hl-text >}} | Geometry of least squares |
| {{< hl-text green >}}3.1.3{{< /hl-text >}} | Sequential learning |
| {{< hl-text green >}}3.1.4{{< /hl-text >}} | Regularized least squares |
| {{< hl-text yellow >}}3.1.5{{< /hl-text >}} | Multiple outputs |
| {{< hl-text cyan >}}3.2{{< /hl-text >}} | The Bias-Variance Decomposition |
| {{< hl-text green >}}3.3{{< /hl-text >}} | Bayesian Linear Regression |
| {{< hl-text green >}}3.3.1{{< /hl-text >}} | Parameter distribution |
| {{< hl-text green >}}3.3.2{{< /hl-text >}} | Predictive distribution |
| {{< hl-text green >}}3.3.3{{< /hl-text >}} | Equivalent kernel |
| {{< hl-text yellow >}}3.4{{< /hl-text >}} | Bayesian Model Comparison |
| {{< hl-text yellow >}}3.5{{< /hl-text >}} | The Evidence Approximation |
| {{< hl-text yellow >}}3.5.1{{< /hl-text >}} | Evaluation of the evidence function |
| {{< hl-text yellow >}}3.5.2{{< /hl-text >}} | Maximizing the evidence function |
| {{< hl-text yellow >}}3.5.3{{< /hl-text >}} | Effective number of parameters |
| {{< hl-text green >}}3.6{{< /hl-text >}} | Limitations of Fixed Basis Functions |

## 2.4. Chapter 4: Linear Models for Classification
| Index | Title |
|---|---|
| {{< hl-text cyan >}}3{{< /hl-text >}} | Linear Models for Regression |
| {{< hl-text cyan >}}3.1{{< /hl-text >}} | Linear Basis Function Models |
| {{< hl-text cyan >}}3.1.1{{< /hl-text >}} | Maximum likelihood and least squares |
| {{< hl-text yellow >}}3.1.2{{< /hl-text >}} | Geometry of least squares |
| {{< hl-text green >}}3.1.3{{< /hl-text >}} | Sequential learning |
| {{< hl-text green >}}3.1.4{{< /hl-text >}} | Regularized least squares |
| {{< hl-text yellow >}}3.1.5{{< /hl-text >}} | Multiple outputs |
| {{< hl-text cyan >}}3.2{{< /hl-text >}} | The Bias-Variance Decomposition |
| {{< hl-text green >}}3.3{{< /hl-text >}} | Bayesian Linear Regression |
| {{< hl-text green >}}3.3.1{{< /hl-text >}} | Parameter distribution |
| {{< hl-text green >}}3.3.2{{< /hl-text >}} | Predictive distribution |
| {{< hl-text green >}}3.3.3{{< /hl-text >}} | Equivalent kernel |
| {{< hl-text yellow >}}3.4{{< /hl-text >}} | Bayesian Model Comparison |
| {{< hl-text yellow >}}3.5{{< /hl-text >}} | The Evidence Approximation |
| {{< hl-text yellow >}}3.5.1{{< /hl-text >}} | Evaluation of the evidence function |
| {{< hl-text yellow >}}3.5.2{{< /hl-text >}} | Maximizing the evidence function |
| {{< hl-text yellow >}}3.5.3{{< /hl-text >}} | Effective number of parameters |
| {{< hl-text green >}}3.6{{< /hl-text >}} | Limitations of Fixed Basis Functions |

## 2.5. Chapter 5: Neural Networks
| Index | Title |
|---|---|
| {{< hl-text cyan >}}5{{< /hl-text >}} | Neural Networks |
| {{< hl-text cyan >}}5.1{{< /hl-text >}} | Feed-forward Networks Functions |
| {{< hl-text cyan >}}5.1.1{{< /hl-text >}} | Weight-space symmetries |
| {{< hl-text cyan >}}5.2{{< /hl-text >}} | Network Training |
| {{< hl-text cyan >}}5.2.1{{< /hl-text >}} | Parameter optimization |
| {{< hl-text cyan >}}5.2.2{{< /hl-text >}} | Local quadratic approximation |
| {{< hl-text cyan >}}5.2.3{{< /hl-text >}} | Use of gradient information |
| {{< hl-text cyan >}}5.2.4{{< /hl-text >}} | Gradient descent optimization |
| {{< hl-text cyan >}}5.3{{< /hl-text >}} | Error Backpropagation |
| {{< hl-text cyan >}}5.3.1{{< /hl-text >}} | Evaluation of error-function derivatives |
| {{< hl-text cyan >}}5.3.2{{< /hl-text >}} | A simple example |
| {{< hl-text cyan >}}5.3.3{{< /hl-text >}} | Efficiency of backpropagation |
| {{< hl-text yellow >}}5.3.4{{< /hl-text >}} | The Jacobian matrix |
| {{< hl-text yellow >}}5.4{{< /hl-text >}} | The Hessian Matrix |
| {{< hl-text yellow >}}5.4.1{{< /hl-text >}} | Diagonal approximation |
| {{< hl-text yellow >}}5.4.2{{< /hl-text >}} | Outer product approximation |
| {{< hl-text yellow >}}5.4.3{{< /hl-text >}} | Inverse Hessian |
| {{< hl-text yellow >}}5.4.4{{< /hl-text >}} | Finite differences |
| {{< hl-text yellow >}}5.4.5{{< /hl-text >}} | Exact evaluation of the Hessian |
| {{< hl-text yellow >}}5.4.6{{< /hl-text >}} | Fast multiplication by the Hessian |
| {{< hl-text green >}}5.5{{< /hl-text >}} | Regularization in Neural Networks |
| {{< hl-text green >}}5.5.1{{< /hl-text >}} | Consistent Gaussian priors |
| {{< hl-text green >}}5.5.2{{< /hl-text >}} | Early stopping |
| {{< hl-text yellow >}}5.5.3{{< /hl-text >}} | Invariances |
| {{< hl-text yellow >}}5.5.4{{< /hl-text >}} | Tangent propagation |
| {{< hl-text yellow >}}5.5.5{{< /hl-text >}} | Training with transformed data |
| {{< hl-text yellow >}}5.5.6{{< /hl-text >}} | Convolutional networks |
| {{< hl-text yellow >}}5.5.7{{< /hl-text >}} | Soft weight sharing |
| {{< hl-text yellow >}}5.6{{< /hl-text >}} | Mixture Density Networks |
| {{< hl-text yellow >}}5.7{{< /hl-text >}} | Bayesian Neural Networks |
| {{< hl-text yellow >}}5.7.1{{< /hl-text >}} | Posterior parameter distribution |
| {{< hl-text yellow >}}5.7.2{{< /hl-text >}} | Hyperparameter optimization |
| {{< hl-text yellow >}}5.7.3{{< /hl-text >}} | Bayesian neural networks for classification |

## 2.6. Chapter 6: Kernel Methods
| Index | Title |
|---|---|
| {{< hl-text cyan >}}6{{< /hl-text >}} | Kernel Methods |
| {{< hl-text cyan >}}6.1{{< /hl-text >}} | Dual Representaions |
| {{< hl-text cyan >}}6.3{{< /hl-text >}} | Constructing Kernels |
| {{< hl-text cyan >}}6.3{{< /hl-text >}} | Radial Basis Function Networks |
| {{< hl-text cyan >}}6.3.1{{< /hl-text >}} | Nadaraya-Watson model |
| {{< hl-text yellow >}}6.4{{< /hl-text >}} | Gaussian Processes |
| {{< hl-text yellow >}}6.4.1{{< /hl-text >}} | Linear regression revisited |
| {{< hl-text yellow >}}6.4.2{{< /hl-text >}} | Gaussian processes for regression |
| {{< hl-text yellow >}}6.4.3{{< /hl-text >}} | Learning the hyperparameter |
| {{< hl-text yellow >}}6.4.4{{< /hl-text >}} | Automatic relevance determination |
| {{< hl-text yellow >}}6.4.5{{< /hl-text >}} | Gaussian processes for classification |
| {{< hl-text yellow >}}6.4.6{{< /hl-text >}} | Laplace approximation |
| {{< hl-text yellow >}}6.4.7{{< /hl-text >}} | Connection to neural networks |

## 2.7. Chapter 7: Sparse Kernel Machines
| Index | Title |
|---|---|
| {{< hl-text cyan >}}7{{< /hl-text >}} | Sparse Kernel Machines |
| {{< hl-text cyan >}}7.1{{< /hl-text >}} | Maximum Margin Classifiers |
| {{< hl-text cyan >}}7.1.1{{< /hl-text >}} | Overlapping class distributions |
| {{< hl-text green >}}7.1.2{{< /hl-text >}} | Relation to logistic regression |
| {{< hl-text green >}}7.1.3{{< /hl-text >}} | Multiclass SVMs |
| {{< hl-text green >}}7.1.4{{< /hl-text >}} | SVMs for regression |
| {{< hl-text green >}}7.1.5{{< /hl-text >}} | Computational learning theory |
| {{< hl-text yellow >}}7.2{{< /hl-text >}} | Relevance Vector Machines |
| {{< hl-text yellow >}}7.2.1{{< /hl-text >}} | RVM for regression |
| {{< hl-text yellow >}}7.2.2{{< /hl-text >}} | Analysis of sparsity |
| {{< hl-text yellow >}}7.2.3{{< /hl-text >}} | RVM for classification |

## 2.8. Chapter 8: Graphical Models
| Index | Title |
|---|---|
| {{< hl-text green >}}8{{< /hl-text >}} | Graphical Models |
| {{< hl-text green >}}8.1{{< /hl-text >}} | Bayesian Networks |
| {{< hl-text green >}}8.1.1{{< /hl-text >}} | Example: Polynomial regression |
| {{< hl-text green >}}8.1.2{{< /hl-text >}} | Generative models |
| {{< hl-text green >}}8.1.3{{< /hl-text >}} | Discrete variables |
| {{< hl-text green >}}8.1.4{{< /hl-text >}} | Linear-Gaussian models |
| {{< hl-text green >}}8.2{{< /hl-text >}} | Conditional Independence |
| {{< hl-text green >}}8.2.1{{< /hl-text >}} | Three example graphs |
| {{< hl-text green >}}8.2.2{{< /hl-text >}} | D-separation |
| {{< hl-text green >}}8.3{{< /hl-text >}} | Markov Random Fields |
| {{< hl-text green >}}8.3.1{{< /hl-text >}} | Conditional independence properties |
| {{< hl-text green >}}8.3.2{{< /hl-text >}} | Factorization properties |
| {{< hl-text green >}}8.3.3{{< /hl-text >}} | Illustration: Image de-noising |
| {{< hl-text green >}}8.3.4{{< /hl-text >}} | Relation to directed graphs |
| {{< hl-text green >}}8.4{{< /hl-text >}} | inference in Graphical Models |
| {{< hl-text green >}}8.4.1{{< /hl-text >}} | Inference on a chain |
| {{< hl-text green >}}8.4.2{{< /hl-text >}} | Trees |
| {{< hl-text green >}}8.4.3{{< /hl-text >}} | Factor graphs |
| {{< hl-text green >}}8.4.4{{< /hl-text >}} | The sum-product algorithm |
| {{< hl-text green >}}8.4.5{{< /hl-text >}} | The max-sum algorithm |
| {{< hl-text yellow >}}8.4.6{{< /hl-text >}} | Exact inference in general graphs |
| {{< hl-text yellow >}}8.4.7{{< /hl-text >}} | Loopy belief propagation |
| {{< hl-text yellow >}}8.4.8{{< /hl-text >}} | Learning the graph structure |

## 2.9. Chapter 9: Mixture Models and EM
| Index | Title |
|---|---|
| {{< hl-text cyan >}}9{{< /hl-text >}} | Mixture Models and EM |
| {{< hl-text cyan >}}9.1{{< /hl-text >}} | K-means Clustering |
| {{< hl-text cyan >}}9.1.1{{< /hl-text >}} | Image segmentation and compression |
| {{< hl-text cyan >}}9.2{{< /hl-text >}} | Mixtures of Gaussians |
| {{< hl-text cyan >}}9.2.1{{< /hl-text >}} | Maximum likelihood |
| {{< hl-text cyan >}}9.2.2{{< /hl-text >}} | EM for Gaussian mixtures |
| {{< hl-text cyan >}}9.3{{< /hl-text >}} | An Alternative View of EM |
| {{< hl-text cyan >}}9.3.1{{< /hl-text >}} | Gaussian mixtures revisited |
| {{< hl-text cyan >}}9.3.2{{< /hl-text >}} | Relation to K-means |
| {{< hl-text yellow >}}9.3.3{{< /hl-text >}} | Mixtures of Bernoulli distributions |
| {{< hl-text yellow >}}9.3.4{{< /hl-text >}} | EM for Bayesian linear regression |
| {{< hl-text green >}}9.4{{< /hl-text >}} | The EM Algorithm in General |

## 2.10. Chapter 10: Approximate Inference
| Index | Title |
|---|---|
| {{< hl-text green >}}10{{< /hl-text >}} | Approximate Inference |
| {{< hl-text green >}}10.1{{< /hl-text >}} | Variational Inference |
| {{< hl-text green >}}10.1.1{{< /hl-text >}} | Factorized distributions |
| {{< hl-text green >}}10.1.2{{< /hl-text >}} | Properties of factorized approximations |
| {{< hl-text green >}}10.1.3{{< /hl-text >}} | Example: The univariate Gaussian |
| {{< hl-text green >}}10.1.4{{< /hl-text >}} | Model comparison |
| {{< hl-text green >}}10.2{{< /hl-text >}} | Illustration: Variational Mixture of Gaussians |
| {{< hl-text green >}}10.2.1{{< /hl-text >}} | Variational distribution |
| {{< hl-text green >}}10.2.2{{< /hl-text >}} | Variational lower bound |
| {{< hl-text green >}}10.2.3{{< /hl-text >}} | Predictive density |
| {{< hl-text yellow >}}10.2.4{{< /hl-text >}} | Determining the number of components |
| {{< hl-text yellow >}}10.2.5{{< /hl-text >}} | Induced factorizations |
| {{< hl-text yellow >}}10.3{{< /hl-text >}} | Variational Linear Regression |
| {{< hl-text yellow >}}10.3.1{{< /hl-text >}} | Variational distribution |
| {{< hl-text yellow >}}10.3.2{{< /hl-text >}} | Predictive distribution |
| {{< hl-text yellow >}}10.3.3{{< /hl-text >}} | Lower bound |
| {{< hl-text yellow >}}10.4{{< /hl-text >}} | Exponential Family Distributions |
| {{< hl-text yellow >}}10.4.1{{< /hl-text >}} | Variational message passing |
| {{< hl-text yellow >}}10.5{{< /hl-text >}} | Local Variational Methods |
| {{< hl-text yellow >}}10.6{{< /hl-text >}} | Variational Logistic Regression |
| {{< hl-text yellow >}}10.6.1{{< /hl-text >}} | Variational posterior distribution |
| {{< hl-text yellow >}}10.6.2{{< /hl-text >}} | Optimizing the variational parameters |
| {{< hl-text yellow >}}10.6.3{{< /hl-text >}} | Inference of hyperparameters |
| {{< hl-text yellow >}}10.7{{< /hl-text >}} | Expectation Propagation |
| {{< hl-text yellow >}}10.7.1{{< /hl-text >}} | Example: The clutter problem |
| {{< hl-text yellow >}}10.7.2{{< /hl-text >}} | Expectation propagation of graphs |

## 2.11. Chapter 11: Sampling Methods
| Index | Title |
|---|---|
| {{< hl-text green >}}11{{< /hl-text >}} | Sampling Methods |
| {{< hl-text green >}}11.1{{< /hl-text >}} | Basis Sampling Algorithms |
| {{< hl-text green >}}11.1.1{{< /hl-text >}} | Standard distributions |
| {{< hl-text green >}}11.1.2{{< /hl-text >}} | Rejection sampling |
| {{< hl-text yellow >}}11.1.3{{< /hl-text >}} | Adaptive rejection sampling |
| {{< hl-text yellow >}}11.1.4{{< /hl-text >}} | Importance sampling |
| {{< hl-text yellow >}}11.1.5{{< /hl-text >}} | Sampling-importance-resampling |
| {{< hl-text yellow >}}11.1.6{{< /hl-text >}} | Sampling and EM algorithm |
| {{< hl-text green >}}11.2{{< /hl-text >}} | Markov Chain Monte Carlo |
| {{< hl-text green >}}11.2.1{{< /hl-text >}} | Markov chains |
| {{< hl-text green >}}11.2.2{{< /hl-text >}} | The Metropolis-Hastings algorithm |
| {{< hl-text green >}}11.3{{< /hl-text >}} | Gibbs Sampling |
| {{< hl-text yellow >}}11.4{{< /hl-text >}} | Slice Sampling |
| {{< hl-text yellow >}}11.5{{< /hl-text >}} | The Hybrid Monte Carlo Algorithm |
| {{< hl-text yellow >}}11.5.1{{< /hl-text >}} | Dynamical systems |
| {{< hl-text yellow >}}11.5.2{{< /hl-text >}} | Hybrid Monte Carlo |
| {{< hl-text yellow >}}11.6{{< /hl-text >}} | Estimating the Partition Function |

## 2.12. Chapter 12: Continuous Latent Variables
| Index | Title |
|---|---|
| {{< hl-text cyan >}}12{{< /hl-text >}} | Continuous Latent Variables |
| {{< hl-text cyan >}}12.1{{< /hl-text >}} | Principal Component Analysis |
| {{< hl-text cyan >}}12.1.1{{< /hl-text >}} | Maximum variance formulation |
| {{< hl-text cyan >}}12.1.2{{< /hl-text >}} | Minimum-error formulation |
| {{< hl-text cyan >}}12.1.3{{< /hl-text >}} | Applications of peA |
| {{< hl-text cyan >}}12.1.4{{< /hl-text >}} | PCA for high-dimensional data |
| {{< hl-text yellow >}}12.2{{< /hl-text >}} | Probabilistic p e A |
| {{< hl-text yellow >}}12.2.1{{< /hl-text >}} | Maximum likelihood peA |
| {{< hl-text yellow >}}12.2.2{{< /hl-text >}} | EM algorithm for peA |
| {{< hl-text yellow >}}12.2.3{{< /hl-text >}} | Bayesian peA |
| {{< hl-text yellow >}}12.2.4{{< /hl-text >}} | Factor analysis |
| {{< hl-text green >}}12.3{{< /hl-text >}} | Kernel PCA |
| {{< hl-text yellow >}}12.4{{< /hl-text >}} | Nonliear Latent Variable Models |
| {{< hl-text yellow >}}12.4.1{{< /hl-text >}} | Independent component analysis |
| {{< hl-text yellow >}}12.4.2{{< /hl-text >}} | Autoassociative neural networks |
| {{< hl-text yellow >}}12.4.3{{< /hl-text >}} | Modelling nonlinear manifolds |

## 2.13. Chapter 13: Sequential Data
| Index | Title |
|---|---|
| {{< hl-text green >}}13{{< /hl-text >}} | Sequential Data |
| {{< hl-text green >}}13.1{{< /hl-text >}} | Markov Models |
| {{< hl-text green >}}13.2{{< /hl-text >}} | Hidden Markov Models |
| {{< hl-text green >}}13.2.1{{< /hl-text >}} | Maximum likelihood for the HMM |
| {{< hl-text green >}}13.2.2{{< /hl-text >}} | The forward-backward algorithm |
| {{< hl-text yellow >}}13.2.3{{< /hl-text >}} | The sum-product algorithm for the HMM |
| {{< hl-text yellow >}}13.2.4{{< /hl-text >}} | Scaling factors |
| {{< hl-text green >}}13.2.5{{< /hl-text >}} | The Viterbi algorithm |
| {{< hl-text yellow >}}13.2.6{{< /hl-text >}} | Extensions of the hidden Markov model |
| {{< hl-text green >}}13.3{{< /hl-text >}} | Linear Dynamical Systems |
| {{< hl-text green >}}13.3.1{{< /hl-text >}} | Inference in LDS |
| {{< hl-text green >}}13.3.2{{< /hl-text >}} | Learning in LDS |
| {{< hl-text yellow >}}13.3.3{{< /hl-text >}} | Extensions of LDS |
| {{< hl-text yellow >}}13.3.4{{< /hl-text >}} | Particle filters |

## 2.14. Chapter 14: Combining Models
| Index | Title |
|---|---|
| {{< hl-text green >}}14{{< /hl-text >}} | Combining Models |
| {{< hl-text green >}}14.1{{< /hl-text >}} | Bayesian Model Averaging |
| {{< hl-text green >}}14.2{{< /hl-text >}} | Committees |
| {{< hl-text green >}}14.3{{< /hl-text >}} | Boosting |
| {{< hl-text green >}}14.3.1{{< /hl-text >}} | Minimizing exponential error |
| {{< hl-text green >}}14.3.2{{< /hl-text >}} | Error functions for boosting |
| {{< hl-text cyan >}}14.4{{< /hl-text >}} | Tree-based Models |
| {{< hl-text yellow >}}14.5{{< /hl-text >}} | Conditional Mixture Models |
| {{< hl-text yellow >}}14.5.1{{< /hl-text >}} | Mixtures of linear regression models |
| {{< hl-text yellow >}}14.5.2{{< /hl-text >}} | Mixtures of logistic models |
| {{< hl-text yellow >}}14.5.3{{< /hl-text >}} | Mixtures of experts |
