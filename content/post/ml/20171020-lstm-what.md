---
title: "[RNN] LSTM là gì?"
slug: what-is-lstm
date: 2017-10-20
categories:
- Học Máy
- Học Sâu
- RNN
tags:
- RNN
- LSTM
keywords:
- Mạng RNN
- LSTM
- Học Sâu
- Deep Learning
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: //res.cloudinary.com/dominhhai/image/upload/dl/logo.png
metaAlignment: center
---
> Bài LSTM này được dịch lại từ trang <a href="//colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">colah's blog</a>.

LSTM là một mạng cải tiến của RNN nhằm giải quyết vấn đề nhớ các bước dài của RNN.
Có nhiều bài đã viết về LSTM, nhưng được đề cập tới nhiều và dễ hiểu nhất có lẽ là của anh
<a href="https://github.com/colah/" target="_blank">Christopher Olah</a>.
Nên mình quyết định dịch lại cho bản thân có thể hiểu thêm và cho cả các bạn đang tìm hiểu.

<!--more-->

<!-- toc -->

# 1. Mạng hồi quy RNN
Con người không bắt đầu suy nghĩ của họ từ đầu tại tất cả các thời điểm.
Cũng như bạn đang đọc bài viết này, bạn hiểu mỗi chữ ở đây dựa vào
từ bạn đã hiểu các chữ trước đó chứ không phải là đọc tới đâu ném hết đi tới đó,
rồi lại bắt đầu suy nghĩ lại từ đầu tới chữ bạn đang đọc.
Tức là tư duy đã có một bộ nhớ để lưu lại những gì diễn ra trước đó.

Tuy nhiên các mô hình mạng nơ-ron truyền thống thì không thể làm được việc đó,
đó có thể coi là một khuyết điểm chính của mạng nơ-ron truyền thống.
Ví dụ, bạn muốn phân loại các bối cảnh xảy ra ở tất cả các thời điểm trong một bộ phim,
thì đúng là không rõ làm thế nào để có thể hiểu được một tình huống trong phim
mà lại phụ thuộc vào các tình huống trước đó nếusử dụng các mạng nơ-ron truyền thống.

Mạng nơ-ron hồi quy (Recurrent Neural Network) sinh ra để giải quyết vấn đề đó.
Mạng này chứa các vòng lặp bên trong cho phép thông tin có thể lưu lại được.

{{< image classes="fancybox center" thumbnail-width="20%" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" title="Recurrent Neural Networks have loops." >}}

Hình vẽ trên mô tả một đoạn của mạng nơ-ron hồi quy $ A $ với đầu vào là $ x_t $ và đầu ra là $ h_t $.
Một vòng lặp cho phép thông tin có thể được truyền từ bước này qua bước này qua bước khác của mạng nơ-ron.

Các vòng lặp này khiến cho mạng nơ-ron hồi quy trông có vẻ khó hiểu.
Tuy nhiên, nếu bạn để ý một chút thì nó không khác mấy so với các mạng nơ-ron thuần.
Một mạng nơ-ron hồi quy có thể được coi là nhiều bản sao chép của cùng một mạng,
trong đó mỗi đầu ra của mạng này là đầu vào của một mạng sao chép khác.
Nói thì hơi khó hiểu, nhưng bạn hãy xem hình mô tả sau:

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" title="An unrolled recurrent neural network." >}}

Chuỗi lặp lại các mạng này chính là phân giải của mạng nơ-ron hồi quy,
các vòng lặp khiến chúng tạo thành một chuỗi danh sách các mạng sao chép nhau.
Bạn có thấy nó khác gì một mạng nơ-ron thuần không? Không khác gì phải không?
Các nút của mạng vẫn nhận đầu vào và có đầu ra hệt như mạng nơ-ron thuần.

Trong vài năm gần đây, việc ứng dụng RNN đã đưa ra được nhiều kết quả không thể tin nổi
trong nhiều lĩnh vực: nhận dạng giọng nói, mô hình hóa ngôn ngữ, dịch máy, mô tả ảnh,...
Danh sách vẫn còn đang được mở rộng tiếp.
Anh Andrej Karpathy đã đề cập tới một số kêt quả mà RNN mang lại tại
<a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank">bài viết này</a>, nên tôi sẽ không bàn luận thêm nữa.
Nhưng tôi vẫn muốn thốt lên rằng chúng thật là quá tuyệt vời.

Đằng sau sự thành công này chính là sự đóng góp của
<a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank">LSTM</a>.
LSTM là một dạng đặc biệt của mạng nơ-ron hồi quy,
với nhiều bài toán thì nó tốt hơn mạng hồi quy thuần.
Hầu hết các kết quả thú vị thu được từ mạng RNN là được sử dụng với LSTM.
Trong bài viết này, ta sẽ cùng khám phá xem mạng LSTM là cái gì nhé.

# 2. Vấn đề phụ thuộc xa
Một điểm nổi bật của RNN chính là ý tưởng kết nối các thông tin phía trước để dự đoán cho hiện tại.
Việc này tương tự như ta sử dụng các cảnh trước của bộ phim để hiểu được cảnh hiện thời.
Nếu mà RNN có thể làm được việc đó thì chúng sẽ cực kì hữu dụng,
tuy nhiên liệu chúng có thể làm được không? Câu trả lời là _còn tùy_.

Đôi lúc ta chỉ cần xem lại thông tin vừa có thôi là đủ để biết được tình huống hiện tại.
Ví dụ, ta có câu: "*các đám may trên bầu trời*" thì ta chỉ cần đọc tới "*các đám may trên bầu*" là đủ biết được chữ tiếp theo là "*trời*" rồi.
Trong tình huống này, khoảng cách tới thông tin có được cần để dự đoán là nhỏ,
nên RNN hoàn toàn có thể học được.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" >}}

Nhưng trong nhiều tình huống ta buộc phải sử dụng nhiều ngữ cảnh hơn để suy luận.
Ví dụ, dự đoán chữ cuối cùng trong đoạn: "*I grew up in France… I speak fluent French.*".
Rõ ràng là các thông tin gần ("*I speak fluent*") chỉ có phép ta biết được đằng sau nó
sẽ là tên của một ngôn ngữ nào đó, còn không thể nào biết được đó là tiếng gì.
Muốn biết là tiếng gì, thì ta cần phải có thêm ngữ cảnh "*I grew up in France*" nữa
mới có thể suy luận được. Rõ ràng là khoảng cách thông tin lúc này có thể đã khá xa rồi.

Thật không may là với khoảng cách càng lớn dần thì RNN bắt đầu không thể nhớ và học được nữa.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" >}}

Về mặt lý thuyết, rõ ràng là RNN có khả năng xử lý các phụ thuộc xa (long-term dependencies).
Chúng ta có thể xem xét và cài đặt các tham số sao cho khéo là có thể giải quyết được vấn đề này.
Tuy nhiên, đáng tiếc trong thực tế RNN có vẻ không thể học được các tham số đó.
Vấn đề này đã được khám phá khá sâu bởi [Hochreiter (1991) [tiếng Đức]](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) và [Bengio, et al. (1994)](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf),
trong các bài báo của mình, họ đã tìm được nhưng lý do căn bản để giải thích tại sao RNN không thể học được.

Tuy nhiên, rất cám ơn là LSTM không vấp phải vấn đề đó!

# 3. Mạng LSTM
Mạng bộ nhớ dài-ngắn (Long Short Term Memory networks), thường được gọi là LSTM -
là một dạng đặc biệt của RNN, nó có khả năng học được các phụ thuộc xa.
LSTM được giới thiệu bởi [Hochreiter & Schmidhuber (1997)](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf),
và sau đó đã được cải tiến và phổ biến bởi rất nhiều người trong ngành.
Chúng hoạt động cực kì hiệu quả trên nhiều bài toán khác nhau nên dần đã trở nên phổ biến như hiện nay.

LSTM được thiết kế để tránh được vấn đề phụ thuộc xa (long-term dependency).
Việc nhớ thông tin trong suốt thời gian dài là đặc tính mặc định của chúng,
chứ ta không cần phải huấn luyện nó để có thể nhớ được.
Tức là ngay nội tại của nó đã có thể ghi nhớ được mà không cần bất kì can thiệp nào.

Mọi mạng hồi quy đều có dạng là một chuỗi các mô-đun lặp đi lặp lại của mạng nơ-ron.
Với mạng RNN chuẩn, các mô-dun này có cấu trúc rất đơn giản,
thường là một tầng $ tanh $.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" title="The repeating module in a standard RNN contains a single layer." >}}

LSTM cũng có kiến trúc dạng chuỗi như vậy, nhưng các mô-đun trong nó có cấu trúc khác với mạng RNN chuẩn.
Thay vì chỉ có một tầng mạng nơ-ron, chúng có tới 4 tầng tương tác với nhau một cách rất đặc biệt.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" title="The repeating module in an LSTM contains four interacting layers." >}}

Giờ thì đừng hoang mang về chi tiết bên trong chúng ngay,
chúng ta sẽ khám phá chúng chi tiết chúng ở bước sau.
Điều bạn cần làm bây giờ là làm hãy làm quen với các kí hiệu mà ta sẽ sử dụng ở dưới đây:

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" >}}

Ở sơ đồ trên, mỗi một đường mang một véc-tơ từ đầu ra của một nút tới đầu vào của một nút khác.
Các hình trong màu hồng biểu diễn các phép toán như phép cộng véc-tơ chẳng hạn,
còn các ô màu vàng được sử dụng để học trong các từng mạng nơ-ron.
Các đường hợp nhau kí hiệu việc kết hợp,
còn các đường rẽ nhánh ám chỉ nội dung của nó được sao chép và chuyển tới các nơi khác nhau.

## 3.1. Ý tưởng cốt lõi của LSTM
Chìa khóa của LSTM là trạng thái tế bào (cell state) -
chính đường chạy thông ngang phía trên của sơ đồ hình vẽ.

Trạng thái tế bào là một dạng giống như băng truyền.
Nó chạy xuyên suốt tất cả các mắt xích (các nút mạng) và chỉ tương tác tuyến tính đôi chút.
Vì vậy mà các thông tin có thể dễ dàng truyền đi thông suốt mà không sợ bị thay đổi.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" >}}

LSTM có khả năng bỏ đi hoặc thêm vào các thông tin cần thiết cho trạng thái tế báo,
chúng được điều chỉnh cẩn thận bởi các nhóm được gọi là cổng (gate).

Các cổng là nơi sàng lọc thông tin đi qua nó,
chúng được kết hợp bởi một tầng mạng sigmoid và một phép nhân.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" >}}

Tầng sigmoid sẽ cho đầu ra là một số trong khoản $ [0, 1] $,
mô tả có bao nhiêu thông tin có thể được thông qua.
Khi đầu ra là $ 0 $ thì có nghĩa là không cho thông tin nào qua cả,
còn khi là $ 1 $ thì có nghĩa là cho tất cả các thông tin đi qua nó.

Một LSTM gồm có 3 cổng như vậy để duy trì và điều hành trạng thái của tế bào.

## 3.2. Bên trong LSTM
Bước đầu tiên của LSTM là quyết định xem thông tin nào cần bỏ đi từ trạng thái tế bào.
Quyết định này được đưa ra bởi tầng sigmoid - gọi là "tầng cổng quên" (forget gate layer).
Nó sẽ lấy đầu vào là $ h_{t-1} $ và $ x_t $ rồi đưa ra kết quả là một số trong khoảng
$ [0, 1] $ cho mỗi số trong trạng thái tế bào $ C\_{t-1} $.
Đẩu ra là $ 1 $ thể hiện rằng nó giữ toàn bộ thông tin lại,
còn $ 0 $ chỉ rằng taonf bộ thông tin sẽ bị bỏ đi.

Quay trở lại với ví dụ mô hình ngôn ngữ dự đoán từ tiếp theo dựa trên tất cả các từ trước đó,
với những bài toán như vậy, thì trạng thái tế bào có thể sẽ mang
thông tin về giới tính của một nhân vật nào đó giúp ta sử dụng được đại từ nhân xưng chuẩn xác.
Tuy nhiên, khi đề cập tới một người khác thì ta sẽ không muốn nhớ tới giới tính của nhân vật nữa,
vì nó không còn tác dụng gì với chủ thế mới này.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" >}}

Bước tiếp theo là quyết định xem thông tin mới nào ta sẽ lưu vào trạng thái tế bào.
Việc này gồm 2 phần.
Đầu tiên là sử dụng một tầng sigmoid được gọi là "tầng cổng vào" (input gate layer)
để quyết định giá trị nào ta sẽ cập nhập.
Tiếp theo là một tầng $ tanh $ tạo ra một véc-tơ cho giá trị mới $ \tilde{C_t} $
nhằm thêm vào cho trạng thái.
Trong bước tiếp theo, ta sẽ kết hợp 2 giá trị đó lại để tạo ra một cập nhập cho trạng thái.

Chẳng hạn với ví dụ mô hình ngôn ngữ của ta,
ta sẽ muốn thêm giới tính của nhân vật mới này vào trạng thái tế bào
và thay thế giới tính của nhân vật trước đó.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" >}}

Giờ là lúc cập nhập trạng thái tế bào cũ $ C_{t-1} $ thành trạng thái mới $ C_t $.
Ở các bước trước đó đã quyết định những việc cần làm, nên giờ ta chỉ cần thực hiện là xong.

Ta sẽ nhân trạng thái cũ với $ f_t $ để bỏ đi những thông tin ta quyết định quên lúc trước.
Sau đó cộng thêm $ i_t * \tilde{C_t} $.
Trạng thái mơi thu được này phụ thuộc vào việc ta quyết định cập nhập mỗi giá trị trạng thái ra sao.

Với bài toàn mô hình ngôn ngữ, chính là việc ta bỏ đi thông tin về giới tính của nhân vật cũ,
và thêm thông tin về giới tính của nhân vật mới như ta đã quyết định ở các bước trước đó.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" >}}

Cuối cùng, ta cần quyết định xem ta muốn đầu ra là gì.
Giá trị đầu ra sẽ dựa vào trạng thái tế bào, nhưng sẽ được tiếp tục sàng lọc.
Đầu tiên, ta chạy một tầng sigmoid để quyết định phần nào của trạng thái tế bào ta muốn xuất ra.
Sau đó, ta đưa nó trạng thái tế bảo qua một hàm $ tanh $ để co giá trị nó về khoảng $ [-1, 1] $,
và nhân nó với đầu ra của cổng sigmoid để được giá trị đầu ra ta mong muốn.

Với ví dụ về mô hình ngôn ngữ, chỉ cần xem chủ thể mà ta có thể đưa ra thông tin về một trạng từ đi sau đó.
Ví dụ, nếu đầu ra của chủ thể là số ít hoặc số nhiều thì ta có thể biết được dạng của trạng từ đi theo sau nó phải như thế nào.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" >}}

# 4. Các biến thể của bộ nhớ dài hạn
Những thứ ta vừa mô tả ở trên là một LSTM khá bình thường.
Nhưng không phải tất cả các LTSM đều giống như vậy.
Thực tế, các bài báo về LTSM đều sử dụng một phiên bản hơi khác so với mô hình LTSM chuẩn.
Sự khác nhau không lớn, nhưng chúng giúp giải quyết phần nào đó trong cấu trúc của LTSM.

Một dạng LTSM phổ biến được giới thiệu bởi
<a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank">Gers & Schmidhuber (2000)</a> được thêm các đường kết nối "peephole connections",
làm cho các tầng cổng nhận được giá trị đầu vào là trạng thái tế bào.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png" >}}

Hình trên mô tả các đường được thêm vào mọi cổng,
nhưng cũng có những bài báo chỉ thêm cho một vài cổng mà thôi.

Một biến thể khác là nối 2 cổng loại trừ và đầu vào với nhau.
Thay vì phân tách các quyết định thông tin loại trừ và thông tin mới thêm vào,
ta sẽ quyết định chúng cùng với nhau luôn.
Ta chỉ bỏ đi thông tin khi mà ta thay thế nó bằng thông tin mới đưa vào.
Ta chỉ đưa thông tin mới vào khi ta bỏ thông tin cũ nào đó đi.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png" >}}

Một biến thể khá thú vị khác của LSTM là Gated Recurrent Unit, hay GRU được giới thiệu bởi
<a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank">Cho, et al. (2014)</a>.
Nó kết hợp các cổng loại trừ và đầu vào thành một cổng "cổng cập nhập" (update gate).
Nó cũng hợp trạng thái tế bào và trạng thái ẩn với nhau tạo ra một thay đổi khác.
Kết quả là mô hình của ta sẽ đơn giản hơn mô hình LSTM chuẩn và ngày càng trở nên phổ biến.

{{< image classes="fancybox center" src="//colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png" >}}

Trên đây chỉ là một vài biến thế được chú ý nhiều nhất thôi,
thực tế có rất nhiều các biến thể khác nhau của LSTM như
Depth Gated RNNs của <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank">Yao, et al. (2015)</a>.
Cũng có những biến thể mà chiến lực xử lý phụ thuộc xa hoàn toàn khác như
Clockwork RNNs của <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank">Koutnik, et al. (2014)</a>.

Nếu bạn muốn tìm hiểu xem biến thể nào là tốt nhất và chúng khác nhau thế nào,
thì có thể đọc bài so sánh khá hay này của
<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank">Greff, et al. (2015)</a>.
Ngoài ra thì <a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank">Jozefowicz, et al. (2015)</a>
thậm chí còn thử hàng chục nghìn kiến trúc RNN khác nhau
và tìm ra một vài mô hình hoạt động tốt hơn cả LSTM ở một số bài toán.

# 5. Kết luận
Như từ đầu tôi đã đề cập tới các kể quả khả quan mà người ta thu được với RNN.
Đằng sau các thành quả đó là việc sử dụng LSTM.
Chúng hoạt động thực sự tốt hơn nhiều cho hầu hết các bài toán!

Viết ra một tập các công thức, khiến cho LSTM trở nên khá khó hiểu.
Nhưng hi vọng là thông qua các bước phân tích trong bày này có thể giúp bạn hình dung được phần nào chiến lược của LSTM thế nào.

LSTM là một bước lớn trong việc sử dụng RNN.
Ý tưởng của nó giúp cho tất cả các bước của RNN có thể truy vấn được thông tin
từ một tập thông tin lớn hơn.
Ví dụ, nếu bạn sử dụng RNN để tạo mô tả cho một bức ảnh,
nó có thể lấy một phần ảnh để dự đoán mô tả từ tất cả các từ đầu vào.
Bằng chứng là <a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank">Xu, et al. (2015)</a> đã thực hiện được chính xác việc này.
Hiện nay cũng đã có nhiều kết qua thực sự rất thú vị được chú ý
và dường như có nhiều kết quả hơn chúng ta vẫn biết.

Sự chú ý không chỉ gói gọn trong nhóm nghiên cứ RNN.
Ví dụ <a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank">Grid LSTMs của Kalchbrenner, et al. (2015)</a> có vẻ như cũng rất tiềm năng.
Cũng có người sử dụng RNN trong các mô hình sinh như
<a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank">Gregor, et al. (2015)</a>, [Chung, et al. (2015)](http://arxiv.org/pdf/1506.02216v3.pdf),
hay <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank">Bayer & Osendorfer (2015)</a> cũng rất thú vị.
Mấy năm gần đây là quãng thời gian rất sôi nổi của mạng nơ-ron hồi quy,
vả chúng còn được kì vọng nhiều hơn nữa trong tương lai.
