<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.54.0 with theme Tranquilpeak 0.4.1-BETA"><title>Pattern Recognition and Machine Learning</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Pattern Recognition and Machine Learning,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/en-us/2017/12/ml-prml/><meta name=description content="This book is known as the textbook for machine learning learners. It covers various algorithm and the theory underline. It&rsquo;s hard to learn too! So, I have to find the complexity of each part in order to study more productivity."><link rel=publisher href=https://plus.google.com/115106277658014197977><meta property=og:locale content=en_US><meta property=og:type content=article><meta property=article:author content=dominhai><meta property=og:title content="Pattern Recognition and Machine Learning"><meta property=og:url content=https://dominhhai.github.io/en-us/2017/12/ml-prml/><meta property=og:description content="This book is known as the textbook for machine learning learners. It covers various algorithm and the theory underline. It&rsquo;s hard to learn too! So, I have to find the complexity of each part in order to study more productivity."><meta property=og:site_name content="Hai's Blog"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/ml/prml.jpg><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:creator content=@minhhai3b><meta name=twitter:card content=summary><meta name=twitter:title content="Pattern Recognition and Machine Learning"><meta name=twitter:url content=https://dominhhai.github.io/en-us/2017/12/ml-prml/><meta name=twitter:description content="This book is known as the textbook for machine learning learners. It covers various algorithm and the theory underline. It&rsquo;s hard to learn too! So, I have to find the complexity of each part in order to study more productivity."><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/ml/prml.jpg><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/en-us/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Author's picture"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Author's picture"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/en-us/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Home</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/en-us/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Categories</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/en-us/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Tags</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/en-us/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Archives</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/en-us/talk/><i class="sidebar-button-icon fa fa-lg fa-child"></i><span class=sidebar-button-desc>Tech Talks</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/en-us/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Stupid Question</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/en-us/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>Pattern Recognition and Machine Learning</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-12-12T16:36:56&#43;09:00>12 December, 2017</time>
<span>in</span>
<a class=category-link href=https://dominhhai.github.io/en-us/categories/machine-learning>Machine Learning</a>,
<a class=category-link href=https://dominhhai.github.io/en-us/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>This book is known as the <strong>textbook</strong> for machine learning learners. It covers various algorithm and the theory underline. It&rsquo;s hard to learn too! So, I have to find the complexity of each part in order to study more productivity.</p><p>I found the guideline and complexity reference from <a href=http://ibisforest.org/index.php?PRML%2Fcourse target=_blank _ rel="noopener noreferrer">this Japanese page</a>. So, I just translate &amp; copy them to here. I have read a small parts of this book, these guideline is fair ok IMO.</p><h1 id=table-of-contents>Table of Contents</h1><nav id=TableOfContents><ul><li><a href=#1-levels>1. Levels</a></li><li><a href=#2-chapters>2. Chapters</a><ul><li><a href=#2-1-chapter-1-introduction>2.1. Chapter 1: Introduction</a></li><li><a href=#2-2-chapter-2-probability-distributions>2.2. Chapter 2: Probability Distributions</a></li><li><a href=#2-3-chapter-3-linear-models-for-regression>2.3. Chapter 3: Linear Models for Regression</a></li><li><a href=#2-4-chapter-4-linear-models-for-classification>2.4. Chapter 4: Linear Models for Classification</a></li><li><a href=#2-5-chapter-5-neural-networks>2.5. Chapter 5: Neural Networks</a></li><li><a href=#2-6-chapter-6-kernel-methods>2.6. Chapter 6: Kernel Methods</a></li><li><a href=#2-7-chapter-7-sparse-kernel-machines>2.7. Chapter 7: Sparse Kernel Machines</a></li><li><a href=#2-8-chapter-8-graphical-models>2.8. Chapter 8: Graphical Models</a></li><li><a href=#2-9-chapter-9-mixture-models-and-em>2.9. Chapter 9: Mixture Models and EM</a></li><li><a href=#2-10-chapter-10-approximate-inference>2.10. Chapter 10: Approximate Inference</a></li><li><a href=#2-11-chapter-11-sampling-methods>2.11. Chapter 11: Sampling Methods</a></li><li><a href=#2-12-chapter-12-continuous-latent-variables>2.12. Chapter 12: Continuous Latent Variables</a></li><li><a href=#2-13-chapter-13-sequential-data>2.13. Chapter 13: Sequential Data</a></li><li><a href=#2-14-chapter-14-combining-models>2.14. Chapter 14: Combining Models</a></li></ul></li></ul></nav><h1 id=1-levels>1. Levels</h1><table><thead><tr><th>Level</th><th>Content</th></tr></thead><tbody><tr><td>ðŸ˜„<br><span class="highlight-text cyan">Basic</span></td><td>The basic theory and methods for beginers. These part should be read first.</td></tr><tr><td>ðŸ˜Š<br><span class="highlight-text green">Intermediate</span></td><td>Basic Bayes inference and a bit adavanced contents. Beside tha, it contents some useful methods for special cases also. These parts target are doctoral students.</td></tr><tr><td>ðŸ˜°<br><span class="highlight-text yellow">Advance</span></td><td>About the advanced contents with deep theory underline. So, these will be fittable for doctoral studiers, researcher and machine learning&rsquo;s engineers.</td></tr></tbody></table><h1 id=2-chapters>2. Chapters</h1><h2 id=2-1-chapter-1-introduction>2.1. Chapter 1: Introduction</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜„ <span class="highlight-text cyan">1</span></td><td>Introduction</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.1</span></td><td>Example: Polynomial Curve Fitting</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.2</span></td><td>Probability Theory</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.2.1</span></td><td>Probability densities</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.2.2</span></td><td>Expectations and covariances</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.2.3</span></td><td>Bayesian probabilities</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.2.4</span></td><td>The Gaussian distribution</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.2.5</span></td><td>Curve fitting re-visited</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">1.2.6</span></td><td>Bayesian curve fitting</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.3</span></td><td>Model Selection</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.4</span></td><td>The Curse of Dimensionality</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.5</span></td><td>Decision Theory</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.5.1</span></td><td>Minimizing the misclassification rate</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.5.2</span></td><td>Minimizing the expected loss</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">1.5.3</span></td><td>The reject option</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">1.5.4</span></td><td>Inference and decision</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.5.5</span></td><td>Loss functions for regression</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.6</span></td><td>Information Theory</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">1.6.1</span></td><td>Relative entropy and mutual information</td></tr></tbody></table><h2 id=2-2-chapter-2-probability-distributions>2.2. Chapter 2: Probability Distributions</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜„ <span class="highlight-text cyan">2</span></td><td>Probability Distributions</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.1</span></td><td>Binary Variables</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.1.1</span></td><td>The beta distribution</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.2</span></td><td>Multinomial Variables</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.2.1</span></td><td>The Dirichlet distribution</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.3</span></td><td>The Gaussian Distribution</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.3.1</span></td><td>Conditional Gaussian distributions</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.3.2</span></td><td>Marginal Gaussian distributions</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.3.3</span></td><td>Bayesâ€™ theorem for Gaussian variables</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.3.4</span></td><td>Maximum likelihood for the Gaussian</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">2.3.5</span></td><td>Sequential estimation</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">2.3.6</span></td><td>Bayesian inference for the Gaussian</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">2.3.7</span></td><td>Studentâ€™s t-distribution</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">2.3.8</span></td><td>Periodic variables</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.3.9</span></td><td>Mixtures of Gaussians</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.4</span></td><td>The Exponential Family</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.4.1</span></td><td>Maximum likelihood and sufficient statistics</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">2.4.2</span></td><td>Conjugate priors</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">2.4.3</span></td><td>Noninformative priors</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.5</span></td><td>Nonparametric Methods</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.5.1</span></td><td>Kernel density estimators</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">2.5.2</span></td><td>Nearest-neighbour methods</td></tr></tbody></table><h2 id=2-3-chapter-3-linear-models-for-regression>2.3. Chapter 3: Linear Models for Regression</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜„ <span class="highlight-text cyan">3</span></td><td>Linear Models for Regression</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">3.1</span></td><td>Linear Basis Function Models</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">3.1.1</span></td><td>Maximum likelihood and least squares</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">3.1.2</span></td><td>Geometry of least squares</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">3.1.3</span></td><td>Sequential learning</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">3.1.4</span></td><td>Regularized least squares</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">3.1.5</span></td><td>Multiple outputs</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">3.2</span></td><td>The Bias-Variance Decomposition</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">3.3</span></td><td>Bayesian Linear Regression</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">3.3.1</span></td><td>Parameter distribution</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">3.3.2</span></td><td>Predictive distribution</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">3.3.3</span></td><td>Equivalent kernel</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">3.4</span></td><td>Bayesian Model Comparison</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">3.5</span></td><td>The Evidence Approximation</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">3.5.1</span></td><td>Evaluation of the evidence function</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">3.5.2</span></td><td>Maximizing the evidence function</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">3.5.3</span></td><td>Effective number of parameters</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">3.6</span></td><td>Limitations of Fixed Basis Functions</td></tr></tbody></table><h2 id=2-4-chapter-4-linear-models-for-classification>2.4. Chapter 4: Linear Models for Classification</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜„ <span class="highlight-text cyan">4</span></td><td>Linear Models for Regression</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.1</span></td><td>Discriminant Functions</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.1.1</span></td><td>Two classes</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.1.2</span></td><td>Multiple classes</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.1.3</span></td><td>Lest squares for classification</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.1.4</span></td><td>Fisher&rsquo;s linear discriminant</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.1.5</span></td><td>Relation to least squares</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.1.6</span></td><td>Fisher&rsquo;s discriminant for multiple classes</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.1.7</span></td><td>The perceptron algorithm</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.2</span></td><td>Probabilistic Generative Models</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.2.1</span></td><td>Continuous inputs</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.2.2</span></td><td>Maximum likelihood solution</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.2.3</span></td><td>Discrete features</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.2.4</span></td><td>Exponential family</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.3</span></td><td>Probabilistic Discriminant Models</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.3.1</span></td><td>Fixed basis functions</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.3.2</span></td><td>Logistic regression</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.3.3</span></td><td>Interative reweighted least squares</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">4.3.4</span></td><td>Multiclass logistic regression</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">4.3.5</span></td><td>Probit regression</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">4.3.6</span></td><td>Canonical link functions</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">4.4</span></td><td>The Laplace Approximation</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">4.4.1</span></td><td>Model comparison and BIC</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">4.5</span></td><td>Bayesian Logistic Regression</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">4.5.1</span></td><td>Laplace approximation</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">4.5.2</span></td><td>Predictive distribution</td></tr></tbody></table><h2 id=2-5-chapter-5-neural-networks>2.5. Chapter 5: Neural Networks</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜„ <span class="highlight-text cyan">5</span></td><td>Neural Networks</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.1</span></td><td>Feed-forward Networks Functions</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.1.1</span></td><td>Weight-space symmetries</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.2</span></td><td>Network Training</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.2.1</span></td><td>Parameter optimization</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.2.2</span></td><td>Local quadratic approximation</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.2.3</span></td><td>Use of gradient information</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.2.4</span></td><td>Gradient descent optimization</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.3</span></td><td>Error Backpropagation</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.3.1</span></td><td>Evaluation of error-function derivatives</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.3.2</span></td><td>A simple example</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">5.3.3</span></td><td>Efficiency of backpropagation</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.3.4</span></td><td>The Jacobian matrix</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.4</span></td><td>The Hessian Matrix</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.4.1</span></td><td>Diagonal approximation</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.4.2</span></td><td>Outer product approximation</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.4.3</span></td><td>Inverse Hessian</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.4.4</span></td><td>Finite differences</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.4.5</span></td><td>Exact evaluation of the Hessian</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.4.6</span></td><td>Fast multiplication by the Hessian</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">5.5</span></td><td>Regularization in Neural Networks</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">5.5.1</span></td><td>Consistent Gaussian priors</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">5.5.2</span></td><td>Early stopping</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.5.3</span></td><td>Invariances</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.5.4</span></td><td>Tangent propagation</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.5.5</span></td><td>Training with transformed data</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.5.6</span></td><td>Convolutional networks</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.5.7</span></td><td>Soft weight sharing</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.6</span></td><td>Mixture Density Networks</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.7</span></td><td>Bayesian Neural Networks</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.7.1</span></td><td>Posterior parameter distribution</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.7.2</span></td><td>Hyperparameter optimization</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">5.7.3</span></td><td>Bayesian neural networks for classification</td></tr></tbody></table><h2 id=2-6-chapter-6-kernel-methods>2.6. Chapter 6: Kernel Methods</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜„ <span class="highlight-text cyan">6</span></td><td>Kernel Methods</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">6.1</span></td><td>Dual Representaions</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">6.3</span></td><td>Constructing Kernels</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">6.3</span></td><td>Radial Basis Function Networks</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">6.3.1</span></td><td>Nadaraya-Watson model</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">6.4</span></td><td>Gaussian Processes</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">6.4.1</span></td><td>Linear regression revisited</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">6.4.2</span></td><td>Gaussian processes for regression</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">6.4.3</span></td><td>Learning the hyperparameter</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">6.4.4</span></td><td>Automatic relevance determination</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">6.4.5</span></td><td>Gaussian processes for classification</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">6.4.6</span></td><td>Laplace approximation</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">6.4.7</span></td><td>Connection to neural networks</td></tr></tbody></table><h2 id=2-7-chapter-7-sparse-kernel-machines>2.7. Chapter 7: Sparse Kernel Machines</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜„ <span class="highlight-text cyan">7</span></td><td>Sparse Kernel Machines</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">7.1</span></td><td>Maximum Margin Classifiers</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">7.1.1</span></td><td>Overlapping class distributions</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">7.1.2</span></td><td>Relation to logistic regression</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">7.1.3</span></td><td>Multiclass SVMs</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">7.1.4</span></td><td>SVMs for regression</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">7.1.5</span></td><td>Computational learning theory</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">7.2</span></td><td>Relevance Vector Machines</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">7.2.1</span></td><td>RVM for regression</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">7.2.2</span></td><td>Analysis of sparsity</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">7.2.3</span></td><td>RVM for classification</td></tr></tbody></table><h2 id=2-8-chapter-8-graphical-models>2.8. Chapter 8: Graphical Models</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜Š <span class="highlight-text green">8</span></td><td>Graphical Models</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.1</span></td><td>Bayesian Networks</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.1.1</span></td><td>Example: Polynomial regression</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.1.2</span></td><td>Generative models</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.1.3</span></td><td>Discrete variables</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.1.4</span></td><td>Linear-Gaussian models</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.2</span></td><td>Conditional Independence</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.2.1</span></td><td>Three example graphs</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.2.2</span></td><td>D-separation</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.3</span></td><td>Markov Random Fields</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.3.1</span></td><td>Conditional independence properties</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.3.2</span></td><td>Factorization properties</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.3.3</span></td><td>Illustration: Image de-noising</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.3.4</span></td><td>Relation to directed graphs</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.4</span></td><td>inference in Graphical Models</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.4.1</span></td><td>Inference on a chain</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.4.2</span></td><td>Trees</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.4.3</span></td><td>Factor graphs</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.4.4</span></td><td>The sum-product algorithm</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">8.4.5</span></td><td>The max-sum algorithm</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">8.4.6</span></td><td>Exact inference in general graphs</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">8.4.7</span></td><td>Loopy belief propagation</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">8.4.8</span></td><td>Learning the graph structure</td></tr></tbody></table><h2 id=2-9-chapter-9-mixture-models-and-em>2.9. Chapter 9: Mixture Models and EM</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜„ <span class="highlight-text cyan">9</span></td><td>Mixture Models and EM</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">9.1</span></td><td>K-means Clustering</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">9.1.1</span></td><td>Image segmentation and compression</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">9.2</span></td><td>Mixtures of Gaussians</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">9.2.1</span></td><td>Maximum likelihood</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">9.2.2</span></td><td>EM for Gaussian mixtures</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">9.3</span></td><td>An Alternative View of EM</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">9.3.1</span></td><td>Gaussian mixtures revisited</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">9.3.2</span></td><td>Relation to K-means</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">9.3.3</span></td><td>Mixtures of Bernoulli distributions</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">9.3.4</span></td><td>EM for Bayesian linear regression</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">9.4</span></td><td>The EM Algorithm in General</td></tr></tbody></table><h2 id=2-10-chapter-10-approximate-inference>2.10. Chapter 10: Approximate Inference</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜Š <span class="highlight-text green">10</span></td><td>Approximate Inference</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">10.1</span></td><td>Variational Inference</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">10.1.1</span></td><td>Factorized distributions</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">10.1.2</span></td><td>Properties of factorized approximations</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">10.1.3</span></td><td>Example: The univariate Gaussian</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">10.1.4</span></td><td>Model comparison</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">10.2</span></td><td>Illustration: Variational Mixture of Gaussians</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">10.2.1</span></td><td>Variational distribution</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">10.2.2</span></td><td>Variational lower bound</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">10.2.3</span></td><td>Predictive density</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.2.4</span></td><td>Determining the number of components</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.2.5</span></td><td>Induced factorizations</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.3</span></td><td>Variational Linear Regression</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.3.1</span></td><td>Variational distribution</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.3.2</span></td><td>Predictive distribution</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.3.3</span></td><td>Lower bound</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.4</span></td><td>Exponential Family Distributions</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.4.1</span></td><td>Variational message passing</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.5</span></td><td>Local Variational Methods</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.6</span></td><td>Variational Logistic Regression</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.6.1</span></td><td>Variational posterior distribution</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.6.2</span></td><td>Optimizing the variational parameters</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.6.3</span></td><td>Inference of hyperparameters</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.7</span></td><td>Expectation Propagation</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.7.1</span></td><td>Example: The clutter problem</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">10.7.2</span></td><td>Expectation propagation of graphs</td></tr></tbody></table><h2 id=2-11-chapter-11-sampling-methods>2.11. Chapter 11: Sampling Methods</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜Š <span class="highlight-text green">11</span></td><td>Sampling Methods</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">11.1</span></td><td>Basis Sampling Algorithms</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">11.1.1</span></td><td>Standard distributions</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">11.1.2</span></td><td>Rejection sampling</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">11.1.3</span></td><td>Adaptive rejection sampling</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">11.1.4</span></td><td>Importance sampling</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">11.1.5</span></td><td>Sampling-importance-resampling</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">11.1.6</span></td><td>Sampling and EM algorithm</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">11.2</span></td><td>Markov Chain Monte Carlo</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">11.2.1</span></td><td>Markov chains</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">11.2.2</span></td><td>The Metropolis-Hastings algorithm</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">11.3</span></td><td>Gibbs Sampling</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">11.4</span></td><td>Slice Sampling</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">11.5</span></td><td>The Hybrid Monte Carlo Algorithm</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">11.5.1</span></td><td>Dynamical systems</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">11.5.2</span></td><td>Hybrid Monte Carlo</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">11.6</span></td><td>Estimating the Partition Function</td></tr></tbody></table><h2 id=2-12-chapter-12-continuous-latent-variables>2.12. Chapter 12: Continuous Latent Variables</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜„ <span class="highlight-text cyan">12</span></td><td>Continuous Latent Variables</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">12.1</span></td><td>Principal Component Analysis</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">12.1.1</span></td><td>Maximum variance formulation</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">12.1.2</span></td><td>Minimum-error formulation</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">12.1.3</span></td><td>Applications of peA</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">12.1.4</span></td><td>PCA for high-dimensional data</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">12.2</span></td><td>Probabilistic p e A</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">12.2.1</span></td><td>Maximum likelihood peA</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">12.2.2</span></td><td>EM algorithm for peA</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">12.2.3</span></td><td>Bayesian peA</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">12.2.4</span></td><td>Factor analysis</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">12.3</span></td><td>Kernel PCA</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">12.4</span></td><td>Nonliear Latent Variable Models</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">12.4.1</span></td><td>Independent component analysis</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">12.4.2</span></td><td>Autoassociative neural networks</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">12.4.3</span></td><td>Modelling nonlinear manifolds</td></tr></tbody></table><h2 id=2-13-chapter-13-sequential-data>2.13. Chapter 13: Sequential Data</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜Š <span class="highlight-text green">13</span></td><td>Sequential Data</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">13.1</span></td><td>Markov Models</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">13.2</span></td><td>Hidden Markov Models</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">13.2.1</span></td><td>Maximum likelihood for the HMM</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">13.2.2</span></td><td>The forward-backward algorithm</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">13.2.3</span></td><td>The sum-product algorithm for the HMM</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">13.2.4</span></td><td>Scaling factors</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">13.2.5</span></td><td>The Viterbi algorithm</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">13.2.6</span></td><td>Extensions of the hidden Markov model</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">13.3</span></td><td>Linear Dynamical Systems</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">13.3.1</span></td><td>Inference in LDS</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">13.3.2</span></td><td>Learning in LDS</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">13.3.3</span></td><td>Extensions of LDS</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">13.3.4</span></td><td>Particle filters</td></tr></tbody></table><h2 id=2-14-chapter-14-combining-models>2.14. Chapter 14: Combining Models</h2><table><thead><tr><th>Index</th><th>Title</th></tr></thead><tbody><tr><td>ðŸ˜Š <span class="highlight-text green">14</span></td><td>Combining Models</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">14.1</span></td><td>Bayesian Model Averaging</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">14.2</span></td><td>Committees</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">14.3</span></td><td>Boosting</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">14.3.1</span></td><td>Minimizing exponential error</td></tr><tr><td>ðŸ˜Š <span class="highlight-text green">14.3.2</span></td><td>Error functions for boosting</td></tr><tr><td>ðŸ˜„ <span class="highlight-text cyan">14.4</span></td><td>Tree-based Models</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">14.5</span></td><td>Conditional Mixture Models</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">14.5.1</span></td><td>Mixtures of linear regression models</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">14.5.2</span></td><td>Mixtures of logistic models</td></tr><tr><td>ðŸ˜° <span class="highlight-text yellow">14.5.3</span></td><td>Mixtures of experts</td></tr></tbody></table></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">TAGGED IN</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/en-us/tags/machine-learning/>Machine Learning</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--disabled"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">NEXT</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/en-us/2017/11/about-git/ data-tooltip="[Git] About GIT by Linus Torvalds"><span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/en-us/2017/12/ml-prml/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/en-us/2017/12/ml-prml/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/en-us/2017/12/ml-prml/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiáº¿ng Viá»‡t" value=vi>Tiáº¿ng Viá»‡t (vi)</option>
<option title=English value=en-us selected>English (en-us)</option>
<option title=æ—¥æœ¬èªž value=ja>æ—¥æœ¬èªž (ja)</option></select></div></div><div id=topic><label>Topic</label><ul><li><a href=https://dominhhai.github.io/en-us/categories/programming/>Programming</a></li><li><a href=https://dominhhai.github.io/en-us/categories/machine-learning/>Machine Learning</a></li><li><a href=https://dominhhai.github.io/en-us/categories/math/>Math</a></li><li><a href=https://dominhhai.github.io/en-us/categories/probability/>Probability</a></li><li><a href=https://dominhhai.github.io/en-us/categories/books/>Books</a></li></ul></div><div id=contact><label>Contact</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Send message</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2021 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--disabled"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">NEXT</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/en-us/2017/11/about-git/ data-tooltip="[Git] About GIT by Linus Torvalds"><span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/en-us/2017/12/ml-prml/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/en-us/2017/12/ml-prml/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/en-us/2017/12/ml-prml/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fen-us%2F2017%2F12%2Fml-prml%2F"><i class="fa fa-facebook-official"></i><span>Share on Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fen-us%2F2017%2F12%2Fml-prml%2F"><i class="fa fa-twitter"></i><span>Share on Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fen-us%2F2017%2F12%2Fml-prml%2F"><i class="fa fa-google-plus"></i><span>Share on Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Author's picture"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/en-us\/2017\/12\/ml-prml\/';this.page.identifier='\/en-us\/2017\/12\/ml-prml\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>