{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_loader.py\n",
    "\n",
    "\"\"\"\n",
    "data_loader\n",
    "~~~~~~~~~~~~\n",
    "Load the MNIST dataset.\n",
    "Use ``load()`` method to load dataset.\n",
    "Check the ``load()`` method for detail.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import urllib2\n",
    "import gzip\n",
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = '../.tmp'\n",
    "DATA_FILENAME = 'mnist.pkl.gz'\n",
    "DATA_FILE = DATA_PATH + '/' + DATA_FILENAME\n",
    "\n",
    "def load():\n",
    "  '''\n",
    "  Load MNIST dataset.\n",
    "  This will return tuple of (training_data, validation_data, test_data)\n",
    "  ``training_data`` is a list of 50,000 training data. Each sample is a tuple ``(x, y)``.\n",
    "  ``x`` is a 784-dim np.ndarray contains the input image, and ``y`` is a 10-dim np.ndarray for one-hot label vector.\n",
    "  ``validataion_test`` and ``test_data`` are lists of 10,000 validataion and test data.\n",
    "  The structure is similar to ``training_data``, except ``y`` is a number corresponding to ``x`` input image.\n",
    "  '''\n",
    "  # download data if not exist\n",
    "  if not os.path.exists(DATA_FILE):\n",
    "    download()\n",
    "  \n",
    "  # load data\n",
    "  with gzip.open(DATA_FILE, 'rd') as file:\n",
    "    tr_dt, v_dt, t_dt = cPickle.load(file)\n",
    "  \n",
    "  # training data\n",
    "  inputs = [x.reshape((784, 1)) for x in tr_dt[0]]\n",
    "  labels = [label_2_vec(y) for y in tr_dt[1]]\n",
    "  training_data = zip(inputs, labels)\n",
    "  \n",
    "  # validation data\n",
    "  inputs = [x.reshape((784, 1)) for x in v_dt[0]]\n",
    "  validation_data = zip(inputs, v_dt[1])\n",
    "  \n",
    "  # test data\n",
    "  inputs = [x.reshape((784, 1)) for x in t_dt[0]]\n",
    "  test_data = zip(inputs, t_dt[1])\n",
    "  \n",
    "  return (training_data, validation_data, test_data)\n",
    "\n",
    "def download():\n",
    "  '''\n",
    "  Download MNIST dataset\n",
    "  '''\n",
    "  # create data dir if not exist\n",
    "  if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "  \n",
    "  # download\n",
    "  url = 'https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz'\n",
    "  input = urllib2.urlopen(url)\n",
    "  with open(DATA_FILE, 'wb') as output:\n",
    "    while True:\n",
    "      data = input.read(4096)\n",
    "      if data:\n",
    "        output.write(data)\n",
    "      else:\n",
    "        break\n",
    "    print('Downloaded MNIST dataset: '+ DATA_FILE)\n",
    "\n",
    "def label_2_vec(label):\n",
    "  '''\n",
    "  One-hot label vector\n",
    "  '''\n",
    "  v = np.zeros((10, 1))\n",
    "  v[label] = 1.0\n",
    "  return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile nn.py\n",
    "\"\"\"\n",
    "NN class\n",
    "~~~~~~~~~~~~~~~~\n",
    "Neural Network implement class.\n",
    "This NN use sigmoid as activation functions with cross-entropy cost function.\n",
    "\n",
    "Usage:\n",
    "1. Init NN\n",
    "nn = NN(layers)\n",
    "\n",
    "2. Train\n",
    "nn.train((x, y), epochs, mini_batch_size, eta)\n",
    "\n",
    "3. Predict\n",
    "y = nn.predict(x)\n",
    "\n",
    "4. Test / Evaluation\n",
    "correct_num = nn.evaluate(test_data)\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Init NN with ``layers`` size.\n",
    "        ``layers`` is array of layer sizes.\n",
    "        E.x: [3, 4, 5, 2] will create a NN of 4 layers.\n",
    "        In which,\n",
    "          input layer containts 3 nodes,\n",
    "         hidden layer 1 contains 4 nodes, \n",
    "         hidden layer 2 contains 5 nodes, \n",
    "         and, output layer contains 2 nodes\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.L = len(layers)\n",
    "        # ``w `` is a list (L-1) dim np.ndarray of matrix W for each layers\n",
    "        # ``w[0]` is layer 2 (hidden layer 1), ..., ``w[L-2]`` is output layer \n",
    "        # Each row hold weights for inputs (from before layer) of correspoding node (on current layer)\n",
    "        # The first column is bias for correspoding node\n",
    "        self.w = [np.random.randn(l2, l1 + 1) for l2, l1 in zip(layers[1:], layers[:-1])]\n",
    "        \n",
    "    def train(self, train_data, epochs, mini_batch_size, eta):\n",
    "        \"\"\"\n",
    "        Train NN with train data ``[(x, y)]``.\n",
    "        This use mini-batch SGD method to train the NN.\n",
    "        \"\"\"\n",
    "        # number of training data        \n",
    "        m = len(train_data)\n",
    "        # cost\n",
    "        cost = []\n",
    "        for j in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch {0} begin...'.format(j + 1))\n",
    "            # shuffle data before run\n",
    "            random.shuffle(train_data)\n",
    "            # divide data into mini batchs\n",
    "            for k in range(0, m, mini_batch_size):\n",
    "                mini_batch = train_data[k:k+mini_batch_size]\n",
    "                m_batch = len(mini_batch)\n",
    "                # calc gradient\n",
    "                w_grad = [np.zeros(W.shape) for W in self.w]\n",
    "                for x, y in mini_batch:\n",
    "                    grad = self.backprop(x, y)\n",
    "                    w_grad = [W_grad + g for W_grad, g in zip(w_grad, grad)]\n",
    "                w_grad = [W_grad / m_batch for W_grad in w_grad]\n",
    "                \n",
    "                # check grad for first mini_batch in first epoch\n",
    "                if j == 0  and k == 0 and not self.check_grad(mini_batch, w_grad):\n",
    "                    print('backprop fail!')\n",
    "                    return False\n",
    "                \n",
    "                # update w\n",
    "                self.w = [W - eta * W_grad for W, W_grad in zip(self.w, w_grad)]\n",
    "            \n",
    "            # calc cost\n",
    "            cost.append(self.cost(train_data))\n",
    "            print('Epoch {0} done: {1}'.format(j + 1, time.time() - start_time))\n",
    "            \n",
    "        return cost\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict label of single input data ``x``\n",
    "        \"\"\"\n",
    "        _, a = self.feedforward(x)\n",
    "        return np.argmax(a[-1])\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluate NN with test data.\n",
    "        This will return the number of correct result\n",
    "        \"\"\"\n",
    "        results = [(self.predict(x), y) for (x, y) in test_data]\n",
    "        return sum(int(_y == y) for (_y, y) in results)\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Feedforward through network for calc ``z``,`` a``.\n",
    "        ``z`` is list of (L-1) vec-tor, ``z[0]`` for layer 2, and so on.\n",
    "        ``a`` is list of (L) vec-tor, ``a[0]`` for layer 1, and so on.\n",
    "        \"\"\"\n",
    "        z = []\n",
    "        a = [self.add_bias(x)]\n",
    "        for l in range(1, self.L):\n",
    "            z_l = np.dot(self.w[l-1], a[l-1])\n",
    "            a_l = self.sigmoid(z_l)\n",
    "            if l < self.L - 1:\n",
    "                a_l = self.add_bias(a_l)\n",
    "            \n",
    "            z.append(z_l)\n",
    "            a.append(a_l)\n",
    "            \n",
    "        return (z, a)\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Backpropagation to calc derivatives\n",
    "        \"\"\"\n",
    "        w_grad = [np.zeros(W.shape) for W in self.w]\n",
    "        # feedforward\n",
    "        z, a = self.feedforward(x)\n",
    "        # backward\n",
    "        dz = a[-1] - y\n",
    "        for _l in range(1, self.L):\n",
    "            l = -_l # layer index\n",
    "            if l < -1:\n",
    "                da = self.sigmoid_grad(z[l])\n",
    "                # do not calc for w_0 (da_0 / dz = 0 because of a_0 = 1 for all z)\n",
    "                dz = np.dot(self.w[l+1][:, 1:].transpose(), dz) * da\n",
    "            # gradient    \n",
    "            w_grad[l] = np.dot(dz, a[l-1].transpose())\n",
    "        \n",
    "        return w_grad\n",
    "    \n",
    "    def add_bias(self, a):\n",
    "        \"\"\"\n",
    "        add a_0 = 1 as input for bias w_0\n",
    "        \"\"\"\n",
    "        return np.insert(a, 0, 1, axis=0)\n",
    "    \n",
    "    def check_grad(self, data, grad, epsilon=1e-4, threshold=1e-6):\n",
    "        \"\"\"\n",
    "        Check gradient with:\n",
    "        * Epsilon      : 1e-4\n",
    "        * Threshold : 1e-6\n",
    "        \"\"\"\n",
    "        for l in range(self.L - 1):\n",
    "            n_row, n_col = self.w[l].shape\n",
    "            for i in range(n_row):\n",
    "                for j in range(n_col):\n",
    "                    w_l_ij = self.w[l][i][j]\n",
    "                    # left\n",
    "                    self.w[l][i][j] = w_l_ij - epsilon\n",
    "                    l_cost = self.cost(data)\n",
    "                    # right\n",
    "                    self.w[l][i][j] = w_l_ij + epsilon\n",
    "                    r_cost = self.cost(data)\n",
    "                    # numerical grad\n",
    "                    num_grad = (r_cost - l_cost) / (2 * epsilon)\n",
    "                    \n",
    "                    # diff\n",
    "                    diff = abs(grad[l][i][j] - num_grad)\n",
    "                    \n",
    "                    # reset w\n",
    "                    self.w[l][i][j] = w_l_ij\n",
    "                    \n",
    "                    if diff > threshold:\n",
    "                        print('Check Grad Error at (l: {0}, col: {1}, row: {2}), | num_grad: {3} vs backprop grad: {4} | : {5}'\n",
    "                              .format(l, i, j, num_grad, grad[l][i][j], diff))\n",
    "                        return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def cost(self, data):\n",
    "        \"\"\"\n",
    "        Return cross-entropy cost of NN on test data\n",
    "        \"\"\"\n",
    "        m = len(data)\n",
    "        j = 0\n",
    "        for x, y in data:\n",
    "            _, a = self.feedforward(x)\n",
    "            a_L = a[-1]\n",
    "            j += np.sum(np.nan_to_num(y*np.log(a_L) + (1-y)*np.log(1-a_L)))\n",
    "        return -j / m\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid function use as activation function\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_grad(self, z):\n",
    "        \"\"\"\n",
    "        Result derivative of sigmoid function\n",
    "        \"\"\"\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data: 50000 / validation_data: 10000 / test_data: 10000\n",
      "Epoch 1 begin...\n",
      "Epoch 1 done: 3142.29335403\n",
      "Epoch 2 begin...\n",
      "Epoch 2 done: 45.204488039\n",
      "Epoch 3 begin...\n",
      "Epoch 3 done: 46.0244069099\n",
      "Epoch 4 begin...\n",
      "Epoch 4 done: 44.3906171322\n",
      "Epoch 5 begin...\n",
      "Epoch 5 done: 44.8469190598\n",
      "Epoch 6 begin...\n",
      "Epoch 6 done: 45.9303090572\n",
      "Epoch 7 begin...\n",
      "Epoch 7 done: 42.4124288559\n",
      "Epoch 8 begin...\n",
      "Epoch 8 done: 43.7103619576\n",
      "Epoch 9 begin...\n",
      "Epoch 9 done: 45.7618911266\n",
      "Epoch 10 begin...\n",
      "Epoch 10 done: 43.5981209278\n",
      "Epoch 11 begin...\n",
      "Epoch 11 done: 43.4115071297\n",
      "Epoch 12 begin...\n",
      "Epoch 12 done: 43.5397629738\n",
      "Epoch 13 begin...\n",
      "Epoch 13 done: 46.2944190502\n",
      "Epoch 14 begin...\n",
      "Epoch 14 done: 46.4924988747\n",
      "Epoch 15 begin...\n",
      "Epoch 15 done: 45.9197728634\n",
      "Epoch 16 begin...\n",
      "Epoch 16 done: 44.5280239582\n",
      "Epoch 17 begin...\n",
      "Epoch 17 done: 45.4846889973\n",
      "Epoch 18 begin...\n",
      "Epoch 18 done: 44.3462500572\n",
      "Epoch 19 begin...\n",
      "Epoch 19 done: 45.3814971447\n",
      "Epoch 20 begin...\n",
      "Epoch 20 done: 45.421173811\n",
      "Epoch 21 begin...\n",
      "Epoch 21 done: 44.5258398056\n",
      "Epoch 22 begin...\n",
      "Epoch 22 done: 44.7778131962\n",
      "Epoch 23 begin...\n",
      "Epoch 23 done: 46.6192789078\n",
      "Epoch 24 begin...\n",
      "Epoch 24 done: 44.2546701431\n",
      "Epoch 25 begin...\n",
      "Epoch 25 done: 44.8986859322\n",
      "Epoch 26 begin...\n",
      "Epoch 26 done: 43.3658089638\n",
      "Epoch 27 begin...\n",
      "Epoch 27 done: 50.0137379169\n",
      "Epoch 28 begin...\n",
      "Epoch 28 done: 44.2950241566\n",
      "Epoch 29 begin...\n",
      "Epoch 29 done: 45.0291910172\n",
      "Epoch 30 begin...\n",
      "Epoch 30 done: 45.4047529697\n",
      "Evaluation: 9662 / 10000 = 96.62%\n"
     ]
    }
   ],
   "source": [
    "import data_loader\n",
    "#from nn import NN\n",
    "\n",
    "# load data\n",
    "training_data, validation_data, test_data = data_loader.load()\n",
    "print('training_data: {0} / validation_data: {1} / test_data: {2}'.format(len(training_data), len(validation_data), len(test_data)))\n",
    "\n",
    "# run NN\n",
    "nn = NN([784, 100, 10])\n",
    "nn.train(training_data, 30, 100, 3.0)\n",
    "correct = nn.evaluate(test_data)\n",
    "total = len(test_data) \n",
    "print('Evaluation: {0} / {1} = {2}%'.format(correct, total, 100 * correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
