<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.41 with theme Tranquilpeak 0.4.1-BETA"><title>[NN] Mạng nơ-ron nhân tạo - Neural Networks</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Học Máy,Machine Learning,Neural Networks,nơ-ron nhân tạo,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2018/04/nn-intro/><meta name=description content="Mạng nơ-ron nhân tạo (Neural Network - NN) là một mô hình lập trình rất đẹp lấy cảm hứng từ mạng nơ-ron thần kinh. Kết hợp với các kĩ thuật học sâu (Deep Learning - DL), NN đang trở thành một công cụ rất mạnh mẽ mang lại hiệu quả tốt nhất cho nhiều bài toán khó như nhận dạng ảnh, giọng nói hay xử lý ngôn ngữ tự nhiên."><meta property=og:type content=website><meta property=og:title content="[NN] Mạng nơ-ron nhân tạo - Neural Networks"><meta property=og:url content=https://dominhhai.github.io/vi/2018/04/nn-intro/><meta property=og:description content="Mạng nơ-ron nhân tạo (Neural Network - NN) là một mô hình lập trình rất đẹp lấy cảm hứng từ mạng nơ-ron thần kinh. Kết hợp với các kĩ thuật học sâu (Deep Learning - DL), NN đang trở thành một công cụ rất mạnh mẽ mang lại hiệu quả tốt nhất cho nhiều bài toán khó như nhận dạng ảnh, giọng nói hay xử lý ngôn ngữ tự nhiên."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="[NN] Mạng nơ-ron nhân tạo - Neural Networks"><meta name=twitter:url content=https://dominhhai.github.io/vi/2018/04/nn-intro/><meta name=twitter:description content="Mạng nơ-ron nhân tạo (Neural Network - NN) là một mô hình lập trình rất đẹp lấy cảm hứng từ mạng nơ-ron thần kinh. Kết hợp với các kĩ thuật học sâu (Deep Learning - DL), NN đang trở thành một công cụ rất mạnh mẽ mang lại hiệu quả tốt nhất cho nhiều bài toán khó như nhận dạng ảnh, giọng nói hay xử lý ngôn ngữ tự nhiên."><meta name=twitter:creator content=@minhhai3b><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/#about><i class="sidebar-button-icon fa fa-lg fa-address-card"></i><span class=sidebar-button-desc>Thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[NN] Mạng nơ-ron nhân tạo - Neural Networks</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2018-04-23T10:20:14&#43;09:00>23 tháng 4, 2018</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Mạng nơ-ron nhân tạo (<em>Neural Network</em> - <strong>NN</strong>) là một mô hình lập trình rất đẹp lấy cảm hứng từ mạng nơ-ron thần kinh. Kết hợp với các kĩ thuật học sâu (<em>Deep Learning</em> - <strong>DL</strong>), NN đang trở thành một công cụ rất mạnh mẽ mang lại hiệu quả tốt nhất cho nhiều bài toán khó như nhận dạng ảnh, giọng nói hay xử lý ngôn ngữ tự nhiên.</p><p>Trong bài này, ta sẽ cùng tìm hiểu và cài đặt một NN cơ bản để làm nền tảng cho các bài về học sâu tiếp theo.</p><h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-perceptrons>1. Perceptrons</a><ul><li><a href=#1-1-perceptron-cơ-bản>1.1. Perceptron cơ bản</a></li><li><a href=#1-2-sigmoid-neurons>1.2. Sigmoid Neurons</a></li></ul></li><li><a href=#2-kiến-trúc-mạng-nn>2. Kiến trúc mạng NN</a></li><li><a href=#3-lan-truyền-tiến>3. Lan truyền tiến</a></li><li><a href=#4-học-với-mạng-nn>4. Học với mạng NN</a></li><li><a href=#5-lan-truyền-ngược-và-đạo-hàm>5. Lan truyền ngược và đạo hàm</a></li><li><a href=#6-tổng-kết>6. Tổng kết</a></li></ul></nav><h1 id=1-perceptrons>1. Perceptrons</h1><h2 id=1-1-perceptron-cơ-bản>1.1. Perceptron cơ bản</h2><p>Một mạng nơ-ron được cấu thành bởi các nơ-ron đơn lẻ được gọi là các <em>perceptron</em>. Nên trước tiên ta tìm hiểu xem perceptron là gì đã rồi tiến tới mô hình của mạng nơ-ron sau. Nơ-ron nhân tạo được lấy cảm hứng từ nơ-ron sinh học như hình mô tả bên dưới:</p><div class="figure center"><a class=fancybox href=https://cs231n.github.io/assets/nn1/neuron.png title="Nơ-ron sinh học. Source: https://cs231n.github.io/" data-fancybox-group><img class=fig-img src=https://cs231n.github.io/assets/nn1/neuron.png style=width:80% alt="Nơ-ron sinh học. Source: https://cs231n.github.io/"></a>
<span class=caption>Nơ-ron sinh học. Source: https://cs231n.github.io/</span></div><p>Như hình trên, ta có thể thấy một nơ-ron có thể nhận nhiều đầu vào và cho ra một kết quả duy nhất. Mô hình của perceptron cũng tương tự như vậy:</p><div class="figure center"><a class=fancybox href=http://neuralnetworksanddeeplearning.com/images/tikz0.png title=perceptron data-fancybox-group><img class=fig-img src=http://neuralnetworksanddeeplearning.com/images/tikz0.png alt=perceptron></a>
<span class=caption>perceptron</span></div><p>Một perceptron sẽ nhận một hoặc nhiều đầu $\mathbf{x}$ vào dạng nhị phân và cho ra một kết quả $o$ dạng nhị phân duy nhất. Các đầu vào được điều phối tầm ảnh hưởng bởi các tham số trọng lượng tương ứng $\mathbf{w}$ của nó, còn kết quả đầu ra được quyết định dựa vào một ngưỡng quyết định $b$ nào đó:</p><p>$$
o = \begin{cases}
0 &amp;\text{if }\displaystyle\sum_iw_ix_i \le \text{threshold}
\cr
1 &amp;\text{if }\displaystyle\sum_iw_ix_i &gt; \text{threshold}
\end{cases}
$$</p><p>Đặt $b=-\text{threshold}$, ta có thể viết lại thành:
$$
o = \begin{cases}
0 &amp;\text{if }\displaystyle\sum_iw_ix_i + b \le 0
\cr
1 &amp;\text{if }\displaystyle\sum_iw_ix_i + b &gt; 0
\end{cases}
$$</p><p>Để dễ hình dung, ta lấy ví dụ việc đi nhậu hay không phụ thuộc vào 4 yếu tố sau:</p><ul><li>1. Trời có nắng hay không?</li><li>2. Có hẹn trước hay không?</li><li>3. Vợ có vui hay không?</li><li>4. Bạn nhậu có ít khi gặp được hay không?</li></ul><p>Thì ta coi 4 yếu tố đầu vào là $x_1, x_2, x_3, x_4$ và nếu $o=0$ thì ta không đi nhậu còn $o=1$ thì ta đi nhậu. Giả sử mức độ quan trọng của 4 yếu tố trên lần lượt là $w_1=0.05, w_2=0.5, w_3=0.2, w_4=0.25$ và chọn ngưỡng $b=-0.5$ thì ta có thể thấy rằng việc trời nắng có ảnh hưởng chỉ 5% tới quyết định đi nhậu và việc có hẹn từ trước ảnh hưởng tới 50% quyết định đi nhậu của ta.</p><p>Nếu gắn $x_0=1$ và $w_0=b$, ta còn có thể viết gọn lại thành:
$$
o = \begin{cases}
0 &amp;\text{if }\mathbf{w}^{\intercal}\mathbf{x} \le 0
\cr
1 &amp;\text{if }\mathbf{w}^{\intercal}\mathbf{x} &gt; 0
\end{cases}
$$</p><h2 id=1-2-sigmoid-neurons>1.2. Sigmoid Neurons</h2><p>Với đầu vào và đầu ra dạng nhị phân, ta rất khó có thể điều chỉnh một lượng nhỏ đầu vào để đầu ra thay đổi chút ít, nên để linh động, ta có thể mở rộng chúng ra cả khoảng $[0, 1]$. Lúc này đầu ra được quyết định bởi một hàm sigmoid $\sigma(\mathbf{w}^{\intercal}\mathbf{x})$. Như các bài trước đã đề cập thì hàm sigmoid có công thức:
$$
\sigma(z) = \dfrac{1}{1+e^{-z}}
$$</p><p>Đồ thị của hàm này cũng cân xứng rất đẹp thể hiện được mức độ công bằng của các tham số:</p><div class="figure center"><a class=fancybox href=https://dominhhai.github.io/images/ml-20180423-nn_sigmoid_shape.svg title="Sigmoid Function" data-fancybox-group><img class=fig-img src=https://dominhhai.github.io/images/ml-20180423-nn_sigmoid_shape.svg alt="Sigmoid Function"></a>
<span class=caption>Sigmoid Function</span></div><p>Đặt $z = \mathbf{w}^{\intercal}\mathbf{x}$ thì công thức của perceptron lúc này sẽ có dạng:
$$
o = \sigma(z) = \dfrac{1}{1+\exp(-\mathbf{w}^{\intercal}\mathbf{x})}
$$</p><p>Tới đây thì ta có thể thấy rằng mỗi sigmoid neuron cũng tương tự như một bộ phân loại tuyến tính (<em>logistic regression</em>) bởi xác suất $P(y_i=1|x_i;\mathbf{w})=\sigma(\mathbf{w}^{\intercal}\mathbf{x})$.</p><p>Thực ra thì ngoài hàm sigmoid ra, ta còn có thể một số hàm khác như <a href=http://mathworld.wolfram.com/HyperbolicTangent.html>$\tanh$</a>, <a href=https://en.wikipedia.org/wiki/Rectifier_(neural_networks)>$\text{ReLU}$</a> để thay thế hàm sigmoid bởi dạng đồ thị của nó cũng tương tự như sigmoid. Một cách tổng quát, hàm perceptron được biểu diễn qua một <strong>hàm kích hoạt</strong> (<em><a href=https://en.wikipedia.org/wiki/Activation_function>activation function</a></em>) $f(z)$ như sau:
$$
o = f(z) = f(\mathbf{w}^{\intercal}\mathbf{x})
$$</p><p>Bằng cách biểu diễn như vậy, ta có thể <em>coi</em> neuron sinh học được thể hiện như sau:</p><div class="figure center"><a class=fancybox href=https://cs231n.github.io/assets/nn1/neuron_model.jpeg title="Mô hình Nơ-ron. . Source: https://cs231n.github.io/" data-fancybox-group><img class=fig-img src=https://cs231n.github.io/assets/nn1/neuron_model.jpeg style=width:80% alt="Mô hình Nơ-ron. . Source: https://cs231n.github.io/"></a>
<span class=caption>Mô hình Nơ-ron. . Source: https://cs231n.github.io/</span></div><p>Một điểm cần lưu ý là các <strong><span class="highlight-text green">hàm kích hoạt buộc phải là hàm phi tuyến</span></strong>. Vì nếu nó là tuyến tính thì khi kết hợp với phép toán tuyến tính $\mathbf{w}^{\intercal}\mathbf{x}$ thì kết quả thu được cũng sẽ là một thao tác tuyến tính dẫn tới chuyện nó trở nên vô nghĩa.</p><h1 id=2-kiến-trúc-mạng-nn>2. Kiến trúc mạng NN</h1><p>Mạng NN là sự kết hợp của của các tầng perceptron hay còn được gọi là perceptron đa tầng (<em>multilayer perceptron</em>) như hình vẽ bên dưới:</p><div class="figure center"><a class=fancybox href=https://cs231n.github.io/assets/nn1/neural_net.jpeg title="Neural Network. . Source: https://cs231n.github.io/" data-fancybox-group><img class=fig-img src=https://cs231n.github.io/assets/nn1/neural_net.jpeg alt="Neural Network. . Source: https://cs231n.github.io/"></a>
<span class=caption>Neural Network. . Source: https://cs231n.github.io/</span></div><p>Một mạng NN sẽ có 3 kiểu tầng:</p><ul><li><strong>Tầng vào</strong> (<em>input layer</em>): Là tầng bên trái cùng của mạng thể hiện cho các đầu vào của mạng.</li><li><strong>Tầng ra</strong> (<em>output layer</em>): Là tầng bên phải cùng của mạng thể hiện cho các đầu ra của mạng.</li><li><strong>Tầng ẩn</strong> (<em>hidden layer</em>): Là tầng nằm giữa tầng vào và tầng ra thể hiện cho việc suy luận logic của mạng.</li></ul><p>Lưu ý rằng, một NN chỉ có 1 tầng vào và 1 tầng ra nhưng có thể có nhiều tầng ẩn.</p><div class="figure center"><a class=fancybox href=https://cs231n.github.io/assets/nn1/neural_net2.jpeg title="NN - 2 hidden layer. . Source: https://cs231n.github.io/" data-fancybox-group><img class=fig-img src=https://cs231n.github.io/assets/nn1/neural_net2.jpeg style=width:80% alt="NN - 2 hidden layer. . Source: https://cs231n.github.io/"></a>
<span class=caption>NN - 2 hidden layer. . Source: https://cs231n.github.io/</span></div><p>Trong mạng NN, mỗi nút mạng là một sigmoid nơ-ron nhưng hàm kích hoạt của chúng có thể khác nhau. Tuy nhiên trong thực tế người ta thường để chúng cùng dạng với nhau để tính toán cho thuận lợi.</p><p>Ở mỗi tầng, số lượng các nút mạng (nơ-ron) có thể khác nhau tuỳ thuộc vào bài toán và cách giải quyết. Nhưng thường khi làm việc người ta để các tầng ẩn có số lượng nơ-ron bằng nhau. Ngoài ra, các nơ-ron ở các tầng thường được liên kết đôi một với nhau tạo thành <strong>mạng kết nối đầy đủ</strong> (<em>full-connected network</em>). Khi đó ta có thể tính được kích cỡ của mạng dựa vào số tầng và số nơ-ron. Ví dụ ở hình trên ta có:</p><ul><li>$4$ tầng mạng, trong đó có $2$ tầng ẩn</li><li>$3+4*2+1=12$ nút mạng</li><li>$(3*4+4*4+4*1)+(4+4+1)=41$ tham số</li></ul><h1 id=3-lan-truyền-tiến>3. Lan truyền tiến</h1><p>Như bạn thấy thì tất cả các nốt mạng (nơ-ron) được kết hợp đôi một với nhau theo một chiều duy nhất từ tầng vào tới tầng ra. Tức là mỗi nốt ở một tầng nào đó sẽ nhận đầu vào là tất cả các nốt ở tầng trước đó mà không suy luận ngược lại. Hay nói cách khác, việc suy luận trong mạng NN là <strong>suy luận tiến</strong> (<em>feedforward</em>):</p><p>$$
\begin{aligned}
z^{(l+1)}_i &amp;= \displaystyle\sum_{j=1}^{n^{(l)}} w^{(l+1)}_{ij}a^{(l)}_j + b^{(l+1)}_i
\cr
a_i^{(l+1)} &amp;= f\big(z^{(l+1)}_i\big)
\end{aligned}
$$</p><p>Trong đó, $n^{(l)}$ số lượng nút ở tầng $l$ tương ứng và $a^{(l)}_j$ là nút mạng thứ $j$ của tầng $l$. Còn $w^{(l+1)}_{ij}$ là tham số trọng lượng của đầu vào $a^{(l)}_j$ đối với nút mạng thứ $i$ của tầng $l+1$ và $b^{(l+1)}_i$ là độ lệch (<em>bias</em>) của nút mạng thứ $i$ của tầng $l+1$. Đầu ra của nút mạng này được biểu diễn bằng $a_i^{(l+1)}$ ứng với hàm kích hoạt $f(z_i)$ tương ứng.</p><blockquote><p>Riêng với tầng vào, thông thường $\mathbf{a}^{(1)}$ cũng chính là các đầu vào $\mathbf{x}$ tương ứng của mạng.</p></blockquote><p>Để tiện tính toán, ta coi $a^{(l)}_0$ là một đầu vào và $w^{(l+1)}_{i0}=b^{(l+1)}_i$ là tham số trọng lượng của đầu vào này. Lúc đó ta có thể viết lại công thức trên dưới dạng véc-tơ:</p><p>$$
\begin{aligned}
z^{(l+1)}_i &amp;= \mathbf{w}^{(l+1)}_i\cdot\mathbf{a}^{(l)}
\cr
a_i^{(l+1)} &amp;= f\big(z^{(l+1)}_i\big)
\end{aligned}
$$</p><p>Nếu nhóm các tham số của mỗi tầng thành một ma trận có các cột tương ứng với tham số mỗi nút mạng thì ta có thể tính toán cho toàn bộ các nút trong một tầng bằng véc-tơ:
$$
\begin{aligned}
\mathbf{z}^{(l+1)} &amp;= \mathbf{W}^{(l+1)}\cdot\mathbf{a}^{(l)}
\cr
\mathbf{a}^{(l+1)} &amp;= f\big(\mathbf{z}^{(l+1)}\big)
\end{aligned}
$$</p><h1 id=4-học-với-mạng-nn>4. Học với mạng NN</h1><p>Cũng tương tự như các bài toán học máy khác thì quá trình học vẫn là tìm lấy một hàm lỗi để đánh giá và tìm cách tối ưu hàm lỗi đó để được kết quả hợp lý nhất có thể. Như đã đề cập mỗi nút mạng của NN có thể coi là một bộ phân loại (<em>logistic regression</em>) có hàm lỗi là:</p><p>$$
J(\mathbf{W}) = -\frac{1}{m}\sum_{i=1}^m\Bigg(y^{(i)}\log\Big(\sigma^{(i)}\Big)+\Big(1-y^{(i)}\Big)\log\Big(1-\sigma^{(i)}\Big)\Bigg)
$$</p><p>Trong đó, $m$ là số lượng dữ liệu huấn luyện, $y^{(i)}$ là đầu ra thực tế của dữ liệu thứ $i$ trong tập huấn luyện. Còn $\sigma^{(i)}$ là kết quả ước lượng được ứng với dữ liệu thứ $i$.</p><p>Hàm lỗi của NN cũng tương tự như vậy, chỉ khác là đầu ra của mạng NN có thể có nhiều nút nên khi tính đầu ra ta cũng cần phải tính cho từng nút ra đó. Giả sử số nút ra là $K$ và $y_k$ là đầu ra thực tế của nút thứ $k$, còn $\sigma_k$ là đầu ra ước lượng được cho nút thứ $k$ tương ứng. Khi đó, công thức tính hàm lỗi sẽ thành:</p><p>$$
J(\mathbb{W}) = -\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K\Bigg(y_k^{(i)}\log\Big(\sigma_k^{(i)}\Big)+\Big(1-y_k^{(i)}\Big)\log\Big(1-\sigma_k^{(i)}\Big)\Bigg)
$$</p><p>Lưu ý rằng, các tham số lúc này không còn đơn thuần là một ma trận nữa mà là một tập của tất cả các ma trận tham số của tất cả các tầng mạng nên tôi biểu diễn nó dưới dạng tập hợp $\mathbb{W}$.</p><p>Để tối ưu hàm lỗi ta vẫn sử dụng các <a href=https://dominhhai.github.io/vi/2017/12/ml-gd/>phương pháp đạo hàm</a> như đã đề cập ở các bài viết trước. Nhưng việc tính đạo hàm lúc này không đơn thuần như logistic regression bởi để ước lượng được đầu ra ta phải trải qua quá trình lan truyền tiến. Tức là để tính được $\sigma_k$ ta cần một loạt các phép tính liên hợp nhau.</p><h1 id=5-lan-truyền-ngược-và-đạo-hàm>5. Lan truyền ngược và đạo hàm</h1><p>Để tính đạo hàm của hàm lỗi $\nabla J(\mathbb{W})$ trong mạng NN, ta sử dụng một giải thuật đặc biệt là giải thuật <strong>lan truyền ngược</strong> (<em>backpropagation</em>). Nhờ có giải thuật được sáng tạo vào năm 1986 này mà mạng NN thực thi hiệu quả được và ứng dụng ngày một nhiều cho tới tận ngày này.</p><p>Về cơ bản phương pháp này được dựa theo <a href=https://dominhhai.github.io/vi/2017/10/multi-var-func/#5-%C4%91%E1%BA%A1o-h%C3%A0m-ri%C3%AAng-c%E1%BB%A7a-h%C3%A0m-h%E1%BB%A3p>quy tắc chuỗi đạo hàm của hàm hợp</a> và phép tính ngược đạo hàm để thu được đạo hàm theo tất cả các tham số cùng lúc chỉ với 2 lần duyệt mạng. Tuy nhiên trong bài viết này, tôi chỉ đề cập ngay tới công thức tính toán còn việc chứng minh thì tôi sẽ dành cho <a href=https://dominhhai.github.io/vi/2018/04/nn-bp/>các bài tiếp theo</a>.</p><p>Giải thuật lan truyền ngược được thực hiện như sau:</p><ul><li><p><strong>1. Lan truyền tiến</strong>:<br>Lần lượt tính các $\mathbf{a}^{(l)}$ từ $l=2\rightarrow L$ theo công thức:
$$
\begin{aligned}
&amp;\mathbf{z}^{(l)}=\mathbf{W}^{(l)}\cdot\mathbf{a}^{(l-1)}
\cr
&amp;\mathbf{a}^{(l)}=f(\mathbf{z}^{(l)})
\end{aligned}
$$
Trong đó, tầng vào $\mathbf{a}^{(1)}$ chính bằng giá trị vào của mạng $\mathbf{x}$.</p></li><li><p><strong>2. Tính đạo hàm theo $z$ ở tầng ra</strong>:
$$\dfrac{\partial{J}}{\partial{\mathbf{z}^{(L)}}} = \dfrac{\partial{J}}{\partial{\mathbf{a}^{(L)}}}\dfrac{\partial{\mathbf{a}^{(L)}}}{\partial{\mathbf{z}^{(L)}}}$$
với $\mathbf{a}^{(L)}, \mathbf{z}^{(L)}$ vừa tính được ở bước 1.</p></li><li><p><strong>3. Lan truyền ngược</strong>:<br>Tính đạo hàm theo $z$ ngược lại từ $l=(L-1)\rightarrow 2$ theo công thức:
$$
\begin{aligned}
\dfrac{\partial{J}}{\partial{\mathbf{z}^{(l)}}} &amp;= \dfrac{\partial{J}}{\partial{\mathbf{z}^{(l+1)}}}\dfrac{\partial{\mathbf{z}^{(l+1)}}}{\partial{\mathbf{a}^{(l)}}}\dfrac{\partial{\mathbf{a}^{(l)}}}{\partial{\mathbf{z}^{(l)}}}
\cr
&amp; = \bigg(\big(\mathbf{W}^{(l+1)}\big)^{\intercal}\dfrac{\partial{J}}{\partial{\mathbf{z}^{(l+1)}}}\bigg)\dfrac{\partial{\mathbf{a}^{(l)}}}{\partial{\mathbf{z}^{(l)}}}
\end{aligned}
$$
với $\mathbf{z}^{(l)}$ tính được ở bước 1 và $\dfrac{\partial{J}}{\partial{\mathbf{z}^{(l+1)}}}$ tính được ở vòng lặp ngay trước.</p></li><li><p><strong>4. Tính đạo hàm</strong>:<br>Tính đạo hàm theo tham số $w$ bằng công thức:
$$
\begin{aligned}
\dfrac{\partial{J}}{\partial{\mathbf{W}^{(l)}}} &amp;= \dfrac{\partial{J}}{\partial{\mathbf{z}^{(l)}}}\dfrac{\partial{\mathbf{z}^{(l)}}}{\partial{\mathbf{W}^{(l)}}}
\cr
&amp; = \dfrac{\partial{J}}{\partial{\mathbf{z}^{(l)}}}\big(\mathbf{a}^{(l-1)}\big)^{\intercal}
\end{aligned}
$$
với $\mathbf{a}^{(l-1)}$ tính được ở bước 1 và $\dfrac{\partial{J}}{\partial{\mathbf{z}^{(l)}}}$ tính được ở bước 3.</p></li></ul><h1 id=6-tổng-kết>6. Tổng kết</h1><p>Lấy cảm hứng từ mạng nơ-ron sinh học, mạng NN được hình thành từ các tầng nơ-ron nhân tạo. Mạng NN gồm 3 kiểu tầng chính là <strong>tầng vào</strong> (<em>input layer</em>) biểu diễn cho đầu vào, <strong>tầng ra</strong> (<em>output layer</em>) biểu diễn cho kết quả đầu ra và <strong>tầng ẩn</strong> (<em>hidden layer</em>) thể hiện cho các bước suy luận trung gian. Mỗi nơ-ron sẽ nhận tất cả đầu vào từ các nơ-ron ở tầng trước đó và sử dụng một <strong>hàm kích hoạt dạng</strong> (<em>activation function</em>) phi tuyến như <em>sigmoid</em>, <em>ReLU</em>, <em>tanh</em> để tính toán đầu ra.</p><p>Quá trình suy luận từ tầng vào tới tầng ra của mạng NN là quá trình <strong>lan truyền tiến</strong> (<em>feedforward</em>), tức là đầu vào các nơ-ron tại 1 tầng đều lấy từ kết quả các nơ-ron tầng trước đó mà không có quá trình suy luận ngược lại.</p><p>$$
\begin{aligned}
\mathbf{z}^{(l+1)} &amp;= \mathbf{W}^{(l+1)}\cdot\mathbf{a}^{(l)}
\cr
\mathbf{a}^{(l+1)} &amp;= f\big(\mathbf{z}^{(l+1)}\big)
\end{aligned}
$$</p><p>Hàm lỗi của mạng cũng tương tự như logistic regression có dạng cross-entropy, tuy nhiên khác logistic regression ở chỗ mạng NN có nhiều đầu ra nên hàm lỗi cũng phải lấy tổng lỗi của tất cả các đầu ra này:</p><p>$$
J(\mathbb{W}) = -\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K\Bigg(y_k^{(i)}\log\Big(\sigma_k^{(i)}\Big)+\Big(1-y_k^{(i)}\Big)\log\Big(1-\sigma_k^{(i)}\Big)\Bigg)
$$</p><p>Để tối ưu được hàm lỗi $J(\mathbb{W})$ này người ta sử dụng giải thuật <strong>lan truyền ngược</strong> (<em>backpropagation</em>) để tính được đạo hàm của hàm lỗi này.</p><p>$$
\begin{aligned}
\dfrac{\partial{J}}{\partial{\mathbf{z}^{(L)}}} &amp;= \dfrac{\partial{J}}{\partial{\mathbf{a}^{(L)}}}\dfrac{\partial{\mathbf{a}^{(L)}}}{\partial{\mathbf{z}^{(L)}}}
\cr
\dfrac{\partial{J}}{\partial{\mathbf{z}^{(l)}}} &amp;= \bigg(\big(\mathbf{W}^{(l+1)}\big)^{\intercal}\dfrac{\partial{J}}{\partial{\mathbf{z}^{(l+1)}}}\bigg)\dfrac{\partial{\mathbf{a}^{(l)}}}{\partial{\mathbf{z}^{(l)}}}
\cr
\dfrac{\partial{J}}{\partial{\mathbf{W}^{(l)}}} &amp;= \dfrac{\partial{J}}{\partial{\mathbf{z}^{(l)}}}\big(\mathbf{a}^{(l-1)}\big)^{\intercal}
\end{aligned}
$$</p><p>Bài viết giới thiệu này cơ bản đã trình bài khái niệm và các lý thuyết cơ bản của một mạng NN, còn cách cài đặt ra sao thì bạn có thể đón đọc ở bài tiếp theo nhé.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a>
<a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/nn/>NN</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/04/matrix-derivs/ data-tooltip="[Giải Tích] Đạo hàm với vec-tơ, ma trận, ten-xơ"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/04/git-clone-tag/ data-tooltip="[Git] Lấy mã nguồn theo tag"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2018/04/nn-intro/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2018/04/nn-intro/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2018/04/nn-intro/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2018 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/04/matrix-derivs/ data-tooltip="[Giải Tích] Đạo hàm với vec-tơ, ma trận, ten-xơ"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/04/git-clone-tag/ data-tooltip="[Git] Lấy mã nguồn theo tag"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2018/04/nn-intro/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2018/04/nn-intro/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2018/04/nn-intro/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2018%2F04%2Fnn-intro%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2018%2F04%2Fnn-intro%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2018%2F04%2Fnn-intro%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2018\/04\/nn-intro\/';this.page.identifier='\/vi\/2018\/04\/nn-intro\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>