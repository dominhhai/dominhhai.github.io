<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.40.2 with theme Tranquilpeak 0.4.1-BETA"><title>[NN] Cài đặt mạng NN</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Học Máy,Machine Learning,backpropagation,lan truyền ngược,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2018/04/nn-implement/><meta name=description content="Bài viết này sẽ tập trung vào việc cài đặt mạng NN để nhận dạng số và đưa ra một số mẹo để thu được kết quả tốt khi làm việc với mạng NN. Nếu bạn chưa có cái nhìn tổng quan về mặt lý thuyết của mạng NN thì tôi nghĩ rằng bạn nên đọc bài viết trước của tôi để có thể dễ dàng hiểu bài này hơn."><meta property=og:type content=website><meta property=og:title content="[NN] Cài đặt mạng NN"><meta property=og:url content=https://dominhhai.github.io/vi/2018/04/nn-implement/><meta property=og:description content="Bài viết này sẽ tập trung vào việc cài đặt mạng NN để nhận dạng số và đưa ra một số mẹo để thu được kết quả tốt khi làm việc với mạng NN. Nếu bạn chưa có cái nhìn tổng quan về mặt lý thuyết của mạng NN thì tôi nghĩ rằng bạn nên đọc bài viết trước của tôi để có thể dễ dàng hiểu bài này hơn."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="[NN] Cài đặt mạng NN"><meta name=twitter:url content=https://dominhhai.github.io/vi/2018/04/nn-implement/><meta name=twitter:description content="Bài viết này sẽ tập trung vào việc cài đặt mạng NN để nhận dạng số và đưa ra một số mẹo để thu được kết quả tốt khi làm việc với mạng NN. Nếu bạn chưa có cái nhìn tổng quan về mặt lý thuyết của mạng NN thì tôi nghĩ rằng bạn nên đọc bài viết trước của tôi để có thể dễ dàng hiểu bài này hơn."><meta name=twitter:creator content=@minhhai3b><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/#about><i class="sidebar-button-icon fa fa-lg fa-address-card"></i><span class=sidebar-button-desc>Thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[NN] Cài đặt mạng NN</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2018-04-26T10:20:14&#43;09:00>26 tháng 4, 2018</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Bài viết này sẽ tập trung vào việc cài đặt mạng NN để nhận dạng số và đưa ra một số mẹo để thu được kết quả tốt khi làm việc với mạng NN. Nếu bạn chưa có cái nhìn tổng quan về mặt lý thuyết của mạng NN thì tôi nghĩ rằng bạn nên đọc <a href=https://dominhhai.github.io/vi/2018/04/nn-intro/>bài viết trước</a> của tôi để có thể dễ dàng hiểu bài này hơn.<h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-giới-thiệu>1. Giới thiệu</a><ul><li><a href=#1-1-nhận-dạng-số>1.1. Nhận dạng số</a></li><li><a href=#1-2-mô-hình-mạng>1.2. Mô hình mạng</a></li></ul></li><li><a href=#2-phân-tích-và-cài-đặt>2. Phân tích và cài đặt</a><ul><li><a href=#2-1-dữ-liệu>2.1. Dữ liệu</a></li><li><a href=#2-2-khởi-tạo-tham-số>2.2. Khởi tạo tham số</a></li><li><a href=#2-3-lan-truyền-tiến>2.3. Lan truyền tiến</a></li><li><a href=#2-4-hàm-lỗi>2.4. Hàm lỗi</a></li><li><a href=#2-5-lan-truyền-ngược>2.5. Lan truyền ngược</a></li><li><a href=#2-6-kiểm-tra-đạo-hàm>2.6. Kiểm tra đạo hàm</a></li><li><a href=#2-7-huấn-luyện>2.7. Huấn luyện</a></li></ul></li><li><a href=#3-bàn-luận>3. Bàn luận</a></li></ul></nav></p><h1 id=1-giới-thiệu>1. Giới thiệu</h1><h2 id=1-1-nhận-dạng-số>1.1. Nhận dạng số</h2><p>Nhận dạng số viết tay dường như là một bài toán khó nếu áp dụng các phương pháp lập trình logic thông dụng. Tuy nhiên, bằng mạng NN ta có thể thực hiện việc này với tỉ lệ chính xác rất cao. Thông thường một bài toán nhận dạng quang học gồm có 1 số bước chính:</p><ul><li>1. Nâng cao chất lượng ảnh</li><li>2. Phân tách các kí tự</li><li>3. Nhận dạng các kí tự</li></ul><p>Các kĩ thuật xử lý ảnh như nhị phân hoá bức ảnh hiện nay giúp ta thu được ảnh có độ tương phản cao giữa nên và chữ giúp cho việc bóc tách chữ dễ dàng hơn. Để có thể nhận dạng được, thì việc cần thiết là phải bóc tách từng kí tự ra một, điều may mắn là các kí tự gần như có khung gần tương tự nhau, giúp ta có thể biết được biên giữa các kí tự. Sau khi bóc tầng kí tự ra rồi thì ta tiến hành bước nhận dạng từng kí tự đơn đó rồi ghép lại với nhau thành văn bản đầy đủ. Về cơ bản là vậy, tuy nhiên trong phần này tôi chỉ chú trọng tới bước thứ 3 là nhận dạng kí tự đơn lẻ, cụ thể là nhận dạng số từ $0$ tới $9$ bằng mạng NN.</p><p>Cũng như các bài toán học máy khác thì yêu cầu đầu tiên là phải có tập dữ liệu. Thật may mắn là ta có sẵn tập dữ liệu <a href=http://yann.lecun.com/exdb/mnist/>MNIST</a> chứa 70,000 ảnh số viết tay kích thước 28x28 pixels đi kèm với nhãn từ $0$ tới $9$.</p><div class="figure center"><a class=fancybox href=https://res.cloudinary.com/dominhhai/image/upload/ml/mnist.png title="MNIST Database Sample" data-fancybox-group><img class=fig-img src=https://res.cloudinary.com/dominhhai/image/upload/ml/mnist.png style=width:60% alt="MNIST Database Sample"></a>
<span class=caption>MNIST Database Sample</span></div><p>Trong đó có 60,000 dữ liệu huấn luyện và 10,000 dữ liệu kiểm tra. Dữ liệu chính thức được chia làm 4 file với đặt tả cụ thể. Tuy nhiên để tiện làm việc tôi lấy <a href=https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz>file nén gộp</a> của chúng lại cùng 1 định dạng duy nhất. Trong file này, dữ liệu được tách ra làm 3 phần: (1) 50,000 dữ liệu huấn luyện - <em>training data</em>; (2) 10,000 dữ liệu kiểm định - <em>validation data</em>; (3) 10,000 dữ liệu kiểm tra - <em>test data</em>. Mỗi dữ liệu được tổ chức thành các cặp 2-tuple gồm 1 mảng 784 chiều chứ ảnh mẫu 28x28 pixels và nhãn tương ứng trong khoảng $0$ tới $9$.</p><h2 id=1-2-mô-hình-mạng>1.2. Mô hình mạng</h2><p>Đầu vào của mạng, ta có thể sử dụng mỗi điểm ảnh cho 1 đầu vào, hay nói cách khác đầu vào của ta gồm 784 nút mạng mỗi nút chứ giá trị của 1 điểm ảnh. Về mặt kĩ thuật, ta có thể coi đầu vào là 1 véc-tơ cột 784 chiều với mỗi phần từ chứa giá trị 1 điểm ảnh.</p><p>Còn đầu ra đại diện cho các số từ $0$ tới $9$. Về nguyên tắc ta có thể sử dụng 4 nút ra để mã hoá cho 10 nhãn đó bởi $4^2=16&gt;10$, tuy nhiên nếu làm như vậy thì khó mà nhìn được xác suất dự đoán tương ứng vỡi mỗi nhãn là bao nhiêu. Nên trong bài toán này ta sử dụng 10 đầu ra tương ứng với 10 nhãn. Mỗi đầu ra sẽ nhận giá trị trong khoảng $[0, 1]$ tương ứng với xác suất dự đoán ở mỗi nhãn. Như vậy, trường hợp chắc chắn đúng thì vì trị tương ứng sẽ bằng 1 và các vị trí khác bằng 0. Về mặt thuật ngữ, nó chính là vec-to <strong><a href=https://en.wikipedia.org/wiki/One-hot>one-hot</a></strong>. Ví dụ, nếu đầu ra là $5$ thì phần tử ở vị trí thứ 6 là 1 còn các vị trí khác là 0:</p><p>$$\begin{bmatrix}0&amp;0&amp;0&amp;0&amp;0&amp;\textcolor{red}{1}&amp;0&amp;0&amp;0&amp;0\end{bmatrix}^{\intercal}$$</p><p>Trong bài toán này, ta sẽ sử dụng hàm <em>sigmoid</em> làm hàm kích hoạt cho các nút mạng và hàm lỗi tương ứng là cross-entropy như đã đề cập ở <a href=https://dominhhai.github.io/vi/2018/04/nn-intro/>bài viết trước</a>. Dạng bài toán này, người ta còn hay sử dụng <a href=https://dominhhai.github.io/vi/2018/04/softmax-derivs/>hàm softmax</a> để làm hàm kích hoạt cho tầng ra của mạng do kết quả của hàm này tương tự như phép lấy xác suất. Việc cài đặt hàm này cũng tương tự như hàm <em>sigmoid</em>, tuy nhiên để đơn giản và phù hợp với mục đích của bài là nói về cách cài đặt mạng NN, tôi không cài đặt ở đây.</p><h1 id=2-phân-tích-và-cài-đặt>2. Phân tích và cài đặt</h1><p>Với mô hình mạng và dữ liệu như trên ta bắt đầu tiến hành cài đặt mạng. Thay vì cố định kích cỡ mạng, tôi tạo ra 1 lớp mạng riêng để dễ dàng cấu hình kích thước gồm số tầng và số nút mỗi tầng.</p><p>Toàn bộ mã nguồn của phần này bạn có thể xem <a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/network.ipynb>tại đây</a>. Tôi có viết toàn bộ bằng 1 file iPython cho dễ theo dõi, tuy nhiên bạn nếu muốn bạn có thể copy nội dung ra file python thông dụng để chạy mà không cần chỉnh sửa gì cả.</p><h2 id=2-1-dữ-liệu>2.1. Dữ liệu</h2><p>Như đã đề cập ở trên, để có thể dự đoán kết quả đầu vào của ta là một véc-tơ cột 784 chiều. Nên các dữ liệu đầu vào ta cũng phải để dạng này cho phù hợp. Đầu ra của mạng ở dạng vec-tơ cột one-hot 10 chiều nên các nhãn cũng cần biến đổi về dạng vec-tơ tương ứng này. Tuy nhiên, với tập dữ liệu kiểm tra, ta không cần biến đổi các nhãn thành vec-tơ one-hot này bởi kết quả dự đoán của mạng dù thế nào ta vẫn phải quy đổi sang nhãn tương ứng dạng không mã hoá.</p><figure class="highlight python language-python"><figcaption><span>data_loader.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/data_loader.py target=_blank rel=external>data_loader.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>def load():
  # download data if not exist
  if not os.path.exists(DATA_FILE):
    download()
  
  # load data
  with gzip.open(DATA_FILE, &#39;rd&#39;) as file:
    tr_dt, v_dt, t_dt = cPickle.load(file)
  
  # training data
  inputs = [x.reshape((784, 1)) for x in tr_dt[0]]
  labels = [label_2_vec(y) for y in tr_dt[1]]
  training_data = zip(inputs, labels)
  
  # validation data
  inputs = [x.reshape((784, 1)) for x in v_dt[0]]
  validation_data = zip(inputs, v_dt[1])
  
  # test data
  inputs = [x.reshape((784, 1)) for x in t_dt[0]]
  test_data = zip(inputs, t_dt[1])
  
  return (training_data, validation_data, test_data)</code></pre></td></tr></tbody></table></figure><p>Hàm <code>load()</code> trên sẽ cho ta 3 tập dữ liệu riêng biệt gồm <em>tập huấn luyện</em>, <em>tập kiểm định</em> và <em>tập kiểm tra</em> tương ứng. Tập dữ liệu là 1 mảng các <code>2-tuples</code> chứa vec-tơ 784 chiều đầu vào $x\in\mathbb{R}^{784,1}$ và nhãn $y\in\mathbb{R}^{10,1}$ tương ứng dạng vec-to one-hot đầu ra. Còn 2 tập kiểm định và kiểm tra là các mảng <code>2-tuples</code> chứa vec-tơ 784 chiều đầu vào $x\in\mathbb{R}^{784,1}$ và nhãn $y\in\mathbb{N}$ tương ứng dạng số nguyên trong khoảng $[0,9]$.</p><h2 id=2-2-khởi-tạo-tham-số>2.2. Khởi tạo tham số</h2><p>Với mạng NN, giá trị tham số khởi tạo ảnh hưởng trực tiếp tới việc mạng có chạy được hay không. Khác với các bài toán <a href=https://dominhhai.github.io/vi/categories/ml/>học máy</a> tôi đã đề cập thì ta <strong><span class="highlight-text danger">không thể khởi tạo tham số của mạng bằng $0$ được</span></strong>. Bởi khi đó, đạo hàm của ta sẽ bị triệt tiêu dẫn tới tham số không thay đổi được nên mạng không thể học được. Việc chứng minh điều nay không hề khó nếu bạn xem lại <a href=https://dominhhai.github.io/vi/2018/04/nn-intro/#5-lan-truyền-ngược-và-đạo-hàm>công thức tính đạo hàm</a> trong mạng NN ở bài viết trước của tôi.</p><p>Cũng thật khó để quyết định xem nên chọn việc khởi tạo sao cho hợp lý, tuy nhiên về nguyên tắc thì tất cả các tham số không thể bằng $0$. Trong bài toán này để đơn giản tôi chọn cách khởi tạo tham số ngẫu nhiên:</p><figure class="highlight python language-python"><figcaption><span>nn.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn.py target=_blank rel=external>nn.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def __init__(self, layers):
        self.layers = layers
        self.L = len(layers)
        self.w = [np.random.randn(l2, l1 &#43; 1)
                        for l2, l1 in zip(layers[1:], layers[:-1])]</code></pre></td></tr></tbody></table></figure><p>Để khởi tạo lớp mạng <code>NN</code>, ta cấu hình kích cỡ mạng bằng 1 mảng <code>layers</code> với số nút ở mỗi tầng tương ứng. Ví dụ, <code>layers = [784, 100, 200, 10]</code> thì ta có mạng gồm 4 tầng với tầng vào gồm 784 nút, tầng ra gồm 10 nút và 2 tầng ẩn có lần lượt là 100 và 200 nút mạng.</p><p>Trọng số <code>w</code> của mạng sẽ là 1 mảng có kích cỡ bằng số tầng, không kể tầng vào của mạng. Mỗi phần tử của mảng <code>w</code> chứ ma trận $\mathbf W$ trọng số tương ứng của mỗi tầng mạng (từ tầng 2 tới tầng ra). Mỗi hàng của ma trận $\mathbf W$ thể hiện cho nút mạng tương ứng ở tầng đó còn mỗi cột thể hiện cho trọng số của các đầu vào tầng trước đó.</p><p>Như quy ước ở bài trước, thì các giá trị độ lệch (<em>bias</em>) $b^{(l)}$ ta sẽ coi như trọng số $w_0^{(l)}$ tương ứng: $w_0^{(l)}=b^{(l)}$. Nên cột đầu của ma trận $\mathbf W$ sẽ đại diện cho các giá trị bias. Ví dụ, tầng 2 có 100 nút và tầng 3 có 200 nút thì ma trận $w[2]$ ở tầng 3 của ta có kích cỡ $[200\times 101]$.</p><h2 id=2-3-lan-truyền-tiến>2.3. Lan truyền tiến</h2><p>Dựa vào công thức lan truyền tiến (<em>feedfoward</em>) ở bài trước:</p><p>$$
\begin{aligned}
\mathbf{z}^{(l+1)} &amp;= \mathbf{W}^{(l+1)}\cdot\mathbf{a}^{(l)}
\cr
\mathbf{a}^{(l+1)} &amp;= f\big(\mathbf{z}^{(l+1)}\big)
\end{aligned}
$$</p><p>Ta có thể dễ dàng cài đặt hàm này như sau:</p><figure class="highlight python language-python"><figcaption><span>nn.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn.py target=_blank rel=external>nn.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def feedforward(self, x):
        z = []
        a = [self.add_bias(x)]
        for l in range(1, self.L):
            z_l = np.dot(self.w[l-1], a[l-1])
            a_l = self.sigmoid(z_l)
            if l &lt; self.L - 1:
                a_l = self.add_bias(a_l)
            z.append(z_l)
            a.append(a_l) 
        return (z, a)

    def add_bias(self, a):
        &#34;&#34;&#34;
        add a_0 = 1 as input for bias w_0
        &#34;&#34;&#34;
        return np.insert(a, 0, 1, axis=0)

    def sigmoid(self, z):
        &#34;&#34;&#34;
        Sigmoid function use as activation function
        &#34;&#34;&#34;
        return 1.0 / (1.0 &#43; np.exp(-z))</code></pre></td></tr></tbody></table></figure><p>Để tiện sử dụng về sau, ta lưu lại tất cả các giá trị $z, a$ trung gian trong quá trình tính toán. Như quy ước thì $a_0$ là gắn bằng đầu vào của mạng có kèm thêm đầu vào bias là $1$ nên mảng <code>a</code> sẽ có kích cỡ bằng số tầng mạng. Còn mảng <code>z</code> có kích thước đúng bằng mảng trọng số <code>w</code> thể hiện cho véc-tơ $\mathbf z$ ở mỗi tầng mạng.</p><p>Ngoài ra, tương tự như tầng vào, tại các tầng ẩn, ta gắn thêm đầu vào bias bằng $1$ để làm đầu vào cho tầng sau.</p><p>Dễ dàng ta thấy rằng phần tử cuối cùng của mảng <code>a</code> chính là vec-tơ đầu ra của mạng: <code>a[-1]</code>. Dựa vào đây, ta có thể biết được nhãn dự đoán được bằng cách lấy địa chỉ của phần tử có giá trị lớn nhất (tương ứng với xác suất lớn nhất):<figure class="highlight python language-python"><figcaption><span>nn.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn.py target=_blank rel=external>nn.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def predict(self, x):
        _, a = self.feedforward(x)
        return np.argmax(a[-1])</code></pre></td></tr></tbody></table></figure></p><h2 id=2-4-hàm-lỗi>2.4. Hàm lỗi</h2><p>Với hàm lỗi cross-entropy:
$$
J(\mathbb{W}) = -\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K\Bigg(y_k^{(i)}\log\Big(\sigma_k^{(i)}\Big)+\Big(1-y_k^{(i)}\Big)\log\Big(1-\sigma_k^{(i)}\Big)\Bigg)
$$</p><p>Trong đó, $\sigma_k^{(i)}$ là đầu ra của nút thứ $k$ ở tầng ra tương ứng với dữ liệu huấn luyện thứ $i$. Hay nói cách khác chính là $a_k^{(L)}$. Bằng công thức đó, ta cài đặt được như sau:</p><figure class="highlight python language-python"><figcaption><span>nn.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn.py target=_blank rel=external>nn.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def cost(self, data):
        &#34;&#34;&#34;
        Return cross-entropy cost of NN on test data
        &#34;&#34;&#34;
        m = len(data)
        j = 0
        for x, y in data:
            _, a = self.feedforward(x)
            a_L = a[-1]
            j &#43;= np.sum(np.nan_to_num(y*np.log(a_L) &#43; (1-y)*np.log(1-a_L)))
        return -j / m</code></pre></td></tr></tbody></table></figure><h2 id=2-5-lan-truyền-ngược>2.5. Lan truyền ngược</h2><p>Bước tiếp theo ta cài đặt thao tác tính đạo hàm với lan truyền ngược (<em>backpropagation</em>). Ở bài trước, ta đã đề cập tới công thức tính của đạo hàm theo phương pháp lan truyền ngược:</p><p>$$
\begin{aligned}
\dfrac{\partial{J}}{\partial{\mathbf{z}^{(L)}}} &amp;= \dfrac{\partial{J}}{\partial{\mathbf{a}^{(L)}}}\dfrac{\partial{\mathbf{a}^{(L)}}}{\partial{\mathbf{z}^{(L)}}}
\cr
\dfrac{\partial{J}}{\partial{\mathbf{z}^{(l)}}} &amp;= \bigg(\big(\mathbf{W}^{(l+1)}\big)^{\intercal}\dfrac{\partial{J}}{\partial{\mathbf{z}^{(l+1)}}}\bigg)\dfrac{\partial{\mathbf{a}^{(l)}}}{\partial{\mathbf{z}^{(l)}}}
\cr
\dfrac{\partial{J}}{\partial{\mathbf{W}^{(l)}}} &amp;= \dfrac{\partial{J}}{\partial{\mathbf{z}^{(l)}}}\big(\mathbf{a}^{(l-1)}\big)^{\intercal}
\end{aligned}
$$</p><p>Một điểm cần lưu ý ở đây là $\mathbf a^{(l)}$ và $\mathbf W^{(l)}$ với $l=\overline{1,L}$ không chứa giá trị cho bias. Bởi lẽ các $a_0=1$ ta tự thêm vào để dễ dàng sử dụng phép nhân ma trận mà thôi. Mà nếu lấy đạo hàm của $a_0$ theo $z$ bất kì thì luôn bằng $0$, nên nó cũng thực sự không có ý nghĩa.</p><blockquote><p>Kinh nghiệm là, nếu để ý số chiều của các giá trị $\mathbf W, \mathbf z, \mathbf a$ thì ta có thể biết được phép toán giữa các biến như thế nào. Nên khi lập trình ta nên in kích cỡ các biến đó ra để tiện theo dõi.</p></blockquote><p>Với hàm lỗi là cross-entropy như trên thì ta có thể chứng minh đạo hàm của $J$ theo $\mathbf z^{L}$:
$$\dfrac{\partial{J}}{\partial{\mathbf{z}^{(L)}}}=\mathbf{a}^{(L)}-\mathbf{y}$$</p><p>Tôi không chứng minh ở đây, nhưng việc chứng minh này cũng không khó, nếu bạn quan tâm thì có thể vẽ vài nhát ra giấy nháp là thấy được ngay.</p><p>Đạo hàm của hàm kích hoạt <em>sigmoid</em> theo $\mathbf z$ có thể bằng công thức:
$$\dfrac{\partial{\mathbf{a}^{(l)}}}{\partial{\mathbf{z}^{(l)}}}=\mathbf{a}^{(l)}\odot\big(1-\mathbf{a}^{(l)}\big)$$</p><p>Bằng phép suy luận ở trên, cùng với <a href=https://dominhhai.github.io/vi/2018/04/nn-intro/#5-lan-truyền-ngược-và-đạo-hàm>giải thuật đã đưa ra</a>, ta có thể cài đặt như sau:</p><figure class="highlight python language-python"><figcaption><span>nn.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn.py target=_blank rel=external>nn.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def backprop(self, x, y):
        &#34;&#34;&#34;
        Backpropagation to calc derivatives
        &#34;&#34;&#34;
        w_grad = [np.zeros(W.shape) for W in self.w]
        # feedforward
        z, a = self.feedforward(x)
        # backward
        dz = a[-1] - y
        for _l in range(1, self.L):
            l = -_l # layer index
            if l &lt; -1:
                da = a[l] * (1 - a[l])
                # do not calc for w_0 (da_0 / dz = 0 because of a_0 = 1 for all z)
                dz = np.dot(self.w[l&#43;1][:, 1:].transpose(), dz) * da
            # gradient    
            w_grad[l] = np.dot(dz, a[l-1].transpose())
        return w_grad</code></pre></td></tr></tbody></table></figure><h2 id=2-6-kiểm-tra-đạo-hàm>2.6. Kiểm tra đạo hàm</h2><p>Nói là việc cài đặt lan truyền ngược đơn giản, nhưng khi thực hiện thì rất dễ xảy ra nhầm lẫn. Cũng như các bài toán học máy khác, việc <strong><span class="highlight-text green">kiểm tra đạo hàm là cực kì quan trọng</span></strong>. Như trong bài <a href=https://dominhhai.github.io/vi/2017/12/ml-gd/#3-2-kiểm-tra-đạo-hàm>tối ưu hàm lỗi với gradient descent</a> đã đề cập, ta tính được đạo hàm bằng phương pháp số học, với $\epsilon$ là 1 giá trị đủ nhỏ nhưng không quá nhỏ (thường là $10^{-4}$):
$$
f^{\prime}(x)\approx\frac{f(x+\epsilon)-f(x-\epsilon)}{2\epsilon}
$$
Khi đó, ta so sánh giá trị tính được này với giá trị tính được của giải thuật lan truyền ngược thì ta có thể rút ra kết luận rằng giải thuật ta chạy hợp lý hay chưa. Nếu chênh lệch nhau nhiều (thường là $10^{-5}$) thì giải thuật của ta nên xem xét lại. Tất nhiên là thì giá trị chênh lệch càng nhỏ thì càng tốt.</p><p>Ở đây, do có nhiều tham số nên ta có thể coi hàm lỗi $J$ là hàm nhiều biến $\mathbb W$. Như vậy, ta cần tính đạo hàm riêng của $J$ theo từng tham số $w$ thành phần và kiểm tra xem đạo hàm riêng này đúng đắn hay chưa. Để tính đạo hàm riêng theo phương pháp số học, ta cũng áp dụng việc tính 2 đầu với $\epsilon$ cho riêng tham số tương ứng:</p><figure class="highlight python language-python"><figcaption><span>nn.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn.py target=_blank rel=external>nn.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def check_grad(self, data, grad, epsilon=1e-4, threshold=1e-6):
        &#34;&#34;&#34;
        Check gradient with:
        * Epsilon      : 1e-4
        * Threshold : 1e-6
        &#34;&#34;&#34;
        for l in range(self.L - 1):
            n_row, n_col = self.w[l].shape
            for i in range(n_row):
                for j in range(n_col):
                    w_l_ij = self.w[l][i][j]
                    # left
                    self.w[l][i][j] = w_l_ij - epsilon
                    l_cost = self.cost(data)
                    # right
                    self.w[l][i][j] = w_l_ij &#43; epsilon
                    r_cost = self.cost(data)
                    # numerical grad
                    num_grad = (r_cost - l_cost) / (2 * epsilon)
                    # diff
                    diff = abs(grad[l][i][j] - num_grad)
                    # reset w
                    self.w[l][i][j] = w_l_ij
                    
                    if diff &gt; threshold:
                        print(&#39;Check Grad Error at (l: {0}, col: {1}, row: {2}), | num_grad: {3} vs backprop grad: {4} | : {5}&#39;
                              .format(l, i, j, num_grad, grad[l][i][j], diff))
                        return False
        
        return True</code></pre></td></tr></tbody></table></figure><h2 id=2-7-huấn-luyện>2.7. Huấn luyện</h2><p>Tính được đạo hàm rồi thì việc tiếp theo ta cần làm là huấn luyện mạng, hay nói cách khác là tìm tập tham số $\mathbb W$ sao cho phép suy luận của ta được hợp lý bằng cách tối ưu hàm lỗi $J$.</p><p>Trong bài này, tôi sẽ sử dụng <a href=https://dominhhai.github.io/vi/2017/12/ml-gd/#5-2-mini-batch-gd>phương pháp mini-batch GD</a> để tối ưu hàm lỗi.</p><figure class="highlight python language-python"><figcaption><span>nn.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn.py target=_blank rel=external>nn.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def train(self, train_data, epochs, mini_batch_size, eta):
        &#34;&#34;&#34;
        Train NN with train data ``[(x, y)]``.
        This use mini-batch SGD method to train the NN.
        &#34;&#34;&#34;
        # number of training data        
        m = len(train_data)
        # cost
        cost = []
        for j in range(epochs):
            start_time = time.time()
            # shuffle data before run
            random.shuffle(train_data)
            # divide data into mini batchs
            for k in range(0, m, mini_batch_size):
                mini_batch = train_data[k:k&#43;mini_batch_size]
                m_batch = len(mini_batch)
                # calc gradient
                w_grad = [np.zeros(W.shape) for W in self.w]
                for x, y in mini_batch:
                    grad = self.backprop(x, y)
                    w_grad = [W_grad &#43; g for W_grad, g in zip(w_grad, grad)]
                w_grad = [W_grad / m_batch for W_grad in w_grad]
                
                # check grad for first mini_batch in first epoch
                if j == 0  and k == 0 and not self.check_grad(mini_batch, w_grad):
                    print(&#39;backprop fail!&#39;)
                    return False
                
                # update w
                self.w = [W - eta * W_grad for W, W_grad in zip(self.w, w_grad)]
            
            # calc cost
            cost.append(self.cost(train_data))
            
        return cost</code></pre></td></tr></tbody></table></figure><p>Hi vọng rằng đoạn mã trên không quá khó hiểu. Nếu bạn chạy luôn hàm trên thì có lẽ sẽ rất chậm bởi việc kiểm tra đạo hàm ta đang làm cho toàn bộ tham số. Trên thực tế với các mạng lớn thì điều này sẽ gây khó khăn, nên ta có thể chấp nhận rủi ro 1 chút là chỉ tính với 1 số tham số ngẫu nhiên nào đó. Ở đây tôi không cài đặt phương pháp này, nhứng nếu bạn hứng thú thì có thể tự cài đặt coi như là 1 bài tập nhỏ nhé. Nếu có khó khăn gì thì cứ để lại <a href=#disqus_thread>bình luận bên dưới</a>, tôi sẽ ngoi lên bàn luận với bạn.</p><p>Chạy thử xem nào:<figure class="highlight python language-python"><figcaption><span>nn.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn.py target=_blank rel=external>nn.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python># load data
training_data, validation_data, test_data = data_loader.load()

# run NN
nn = NN([784, 100, 10])
nn.train(training_data, 30, 100, 3.0)
correct = nn.evaluate(test_data)
total = len(test_data)
print(&#39;Evaluation: {0} / {1} = {2}%&#39;.format(correct, total, 100 * correct/total))</code></pre></td></tr></tbody></table></figure></p><p>Đoạn mã trên sẽ cho ta kết quả:</p><pre><code>Evaluation: 9662 / 10000 = 96.62%
</code></pre><p>Như vậy với tập kiểm tra, ta đạt được <code>96.62%</code> kết quả chính xác! Một mạng NN cơ bản mà kết quả rất ấn tượng phải không nào!</p><p>Nếu bạn hứng thú với toàn bộ mã nguồn của phần này thì có thể <a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/network.ipynb>xem tại đây</a> nhé.</p><h1 id=3-bàn-luận>3. Bàn luận</h1><p>Nếu bạn thử khởi tạo mạng với các giá trị khác nhau thì sẽ nhận ra rằng, việc tăng giảm kích cỡ mạng có thể cho kết quả rất khác nhau. Mạng càng nhỏ thì kết quả dự đoán sẽ càng tệ và ngược lại. Tuy nhiên, nếu kích cỡ mạng quá lớn thì kết quả cũng không hề khả quan trên tập kiểm tra mặc dù vẫn rất ngon lành với tập huấn luyện. Đó chính là <a href=https://dominhhai.github.io/vi/2017/12/ml-overfitting/>vấn đề mô hình quá khớp</a> mà tôi đã từng đề cập tới. Vấn đề này tôi sẽ viết vào bài tiếp theo trong chủ đề về mạng nơ-ron này.</p><p>Ngoài ra thì việc xây dựng mô hình mạng với các hàm khởi tạo, hàm lỗi khác nhau cũng sẽ cho hiệu năng khác nhau. Mô hình nào phù hợp thì câu trả lời là tuỳ bài toán mà ta cần giải quyết. Ví dụ, như xử lý ảnh người ta có thể xử dụng các mô hình <a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>ConvNet</a> hay xử lý ngôn ngữ thì sử dụng <a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>RNN</a> chẳng hạn.</p><p>Tất cả những vấn đề như vậy, tôi sẽ đề cập sau vì mục tiêu của bài này chỉ là thực hiện việc cài đặt cơ bản để làm sáng tỏ <a href=https://dominhhai.github.io/vi/2018/04/nn-intro/>lý thuyết về mạng NN ở bài trước</a>. Còn bây giờ, nếu bạn có thắc mắc gì thì cứ để lại bình luận bên dưới nhé.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a>
<a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/nn/>NN</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/04/nn-bp/ data-tooltip="[NN] Về lan truyền ngược - Backpropagation"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/04/list-derivs/ data-tooltip="[Giải Tích] Tra cứu đạo hàm"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2018/04/nn-implement/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2018/04/nn-implement/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2018/04/nn-implement/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2018 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/04/nn-bp/ data-tooltip="[NN] Về lan truyền ngược - Backpropagation"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/04/list-derivs/ data-tooltip="[Giải Tích] Tra cứu đạo hàm"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2018/04/nn-implement/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2018/04/nn-implement/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2018/04/nn-implement/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2018%2F04%2Fnn-implement%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2018%2F04%2Fnn-implement%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2018%2F04%2Fnn-implement%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2018\/04\/nn-implement\/';this.page.identifier='\/vi\/2018\/04\/nn-implement\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>