<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.54.0 with theme Tranquilpeak 0.4.1-BETA"><title>[NN] Mạng quá khớp - Overfitting</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Học Máy,Machine Learning,Neural Networks,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2018/05/nn-overfitting/><meta name=description content="Cũng như các bài toán ML khác, mạng NN hoàn toàn có thể bị quá khớp nếu kích cỡ lớn quá mức cần thiết. Nên khi cài đặt mạng NN, người ta thường cài thêm các phương pháp như chính quy hoá, bỏ nút mạng&hellip; nhằm giảm được vấn đề này."><link rel=publisher href=https://plus.google.com/115106277658014197977><meta property=fb:app_id content=333198270561466><meta property=og:locale content=vi_VN><meta property=og:type content=article><meta property=article:author content=dominhai><meta property=og:title content="[NN] Mạng quá khớp - Overfitting"><meta property=og:url content=https://dominhhai.github.io/vi/2018/05/nn-overfitting/><meta property=og:description content="Cũng như các bài toán ML khác, mạng NN hoàn toàn có thể bị quá khớp nếu kích cỡ lớn quá mức cần thiết. Nên khi cài đặt mạng NN, người ta thường cài thêm các phương pháp như chính quy hoá, bỏ nút mạng&hellip; nhằm giảm được vấn đề này."><meta property=og:site_name content="Hai's Blog"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:creator content=@minhhai3b><meta name=twitter:card content=summary><meta name=twitter:title content="[NN] Mạng quá khớp - Overfitting"><meta name=twitter:url content=https://dominhhai.github.io/vi/2018/05/nn-overfitting/><meta name=twitter:description content="Cũng như các bài toán ML khác, mạng NN hoàn toàn có thể bị quá khớp nếu kích cỡ lớn quá mức cần thiết. Nên khi cài đặt mạng NN, người ta thường cài thêm các phương pháp như chính quy hoá, bỏ nút mạng&hellip; nhằm giảm được vấn đề này."><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/talk/><i class="sidebar-button-icon fa fa-lg fa-child"></i><span class=sidebar-button-desc>Chém gió</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[NN] Mạng quá khớp - Overfitting</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2018-05-28T10:20:14&#43;09:00>28 tháng 5, 2018</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Cũng như các bài toán ML khác, mạng NN hoàn toàn có thể bị quá khớp nếu kích cỡ lớn quá mức cần thiết. Nên khi cài đặt mạng NN, người ta thường cài thêm các phương pháp như <em>chính quy hoá</em>, <em>bỏ nút mạng</em>&hellip; nhằm giảm được vấn đề này.</p><p>Như tôi đã viết ở bài <a href=https://dominhhai.github.io/vi/2017/12/ml-overfitting/>[ML] Mô hình quá khớp (Overfitting)</a> thì vấn đề quá khớp là mô hình của ta đưa ra kết quả rất ngon cho tập dữ liệu huấn luyện, nhưng khi đem thử nghiệm thực tế thì lại cho kết quả không mấy khả quan. Nguyên nhân là do mô hình quá phức tạp dẫn tới nó khớp được với nhiều dữ liệu huấn luyện nhưng lại không đủ tổng quát để khớp với các dữ liệu thực tế.</p><p>Dựa vào bài viết đó, tôi sẽ cài đặt kĩ thuật <a href=https://dominhhai.github.io/vi/2017/12/ml-overfitting/#4-kĩ-thuật-chính-quy-hoá><strong>chính quy hoá</strong></a> (<em>Regularization</em>) cho mạng NN. Ngoài ra, sẽ đưa ra thêm 1 phương pháp phổ biến nữa là <strong><a href=https://en.wikipedia.org/wiki/Dropout_(neural_networks)>bỏ nút mạng</a></strong> (<em>Dropout</em>). Nếu bạn cần tìm hiểu thêm lý thuyết cũng như các phương pháp phát hiện hiện tượng này thì có thể đọc <a href=https://dominhhai.github.io/vi/2017/12/ml-overfitting/>bài viết đó</a> để có cái nhìn chi tiết hơn. Còn ở đây, tôi chủ yếu tập trung vào việc cài đặt mạng NN mà thôi.</p><h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-regularization>1. Regularization</a></li><li><a href=#2-dropout>2. Dropout</a></li><li><a href=#3-kết-luận>3. Kết luận</a></li></ul></nav><h1 id=1-regularization>1. Regularization</h1><p>Kĩ thuật chính quy hoá được thực hiện bằng cách thêm phần tử chính quy hoá vào hàm lỗi nhằm suy giảm độ lớn của các trọng số sau khi tối ưu:</p><p>$$J(\mathbb W) = J_0(\mathbb W) + \lambda\frac{1}{p}\sum_{i=1}^n\lvert w_i\rvert^p$$</p><p>Trong đó, $J_0(\mathbb W)$ là hàm lỗi ban đầu của ta, $\lambda$ là hệ số chính quy hoá, $p$ là cấp của norm và $w_i$ là trọng số thứ $i$ của mô hình. Thông thường người ta hay lấy $p=2$ (<strong>L2</strong>) hoặc $p=1$ (<strong>L1</strong>) để thực hiện kĩ thuật này.</p><p>Bài viết này, tôi sẽ cài đặt <strong>L2</strong> cho mạng NN. Việc cài đặt mạng <strong>L1</strong> cũng hoàn toàn tương tự như vậy không khó khăn gì cả. Với $p=2$, ta có thể viết lại công thức cho hàm lỗi của mạng NN như sau:</p><p>$$
J(\mathbb{W}) = -\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K\Bigg(y_k^{(i)}\log\Big(\sigma_k^{(i)}\Big)+\Big(1-y_k^{(i)}\Big)\log\Big(1-\sigma_k^{(i)}\Big)\Bigg) + \frac{\lambda}{2m}\sum_{j=1}^nw_j^2
$$</p><p>$w_j$ ở đây là trọng số thứ $j$ của mạng và nó <strong><span class="highlight-text danger">không bao gồm các bias</span></strong> của mạng. Ngoài ra, ta chia cho $m$ để lấy trung bình cho toàn bộ mẫu tương tự như ý nghĩa của hàm lỗi nguyên gốc.</p><p>Nếu, ta sử dụng phép véc-tơ hoá để mô phỏng mạng có $L$ tầng có ma trận trọng lượng tương ứng $\mathbf W_l$ thì ta có thể viết lại như sau:
$$
J(\mathbb{W}) = -\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K\Bigg(y_k^{(i)}\log\Big(\sigma_k^{(i)}\Big)+\Big(1-y_k^{(i)}\Big)\log\Big(1-\sigma_k^{(i)}\Big)\Bigg) + \frac{\lambda}{2m}\sum_{l=1}^L\sum_j\mathbf W_l[:,1:]^{\intercal}\mathbf W_l[:,1:]
$$</p><p>Khi đó, đạo hàm của hàm lỗi sẽ có dạng:
$$
\frac{\partial J}{\partial w_i} = \frac{\partial J_0}{\partial w_i} + \frac{\lambda}{m}w_i
$$</p><p>Trong đó, $\dfrac{\partial J_0}{\partial w_i}$ là đạo hàm của hàm lỗi không có cụm chính quy hoá $J_0(\mathbb W)$ tính được bằng <a href=https://dominhhai.github.io/vi/2018/04/nn-implement/#2-5-lan-truy%E1%BB%81n-ng%C6%B0%E1%BB%A3c>phương pháp lan truyền ngược</a> như đã biết. Việc chứng minh công thức trên hoàn toàn không khó, hi vọng là nhìn cái bạn có thể luận được luôn nên tôi không viết ra đây nữa.</p><p>Bằng lập luận như vậy, ta viết lại được mã tính hàm lỗi như sau:</p><figure class="highlight python language-python"><figcaption><span>nn-overfitting.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn-overfitting.py target=_blank rel=external>nn-overfitting.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def cost(self, data, lamda):
        &#34;&#34;&#34;
        Return cross-entropy cost of NN on test data
        &#34;&#34;&#34;
        m = len(data)
        j = 0
        for x, y in data:
            _, a = self.feedforward(x)
            a_L = a[-1]
            j -= np.sum(np.nan_to_num(y*np.log(a_L) &#43; (1-y)*np.log(1-a_L)))
        # regularization term
        j &#43;= 0.5 * lamda * sum(np.linalg.norm(W[:,1:])**2 for W in self.w)
        return j / m</code></pre></td></tr></tbody></table></figure><p>Việc tính đạo hàm cũng được viết lại thành:<figure class="highlight python language-python"><figcaption><span>nn-overfitting.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn-overfitting.py target=_blank rel=external>nn-overfitting.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def train(self, train_data, epochs, mini_batch_size, eta, lamda=0.0):
        &#34;&#34;&#34;
        Train NN with train data ``[(x, y)]``.
        This use mini-batch SGD method to train the NN.
        &#34;&#34;&#34;
        # number of training data        
        m = len(train_data)
        # cost
        cost = []
        for j in range(epochs):
            start_time = time.time()
            # shuffle data before run
            random.shuffle(train_data)
            # divide data into mini batchs
            for k in range(0, m, mini_batch_size):
                mini_batch = train_data[k:k&#43;mini_batch_size]
                m_batch = len(mini_batch)
                # calc gradient
                w_grad = [np.zeros(W.shape) for W in self.w]
                for x, y in mini_batch:
                    grad = self.backprop(x, y)
                    w_grad = [W_grad &#43; g for W_grad, g in zip(w_grad, grad)]
                w_grad = [W_grad / m_batch for W_grad in w_grad]

                # add regularization term
                w_grad = [W_grad &#43; (lamda/m_batch * np.insert(W[:,1:],0,0,axis=1))
                            for W, W_grad in zip(self.w, w_grad)]
                
                # check grad for first mini_batch in first epoch
                if j == 0  and k == 0 and not self.check_grad(mini_batch, w_grad):
                    print(&#39;backprop fail!&#39;)
                    return False
                
                # update w
                self.w = [W - eta * W_grad for W, W_grad in zip(self.w, w_grad)]
            
            # calc cost
            cost.append(self.cost(train_data))
            
        return cost</code></pre></td></tr></tbody></table></figure></p><p>Nếu chạy thử với mạng 1 tầng ẩn 100 nút và $\lambda=4$ thì ta có thể thu được <a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/network-overfitting.ipynb#3.-Test>kết quả</a> chính xác tới <strong>96.72%</strong>, tăng được <em>0.1%</em> so với $\lambda=0$ tức là không thực hiện việc chính quy hoá.</p><h1 id=2-dropout>2. Dropout</h1><p>Một kĩ thuật nữa rất hay được sử dụng là <strong><a href=http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf>bỏ nút mạng</a></strong> (<em>dropout</em>) rất đơn giản và cho kết quả rất khả quan. Ý tưởng của phương pháp này là trong quá trình huấn luyện ta bỏ đi ngẫu nhiêu một vài nút mạng nhằm giảm độ phức tạp của mạng.</p><p>Ta có thể coi mạng sau khi bỏ đi các nút đó là một mạng mới tinh gọn hơn mạng gốc. Như vậy, Với mỗi các lô dữ liệu huấn luyện khác nhau mà ta thực hiện với các mạng tinh giản khác nhau thì kết quả ta thu được sẽ là một mạng trung bình của các mạng tinh gọn đó. Bằng việc lấy mạng trung bình đó, thì ta có thể hi vọng rằng mạng của ta có thể tổng quát được nhiều trường hợp hơn hay nói cách khác là bớt được vấn đề quá khớp.</p><p>Tuy nhiên một điểm cần lưu ý là ta <strong><span class="highlight-text danger">không được bỏ bất kì nút mạng nào ở tầng ra</span></strong>, bởi đầu ra của ta cần phải ở dạng mã hoá đầy đủ. Thường người ta sẽ bỏ nút mạng ở <strong><span class="highlight-text success">đầu vào với xác xuất là 20%</span></strong> và <strong><span class="highlight-text blue">các tầng ẩn là 50%</span></strong>.</p><p>Với ý tưởng như vậy, ta có thể cài đặt mạng theo quy trình sau:</p><ul><li>1. Phân lô dữ liệu</li><li>2. Xử lý mỗi lô với mạng tinh giản<ul><li>2.1. Bỏ đi ngẫu nhiên một số nút mạng ẩn</li><li>2.2. Học với mạng sau khi bỏ nút</li><li>2.3. Hồi phục lại các nút bị bỏ đi</li></ul></li></ul><p>Ví dụ, tôi cài đặt cho việc các nút ẩn với xác xuất được truyền bởi tham số <code>dropout</code> của hàm huấn luyện <code>train</code> như sau:</p><figure class="highlight python language-python"><figcaption><span>nn-overfitting.py</span><a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn-overfitting.py target=_blank rel=external>nn-overfitting.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br><span class=line>37</span><br><span class=line>38</span><br><span class=line>39</span><br><span class=line>40</span><br><span class=line>41</span><br><span class=line>42</span><br><span class=line>43</span><br><span class=line>44</span><br><span class=line>45</span><br><span class=line>46</span><br><span class=line>47</span><br><span class=line>48</span><br><span class=line>49</span><br><span class=line>50</span><br><span class=line>51</span><br><span class=line>52</span><br><span class=line>53</span><br><span class=line>54</span><br><span class=line>55</span><br><span class=line>56</span><br><span class=line>57</span><br><span class=line>58</span><br><span class=line>59</span><br><span class=line>60</span><br><span class=line>61</span><br><span class=line>62</span><br><span class=line>63</span><br><span class=line>64</span><br><span class=line>65</span><br><span class=line>66</span><br><span class=line>67</span><br><span class=line>68</span><br><span class=line>69</span><br><span class=line>70</span><br><span class=line>71</span><br><span class=line>72</span><br><span class=line>73</span><br><span class=line>74</span><br><span class=line>75</span><br><span class=line>76</span><br><span class=line>77</span><br><span class=line>78</span><br><span class=line>79</span><br><span class=line>80</span><br><span class=line>81</span><br><span class=line>82</span><br><span class=line>83</span><br><span class=line>84</span><br><span class=line>85</span><br><span class=line>86</span><br><span class=line>87</span><br><span class=line>88</span><br><span class=line>89</span><br><span class=line>90</span><br><span class=line>91</span><br><span class=line>92</span><br><span class=line>93</span><br><span class=line>94</span><br><span class=line>95</span><br><span class=line>96</span><br><span class=line>97</span><br><span class=line>98</span><br><span class=line>99</span><br><span class=line>100</span><br><span class=line>101</span><br><span class=line>102</span><br><span class=line>103</span><br><span class=line>104</span><br><span class=line>105</span><br><span class=line>106</span><br><span class=line>107</span><br><span class=line>108</span><br><span class=line>109</span><br><span class=line>110</span><br><span class=line>111</span><br><span class=line>112</span><br><span class=line>113</span><br><span class=line>114</span><br><span class=line>115</span><br><span class=line>116</span><br><span class=line>117</span><br><span class=line>118</span><br><span class=line>119</span><br><span class=line>120</span><br><span class=line>121</span><br><span class=line>122</span><br><span class=line>123</span><br><span class=line>124</span><br><span class=line>125</span><br><span class=line>126</span><br><span class=line>127</span><br><span class=line>128</span><br><span class=line>129</span><br><span class=line>130</span><br><span class=line>131</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>class NN():
    def train(self, train_data, epochs, mini_batch_size, eta,
		lamda=0.0,
		dropout=0.0):
        &#34;&#34;&#34;
        Train NN with train data ``[(x, y)]``.
        This use mini-batch SGD method to train the NN.
        &#34;&#34;&#34;
        # number of training data        
        m = len(train_data)
        # cost
        cost = []
        for j in range(epochs):
            start_time = time.time()
            # shuffle data before run
            random.shuffle(train_data)
            # divide data into mini batchs
            for k in range(0, m, mini_batch_size):
                mini_batch = train_data[k:k&#43;mini_batch_size]
                m_batch = len(mini_batch)
                # dropout
                m_dropout = None
                if dropout &gt; 0:
                    m_dropout = [np.random.binomial(n=1, p=1.0-dropout, size=l).reshape((l,1))
                                                    for l in self.layers[1:-1]]
                # calc gradient
                w_grad = [np.zeros(W.shape) for W in self.w]
                for x, y in mini_batch:
                    grad = self.backprop(x, y, m_dropout)
                    w_grad = [W_grad &#43; g for W_grad, g in zip(w_grad, grad)]
                w_grad = [W_grad / m_batch for W_grad in w_grad]
                # add regularization term
                w_grad = [W_grad &#43; (lamda/m_batch * np.insert(W[:,1:],0,0,axis=1))
                            for W, W_grad in zip(self.w, w_grad)]
                # check grad for first mini_batch in first epoch
                if j == 0  and k == 0 and not self.check_grad(mini_batch, w_grad, m_dropout):
                    print(&#39;backprop fail!&#39;)
                    return False
                # update w
                self.w = [W - eta * W_grad for W, W_grad in zip(self.w, w_grad)]
            # calc cost
            cost.append(self.cost(train_data, m_dropout))
        return cost

    def backprop(self, x, y, m_dropout=None):
        &#34;&#34;&#34;
        Backpropagation to calc derivatives
        &#34;&#34;&#34;
        w_grad = [np.zeros(W.shape) for W in self.w]
        # feedforward
        z, a = self.feedforward(x, m_dropout)
        # backward
        dz = a[-1] - y
        for _l in range(1, self.L):
            l = -_l # layer index
            if l &lt; -1:
                da = self.sigmoid_grad(z[l])
                # dropout
                if not (m_dropout is None):
                    da *= m_dropout[l&#43;1]
                # do not calc for w_0 (da_0 / dz = 0 because of a_0 = 1 for all z)
                dz = np.dot(self.w[l&#43;1][:, 1:].transpose(), dz) * da
            # gradient    
            w_grad[l] = np.dot(dz, a[l-1].transpose())
        return w_grad

    def feedforward(self, x, m_dropout=None):
        &#34;&#34;&#34;
        Feedforward through network for calc ``z``,`` a``.
        ``z`` is list of (L-1) vec-tor, ``z[0]`` for layer 2, and so on.
        ``a`` is list of (L) vec-tor, ``a[0]`` for layer 1, and so on.
        &#34;&#34;&#34;
        z = []
        a = [self.add_bias(x)]
        for l in range(1, self.L):
            z_l = np.dot(self.w[l-1], a[l-1])
            a_l = self.sigmoid(z_l)
            if l &lt; self.L - 1:
                # dropout
                if not (m_dropout is None):
                    a_l *= m_dropout[l-1]
                # add bias a_0
                a_l = self.add_bias(a_l)
            z.append(z_l)
            a.append(a_l)
        return (z, a)

    def check_grad(self, data, lamda, grad,
		m_dropout=None,
		epsilon=1e-4,
		threshold=1e-6):
        &#34;&#34;&#34;
        Check gradient with:
        * Epsilon      : 1e-4
        * Threshold : 1e-6
        &#34;&#34;&#34;
        for l in range(self.L - 1):
            n_row, n_col = self.w[l].shape
            for i in range(n_row):
                for j in range(n_col):
                    w_l_ij = self.w[l][i][j]
                    # left
                    self.w[l][i][j] = w_l_ij - epsilon
                    l_cost = self.cost(data, lamda, m_dropout)
                    # right
                    self.w[l][i][j] = w_l_ij &#43; epsilon
                    r_cost = self.cost(data, lamda, m_dropout)
                    # numerical grad
                    num_grad = (r_cost - l_cost) / (2 * epsilon)
                    # diff
                    diff = abs(grad[l][i][j] - num_grad)
                    # reset w
                    self.w[l][i][j] = w_l_ij
                    
                    if diff &gt; threshold:
                        return False
        return True

    def cost(self, data, lamda, m_dropout=None):
        &#34;&#34;&#34;
        Return cross-entropy cost of NN on test data
        &#34;&#34;&#34;
        m = len(data)
        j = 0
        for x, y in data:
            _, a = self.feedforward(x, m_dropout)
            a_L = a[-1]
            j -= np.sum(np.nan_to_num(y*np.log(a_L) &#43; (1-y)*np.log(1-a_L)))
        # regularization term
        j &#43;= 0.5 * lamda * sum(np.linalg.norm(W[:,1:])**2 for W in self.w)
        return j / m</code></pre></td></tr></tbody></table></figure><p>Sau khi chạy thử với xác xuất bỏ là 50%, kết quả tôi thu được chính xác tới <strong>96.77%</strong>. Dù hơn phương pháp chưa bỏ nút một chút, nhưng hi vọng với các tập dữ liệu và khởi tạo tham số khác nhau thì cho được kết quả khả quan hơn.</p><h1 id=3-kết-luận>3. Kết luận</h1><p>Bài này đã đưa ra 2 phương pháp làm giảm độ phức tạp của mạng NN nhằm nâng cao tính tổng quát hoá của mạng là kĩ thuật chính quy hoá - <em>regularization</em> và bỏ nút mạng - <em>dropout</em>. Trong thực tế, người ta thường kết hợp cả 2 phương pháp này với nhau vì việc cài đặt không quá phức tạp mà cho hiệu quả rất tốt. Mã nguồn của phần này, tôi cũng cài đặt theo phương pháp kết hợp cả 2, nếu bạn hứng thú thì có thể đọc <a href=https://github.com/dominhhai/dominhhai.github.io/blob/dev/code/nn-mnist/nn-overfitting.py>tại đây</a> nhé.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a>
<a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/nn/>NN</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/05/hito-ikasu/ data-tooltip="Giám đốc làm gì?"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/05/vim-config/ data-tooltip="[Vim] Cấu hình cơ bản trên CentOS"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2018/05/nn-overfitting/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2018/05/nn-overfitting/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2018/05/nn-overfitting/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=fb-root></div><script>(function(d,s,id){if(window.location.hostname=='localhost')return;var js,fjs=d.getElementsByTagName(s)[0];if(d.getElementById(id))return;js=d.createElement(s);js.id=id;js.src='https://connect.facebook.net/vi_VN/sdk.js#xfbml=1&version=v3.1&appId=333198270561466&autoLogAppEvents=1';fjs.parentNode.insertBefore(js,fjs);}(document,'script','facebook-jssdk'));</script><div id=fb-cmt-thread class=fb-comments data-href=https://dominhhai.github.io/vi/2018/05/nn-overfitting/ data-width=100%></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2021 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/05/hito-ikasu/ data-tooltip="Giám đốc làm gì?"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/05/vim-config/ data-tooltip="[Vim] Cấu hình cơ bản trên CentOS"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2018/05/nn-overfitting/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2018/05/nn-overfitting/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2018/05/nn-overfitting/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2018%2F05%2Fnn-overfitting%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2018%2F05%2Fnn-overfitting%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2018%2F05%2Fnn-overfitting%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2018\/05\/nn-overfitting\/';this.page.identifier='\/vi\/2018\/05\/nn-overfitting\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>