<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><title>[ICML] Learning Longer-term Dependencies in RNNs with Auxiliary Losses</title><meta name=author content="Do Minh Hai"><meta name=keywords content="RNN & LSTM,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/talk/paper-longer-term-rnn/><meta name=description content="Paper by Trieu H.Trinh, Andrew M.Dai, Minh-Thang Luong, Quoc V.Le. ICML2018. Talk at Tokyo ML Event."><meta property=og:type content=website><meta property=og:title content="[ICML] Learning Longer-term Dependencies in RNNs with Auxiliary Losses"><meta property=og:url content=https://dominhhai.github.io/vi/talk/paper-longer-term-rnn/><meta property=og:description content="Paper by Trieu H.Trinh, Andrew M.Dai, Minh-Thang Luong, Quoc V.Le. ICML2018. Talk at Tokyo ML Event."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="[ICML] Learning Longer-term Dependencies in RNNs with Auxiliary Losses"><meta name=twitter:url content=https://dominhhai.github.io/vi/talk/paper-longer-term-rnn/><meta name=twitter:description content="Paper by Trieu H.Trinh, Andrew M.Dai, Minh-Thang Luong, Quoc V.Le. ICML2018. Talk at Tokyo ML Event."><meta name=twitter:creator content=@minhhai3b><meta property=fb:app_id content=333198270561466><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.1.0/css/all.css integrity=sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/remark.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><textarea id=source>layout: true
class: center, middle
---
# Learning Longer-term Dependencies in RNNs with Auxiliary Losses

.red[.refer[
Trieu H.Trinh, Andrew M.Dai, Minh-Thang Luong, Quoc V.Le
]]

*19-08-2018*

Do Minh Hai

[&lt;i class=&#34;fab fa-github&#34;&gt; @dominhhai&lt;/i&gt;](https://github.com/dominhhai)

.footnote[.refer[
[&lt;i class=&#34;fab fa-facebook&#34;&gt; Tokyo Paper Reading Fest&lt;/i&gt;](https://www.facebook.com/events/176136236447633/)
]]
---
layout: false
class: left

# Outline

- Long-term Dependencies problem in RNNs

- Methodology

  - Auxiliary Losses

  - Procedure

- Exeperiments

  - Model

  - Results

- Analysis
---
# Long-term Dependencies problem
- Long-term dependencies

  - BPTT tend to vanish or explode during training

  - States memory is expensive

- SoTA methods
  - LSTM

  - Gradient Clipping

  - Truncated BPTT

.footnote[.refer[
\# More here: https://dominhhai.github.io/vi/talk/dl-rnn
]]
---
# Methodology
.center[&lt;img width=&#34;80%&#34; src=&#34;/images/talk-paper-long-term-rnn-1.png&#34; alt=&#34;Method Overview&#34;&gt;]

### Auxiliary Loss:
- Unsupervised
- Sampling at random anchor points

### BPTT:
  - All gradients are truncated
  - Improves performance and scalability
---
# Methodology
.center[&lt;img width=&#34;100%&#34; src=&#34;/images/talk-paper-long-term-rnn-2.png&#34; alt=&#34;Auxiliary Loss&#34;&gt;]

### Auxiliary Loss:
- *r*-LSTM: Reconstruction
  - Reconstructs the past events before anchor point
- *p*-LSTM: Prediction
  - Predict the next events after anchor point

### Anchor Points
- Temporary Memory
- Difference RNNs
---
# Methodology
.center[&lt;img width=&#34;100%&#34; src=&#34;/images/talk-paper-long-term-rnn-2.png&#34; alt=&#34;Auxiliary Loss&#34;&gt;]

### Training
- Random anchor points

- Train in 2 phases
  - Pretrain: minimize auxiliary losses only
  - Train: minimize the sum of main objective loss and auxiliary losses
  $$L = L\_\text{supervised} &#43; L\_\text{auxiliary}$$

---
# Methodology
.center[&lt;img width=&#34;80%&#34; src=&#34;/images/talk-paper-long-term-rnn-2.png&#34; alt=&#34;Auxiliary Loss&#34;&gt;]

### Sampling
- Add extra hyper-parameters
  - Sampling Frequency $n$
  - Subsequence Length $\\{l_i\\}~~~, i=\overline{1,n}$

### Auxiliary Loss:
$$L\_\text{auxiliary}=\frac{\sum\_{i=1}^n L\_i}{\sum\_{i=1}^n l\_i}$$

  where, the sum of cross-entroy loss $L\_i=\sum\_{t=1}^{l\_i}\text{TokenLost}\_t$
---
# Exeperiments - Datasets
.center[&lt;img width=&#34;90%&#34; src=&#34;/images/talk-paper-long-term-rnn-3.png&#34; alt=&#34;Datasets&#34;&gt;]

- Up to 16,000
  - over 20 times longer than any previously use benhmark
---
# Exeperiments - Models

### Main objective model
- Input: embedding size of 128
- 1-layer LSTM with 128 cells
- 2-layers FFN with 256 hidden units
  - Dropout with probability 0.5 on the 2nd-FFN
- Output: softmax prediction

### Auxiliary model
- 2-layers LSTM
  - Bottom layer is initialized with main LSTM states
  - Top layer starts with zero
- 2-layers FFN with 256 hidden units
  - Dropout with probability 0.5 on the 2nd-FFN

#### Hyper-parameters
- $n=1$
- $l_i=600$
---
# Exeperiments - Results
.center[&lt;img width=&#34;80%&#34; src=&#34;/images/talk-paper-long-term-rnn-4.png&#34; alt=&#34;MNIST, pMNIST, CIFAR10&#34;&gt;]
---
# Exeperiments - Results
- StandfordDogs

.center[&lt;img width=&#34;65%&#34; src=&#34;/images/talk-paper-long-term-rnn-5.png&#34; alt=&#34;StandfordDogs&#34;&gt;]
---
# Exeperiments - Results
- Compare with Transformer

.center[&lt;img width=&#34;60%&#34; src=&#34;/images/talk-paper-long-term-rnn-6.png&#34; alt=&#34;Transformer&#34;&gt;]

- DBpedia

  - $l_i=300$

.center[&lt;img width=&#34;60%&#34; src=&#34;/images/talk-paper-long-term-rnn-7.png&#34; alt=&#34;DBpedia&#34;&gt;]
---
# Analysis
- Shrinking BPTT length

  - Dataset: CIFAR10

.center[&lt;img width=&#34;80%&#34; src=&#34;/images/talk-paper-long-term-rnn-8.png&#34; alt=&#34;Shrinking BPTT length&#34;&gt;]

---
# Analysis
- Multiple Reconstructions with fixed BPTT cost

  - Dataset: CIFAR10

.center[&lt;img width=&#34;60%&#34; src=&#34;/images/talk-paper-long-term-rnn-9.png&#34; alt=&#34;Multiple Reconstructions with fixed BPTT cost&#34;&gt;]

- When $l_i$ is constant, we can use batch to utilize data parallelism
---
# Analysis
- Regulaziation and Optimization
  - Dataset: CIFAR10

.center[&lt;img width=&#34;56%&#34; src=&#34;/images/talk-paper-long-term-rnn-10.png&#34; alt=&#34;Regulaziation and Optimization&#34;&gt;]

---
# Analysis
- Relative contribution of difference factors to *r*-LSTM&#39;s performance

  - Dataset: CIFAR10
  - Turning off part from original full setting

.center[&lt;img width=&#34;80%&#34; src=&#34;/images/talk-paper-long-term-rnn-11.png&#34; alt=&#34;Ablation Study&#34;&gt;]

- Jointly training Unsupervised and Supervised loss is most important

- More randomness is better
---
layout: true
class: center, middle
---
# Thank You ðŸ˜Š

Happy Coding!
</textarea>
<script src=https://dominhhai.github.io/js/remark-latest.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq></script><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc></script><script>var slideshow=remark.create({ratio:'4:3',highlightStyle:'monokai',highlightLanguage:'python',highlightLines:false,highlightSpans:false},function(){var katexOpts={delimiters:[{left:"$$",right:"$$",display:true},{left:"\\[",right:"\\]",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false}]};renderMathInElement(document.body,katexOpts);});</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>