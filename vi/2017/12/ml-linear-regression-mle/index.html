<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.54.0 with theme Tranquilpeak 0.4.1-BETA"><title>[ML] MLE của hồi quy tuyến tính</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Học Máy,Machine Learning,Maximum Likelihood Esitmation,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/><meta name=description content="Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (mean squared error). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng MLE (Maximum Likelihood Esitmation) xem sao."><link rel=publisher href=https://plus.google.com/115106277658014197977><meta property=fb:app_id content=333198270561466><meta property=og:locale content=vi_VN><meta property=og:type content=article><meta property=article:author content=dominhai><meta property=og:title content="[ML] MLE của hồi quy tuyến tính"><meta property=og:url content=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/><meta property=og:description content="Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (mean squared error). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng MLE (Maximum Likelihood Esitmation) xem sao."><meta property=og:site_name content="Hai's Blog"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:creator content=@minhhai3b><meta name=twitter:card content=summary><meta name=twitter:title content="[ML] MLE của hồi quy tuyến tính"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/><meta name=twitter:description content="Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (mean squared error). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng MLE (Maximum Likelihood Esitmation) xem sao."><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/talk/><i class="sidebar-button-icon fa fa-lg fa-child"></i><span class=sidebar-button-desc>Chém gió</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[ML] MLE của hồi quy tuyến tính</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-12-21T12:28:26&#43;09:00>21 tháng 12, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (<em>mean squared error</em>). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng <a href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/#2-2-mle>MLE (<em>Maximum Likelihood Esitmation</em>)</a> xem sao.</p><h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-mô-phỏng-xác-suất>1. Mô phỏng xác suất</a></li><li><a href=#2-mle-với-phân-phối>2. MLE với phân phối</a><ul><li><a href=#2-1-mle-với-phân-phối-chuẩn>2.1. MLE với phân phối chuẩn</a></li><li><a href=#2-2-mle-với-phân-phối-có-nhiễu-dạng-chuẩn>2.2. MLE với phân phối có nhiễu dạng chuẩn</a></li></ul></li><li><a href=#3-mle-cho-hồi-quy-tuyến-tính>3. MLE cho hồi quy tuyến tính</a></li><li><a href=#4-kết-luận>4. Kết luận</a></li></ul></nav><h1 id=1-mô-phỏng-xác-suất>1. Mô phỏng xác suất</h1><p>Theo định lý <a href=https://en.wikipedia.org/wiki/Central_limit_theorem target=_blank _ rel="noopener noreferrer">giới hạn trung tâm</a> (<em>central limit theorem</em>) thì phân phối xác suất của biến ngẫu nhiên sẽ hội tụ về phân phối chuẩn. Vận dụng định lý này cho đầu ra của mỗi mô hình hồi quy tuyến tính, ta sẽ thêm 1 lượng nhiễu theo xác suất chuẩn vào đầu ra, ta sẽ được:
$$t=y(\mathbf{x},\theta)+\mathcal{N}(0,\sigma^2)$$</p><p>Như đã phân tích ở <a href=https://dominhhai.github.io/vi/2017/10/prob-com-var/#2-2-1-%C4%91%E1%BB%91i-v%E1%BB%9Bi-bi%E1%BA%BFn-1-chi%E1%BB%81u-univariate>phần phân phối chuẩn</a> ta có thể biểu diễn phân phối của $t$ bằng phân phối chuẩn:
$$p(t|\mathbf{x},\theta,\sigma)=\mathcal{N}(t|y(\mathbf{x},\theta),\sigma^2)$$</p><p>Đặt $\beta=\dfrac{1}{\sigma^2}$, ta có:
$$p(t|\mathbf{x},\theta,\beta)=\mathcal{N}(t|y(\mathbf{x},\theta),\beta^{-1})$$</p><p>Do $y(\mathbf{x},\theta)=\theta^{\intercal}\phi(\mathbf{x})$, nên:
$$p(t|\mathbf{x},\theta,\beta)=\mathcal{N}(t|\theta^{\intercal}\phi(\mathbf{x}),\beta^{-1})$$</p><p>Với giả sử dữ liệu huấn luyện của ta là I.I.D (mẫu ngẫu nhiên), ta sẽ thu được xác suất toàn mẫu là:
$$p(\mathbf{t}|\mathbf{X},\theta,\beta)=\prod_{i=1}^m\mathcal{N}(t_i|\theta^{\intercal}\phi(\mathbf{x}_i),\beta^{-1})$$</p><p>Trong đó $\mathbf{t}=[t_1,t_2,&hellip;,t_m]^{\intercal}$ và $\mathbf{X}=[\mathbf{x}_1, \mathbf{x}_2,&hellip;,\mathbf{x}_m]^{\intercal}$ lần lượt là đầu ra và đầu vào thực tế (dữ liệu từ tập huấn luyện).</p><h1 id=2-mle-với-phân-phối>2. MLE với phân phối</h1><p>Trước tiên ta sẽ xét MLE cho phân phối chuẩn một cách tổng quát rồi sẽ đi vào bài toán hồi quy tuyến tính. Vì việc nắm được lý thuyết sẽ giúp phân tích trường hợp cụ thể đơn giản hơn. Ở đây tôi không nhắc lại MLE là gì nữa mà sẽ đi thẳng vào vấn đề luôn. Nếu bạn cần tìm hiểu MLE là gì thì có thể xem tại <a href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/#2-2-mle target=_blank _>bài viết này</a>.</p><h2 id=2-1-mle-với-phân-phối-chuẩn>2.1. MLE với phân phối chuẩn</h2><p>Giả sử rằng ta có $\theta=[\mu,\sigma^2]$ và tập mẫu biến ngẫu nhiên $\mathbf{X}=[X_1,X_2,&hellip;,X_m]$ tuân theo phân phối chuẩn: $X_i\sim\mathcal{N}(\mu,\sigma^2)$. Giờ nhiệm vụ của ta là phải tìm được các tham số $\theta$ để phân phối toàn mẫu đạt lớn nhất có thể.</p><p>Ta có xác suất toàn mẫu là:
$$
\begin{aligned}
L(\theta)&amp;=\prod_{i=1}^mf(X_i|\theta)
\cr\ &amp;=\prod_{i=1}^m\frac{1}{\sqrt{2\pi\theta_1}}\exp\Bigg(-\frac{(X_i-\theta_0)^2}{2\theta_1}\Bigg)
\end{aligned}
$$</p><p>Lấy log ta sẽ được:
$$
\begin{aligned}
LL(\theta)&amp;=\sum_{i=1}^m\log\frac{1}{\sqrt{2\pi\theta_1}}\exp\Bigg(-\frac{(X_i-\theta_0)^2}{2\theta_1}\Bigg)
\cr\ &amp;=\sum_{i=1}^m\Big(-\log\sqrt{2\pi\theta_1}-\frac{(X_i-\theta_0)^2}{2\theta_1}\Big)
\cr\ &amp;=-\frac{m}{2}\log(2\pi\theta_1)-\frac{1}{2\theta_1}\sum_{i=1}^m(X_i-\theta_0)^2
\cr\ &amp;=-\frac{m}{2}\log(2\pi)-\frac{m}{2}\log(\theta_1)-\frac{1}{2\theta_1}\sum_{i=1}^m(X_i-\theta_0)^2
\end{aligned}
$$</p><p>Để tìm tham số $\theta$ cho hàm $LL(\theta)$ trên đạt cực đại, ta sẽ sử dụng đạo hàm để giải quyết.</p><p>Với tham số $\theta_0$, đạo hàm riêng sẽ là:
$$\frac{\partial{LL}}{\theta_0}=\frac{1}{\theta_1}\sum_{i=1}^m(X_i-\theta_0)$$
Khi đạt cực đại, đạo hàm bị triệt tiêu ta sẽ có:
$$
\begin{aligned}
\ &amp;\frac{1}{\theta_1}\sum_{i=1}^m(X_i-\theta_0)=0
\cr\iff &amp;m\theta_0=\sum_{i=1}^mX_i
\cr\iff &amp;\theta_0=\frac{1}{m}\sum_{i=1}^mX_i
\end{aligned}
$$</p><p>Tương tự, đạo hàm theo $\theta_1$ triệt tiêu:
$$
\begin{aligned}
\ &amp;\frac{\partial{LL}}{\theta_1}=0
\cr\iff &amp;-\frac{m}{2\theta_1}+\frac{1}{2\theta_1^2}\sum_{i=1}^m(X_i-\theta_0)^2=0
\cr\iff &amp;m\theta_1=\sum_{i=1}^m(X_i-\theta_0)^2
\cr\iff &amp;\theta_1=\frac{1}{m}\sum_{i=1}^m(X_i-\theta_0)^2
\end{aligned}
$$</p><p>Như vậy giá tham số ước lượng được là $\hat\theta_0=\hat\mu=\dfrac{1}{m}\displaystyle\sum_{i=1}^mX_i$ và $\hat\theta_1=\hat\sigma^2=\dfrac{1}{m}\displaystyle\sum_{i=1}^m(X_i-\theta_0)^2$.</p><h2 id=2-2-mle-với-phân-phối-có-nhiễu-dạng-chuẩn>2.2. MLE với phân phối có nhiễu dạng chuẩn</h2><p>Giả sử biến ngẫu nhiên $Y=\theta X+\mathcal{N}(0,\sigma^2)$ với phương sai $\sigma^2$ là cố định (biên dao động của nhiễu không đổi) và ta chưa biết phân phối của $X$ thế nào cả. Như vậy thì nếu $X$ đã biết trước $Y$ cũng tuân theo phân phối chuẩn $Y|X\sim\mathcal{N}(\theta X,\sigma^2)$. Nhiệm vụ của ta là ước lượng tham số $\theta$ sao cho xác suất của mẫu ngẫu nhiên đạt lớn nhất.</p><p>Giả sử ta có mẫu ngẫu nhiên có $m$ cặp dữ liệu $\{(X_1,Y_1),(X_2,Y_2),&hellip;,(X_m,Y_m)\}$, xác suất hợp của toàn mẫu sẽ là:</p><p>$$
\begin{aligned}
L(\theta)&amp;=\prod_{i=1}^mf(Y_i,X_i|\theta)
\cr\ &amp;=\prod_{i=1}^mf(Y_i|X_i,\theta)f(X_i|\theta)
\cr\ &amp;=\prod_{i=1}^mf(Y_i|X_i,\theta)f(X_i)
\cr\ &amp;=\prod_{i=1}^m\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Bigg(-\frac{(Y_i-\theta X_i)^2}{2\sigma^2}\Bigg)f(X_i)
\end{aligned}
$$</p><p>Lấy log ta được:
$$
\begin{aligned}
LL(\theta)&amp;=\sum_{i=1}^m\log\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Bigg(-\frac{(Y_i-\theta X_i)^2}{2\sigma^2}\Bigg)f(X_i)
\cr\ &amp;=-\frac{m}{2}\log(2\pi)-\frac{m}{2}\log(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^m(Y_i-\theta X_i)^2+\sum_{i=1}^m\log f(X_i)
\end{aligned}
$$</p><p>Nếu đặt $\beta=\dfrac{1}{\sigma^2}$ thì công thức trên sẽ thành:
$$LL(\theta)=\frac{m}{2}\log\beta-\frac{m}{2}\log(2\pi)-\frac{1}{2}\beta\sum_{i=1}^m(Y_i-\theta X_i)^2+\sum_{i=1}^m\log f(X_i)$$</p><p>Giờ để ước lượng $\theta$ sao cho $LL(\theta)$ đạt cực đại thì ta chỉ cần quan tâm tới thành phần có $\theta$ tức là việc ước lượng thành:
$$\hat\theta=\arg\min_\theta\sum_{i=1}^m(Y_i-\theta X_i)^2$$</p><h1 id=3-mle-cho-hồi-quy-tuyến-tính>3. MLE cho hồi quy tuyến tính</h1><p>Giờ áp dụng MLE ta cần tìm tham số để cho xác suất toàn mẫu là lớn nhất có thể:
$$
\begin{aligned}
(\hat\theta,\hat\beta)&amp;=\arg\max_{\theta,\beta}\prod_{i=1}^m\mathcal{N}(t_i|\theta^{\intercal}\phi(\mathbf{x}_i),\beta^{-1})
\cr\ &amp;=\arg\max_{\theta,\beta}\sum_{i=1}^m\log\mathcal{N}(t_i|\theta^{\intercal}\phi(\mathbf{x}_i),\beta^{-1})
\end{aligned}
$$</p><p>Như phân tích ở trên ta đã có:
$$\sum_{i=1}^m\log\mathcal{N}(t_i|\theta^{\intercal}\phi(\mathbf{x}_i),\beta^{-1})=\frac{m}{2}\beta-\frac{m}{2}\log(2\pi)-\frac{1}{2}\beta\sum_{i=1}^m\Big(t_i-\theta^{\intercal}\phi(\mathbf{x}_i)\Big)^2$$</p><p>Ở đây tôi lược bỏ thành phần $X$ đi để cho đơn giản. Thế giờ bạn nhìn thấy hàm lỗi $J(\theta)$ chưa?
$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m\Big(t_i-\theta^{\intercal}\phi(\mathbf{x}_i)\Big)^2$$</p><p>Từ đây ta sẽ được:
$$
\begin{aligned}
(\hat\theta,\hat\beta)&amp;=\arg\max_{\theta,\beta}\Big(\frac{m}{2}\beta-\frac{m}{2}\log(2\pi)-m\beta J(\theta)\Big)
\cr\ &amp;=\arg\max_{\theta,\beta}\beta\Big(1-2J(\theta)\Big)
\end{aligned}
$$</p><p>Nếu coi $\beta$ ở đây là cố định (các điểm đầu ra có mức dao động như nhau) thì việc cực đại hoá này được quy về việc cực tiểu hoá hàm lỗi $J(\theta)$:
$$\hat\theta=\arg\min_\theta J(\theta)$$</p><h1 id=4-kết-luận>4. Kết luận</h1><p>Qua quá trình phân tích này ta nhận thấy được sự tương đồng giữa việc tối thiểu hoá hàm lỗi và cực đại hoá độ hợp lý tham số. Trên cơ sở đó ta hoàn toàn có thể yên tâm về mức độ tin cậy của phương pháp tối ưu hàm lỗi của ta.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-gd/ data-tooltip="[ML] Tối ưu hàm lỗi với Gradient Descent"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/ data-tooltip="[ML] Hồi quy tuyến tính (Linear Regression)"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=fb-root></div><script>(function(d,s,id){if(window.location.hostname=='localhost')return;var js,fjs=d.getElementsByTagName(s)[0];if(d.getElementById(id))return;js=d.createElement(s);js.id=id;js.src='https://connect.facebook.net/vi_VN/sdk.js#xfbml=1&version=v3.1&appId=333198270561466&autoLogAppEvents=1';fjs.parentNode.insertBefore(js,fjs);}(document,'script','facebook-jssdk'));</script><div id=fb-cmt-thread class=fb-comments data-href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/ data-width=100%></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2021 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-gd/ data-tooltip="[ML] Tối ưu hàm lỗi với Gradient Descent"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/ data-tooltip="[ML] Hồi quy tuyến tính (Linear Regression)"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-linear-regression-mle%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-linear-regression-mle%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-linear-regression-mle%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/12\/ml-linear-regression-mle\/';this.page.identifier='\/vi\/2017\/12\/ml-linear-regression-mle\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>