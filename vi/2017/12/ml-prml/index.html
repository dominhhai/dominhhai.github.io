<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.31.1 with theme Tranquilpeak 0.4.1-BETA"><title>Pattern Recognition and Machine Learning</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Há»c MÃ¡y,Pattern Recognition and Machine Learning,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/12/ml-prml/><meta name=description content="ÄÆ°á»£c coi lÃ  sÃ¡ch giÃ¡o khoa cho nhá»¯ng ngÆ°á»i lÃ m há»c mÃ¡y, cuá»‘n sÃ¡ch nÃ y viáº¿t vá» cÃ¡c giáº£i thuáº­t vÃ  lÃ½ thuyáº¿t xÃ¢y dá»±ng cÃ¡c giáº£i thuáº­t nháº­n dáº¡ng máº«u vÃ  há»c mÃ¡y. Tuy nhiÃªn lÃºc má»›i Ä‘á»c thÃ¬ tháº¥y khÃ¡ khÃ³ nháº±n nÃªn tÃ´i Ä‘Ã£ tÃ¬m hiá»ƒu Ä‘á»™ khÃ³ cÃ¡c pháº§n Ä‘á» biáº¿t Ä‘Æ°á»ng mÃ  Ä‘á»c."><meta property=og:type content=website><meta property=og:title content="Pattern Recognition and Machine Learning"><meta property=og:url content=https://dominhhai.github.io/vi/2017/12/ml-prml/><meta property=og:description content="ÄÆ°á»£c coi lÃ  sÃ¡ch giÃ¡o khoa cho nhá»¯ng ngÆ°á»i lÃ m há»c mÃ¡y, cuá»‘n sÃ¡ch nÃ y viáº¿t vá» cÃ¡c giáº£i thuáº­t vÃ  lÃ½ thuyáº¿t xÃ¢y dá»±ng cÃ¡c giáº£i thuáº­t nháº­n dáº¡ng máº«u vÃ  há»c mÃ¡y. Tuy nhiÃªn lÃºc má»›i Ä‘á»c thÃ¬ tháº¥y khÃ¡ khÃ³ nháº±n nÃªn tÃ´i Ä‘Ã£ tÃ¬m hiá»ƒu Ä‘á»™ khÃ³ cÃ¡c pháº§n Ä‘á» biáº¿t Ä‘Æ°á»ng mÃ  Ä‘á»c."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="Pattern Recognition and Machine Learning"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/12/ml-prml/><meta name=twitter:description content="ÄÆ°á»£c coi lÃ  sÃ¡ch giÃ¡o khoa cho nhá»¯ng ngÆ°á»i lÃ m há»c mÃ¡y, cuá»‘n sÃ¡ch nÃ y viáº¿t vá» cÃ¡c giáº£i thuáº­t vÃ  lÃ½ thuyáº¿t xÃ¢y dá»±ng cÃ¡c giáº£i thuáº­t nháº­n dáº¡ng máº«u vÃ  há»c mÃ¡y. Tuy nhiÃªn lÃºc má»›i Ä‘á»c thÃ¬ tháº¥y khÃ¡ khÃ³ nháº±n nÃªn tÃ´i Ä‘Ã£ tÃ¬m hiá»ƒu Ä‘á»™ khÃ³ cÃ¡c pháº§n Ä‘á» biáº¿t Ä‘Æ°á»ng mÃ  Ä‘á»c."><meta name=twitter:creator content=@minhhai3b><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/ml/prml.jpg><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/ml/prml.jpg><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.css integrity=sha384-8QOKbPtTFvh/lMY0qPVbXj9hDh+v8US0pD//FcoYFst2lCIf0BmT58+Heqj0IGyx><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="áº¢nh Ä‘áº¡i diá»‡n"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="áº¢nh Ä‘áº¡i diá»‡n"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chá»§</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh má»¥c</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Tháº» thÃ´ng tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>LÆ°u trá»¯</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/#about><i class="sidebar-button-icon fa fa-lg fa-address-card"></i><span class=sidebar-button-desc>ThÃ´ng tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Há»i ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>Pattern Recognition and Machine Learning</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-12-12T16:36:56&#43;09:00>12 thÃ¡ng 12, 2017</time>
<span>má»¥c</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Há»c MÃ¡y</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>ÄÆ°á»£c coi lÃ  sÃ¡ch giÃ¡o khoa cho nhá»¯ng ngÆ°á»i lÃ m há»c mÃ¡y, cuá»‘n sÃ¡ch nÃ y viáº¿t vá» cÃ¡c giáº£i thuáº­t vÃ  lÃ½ thuyáº¿t xÃ¢y dá»±ng cÃ¡c giáº£i thuáº­t nháº­n dáº¡ng máº«u vÃ  há»c mÃ¡y. Tuy nhiÃªn lÃºc má»›i Ä‘á»c thÃ¬ tháº¥y khÃ¡ khÃ³ nháº±n nÃªn tÃ´i Ä‘Ã£ tÃ¬m hiá»ƒu Ä‘á»™ khÃ³ cÃ¡c pháº§n Ä‘á» biáº¿t Ä‘Æ°á»ng mÃ  Ä‘á»c.</p><p>Äá»™ khÃ³ nÃ y khÃ´ng pháº£i do tÃ´i nghÄ© ra mÃ  lÃ  láº¥y tá»« trang <a href=http://ibisforest.org/index.php?PRML%2Fcourse target=_blank _ rel="noopener noreferrer">trá»£ giÃºp sÃ¡ch nÃ y cá»§a Nháº­t</a>. TÃ´i má»›i Ä‘á»c háº¿t 1 pháº§n cuá»‘n nÃ y vÃ  tháº¥y Ä‘Ã¡nh giÃ¡ á»Ÿ trang Ä‘Ã³ cÃ³ váº» Ä‘Ãºng. TÃ´i thÃ¬ Æ°u tiÃªn Ä‘á»c pháº§n dá»… vÃ  vá»«a trÆ°á»›c rá»“i má»›i tá»›i cÃ¡c pháº§n nÃ¢ng cao. Tuy nhiÃªn khÃ´ng pháº£i lÃ  khÃ´ng Ä‘á»c tÃ­ nÃ o pháº§n nÃ¢ng cao mÃ  cÃ³ Ä‘á»c 1 chÃºt dÃ¹ khÃ´ng hiá»ƒu gÃ¬ cÅ©ng máº·c.</p><h1 id=table-of-contents>Má»¥c lá»¥c</h1><nav id=TableOfContents><ul><li><a href=#1-cÃ¡c-cáº¥p-Ä‘á»™>1. CÃ¡c cáº¥p Ä‘á»™</a></li><li><a href=#2-cÃ¡c-chÆ°Æ¡ng>2. CÃ¡c chÆ°Æ¡ng</a><ul><li><a href=#2-1-chÆ°Æ¡ng-1-introduction>2.1. ChÆ°Æ¡ng 1: Introduction</a></li><li><a href=#2-2-chÆ°Æ¡ng-2-probability-distributions>2.2. ChÆ°Æ¡ng 2: Probability Distributions</a></li><li><a href=#2-3-chÆ°Æ¡ng-3-linear-models-for-regression>2.3. ChÆ°Æ¡ng 3: Linear Models for Regression</a></li><li><a href=#2-4-chÆ°Æ¡ng-4-linear-models-for-classification>2.4. ChÆ°Æ¡ng 4: Linear Models for Classification</a></li><li><a href=#2-5-chÆ°Æ¡ng-5-neural-networks>2.5. ChÆ°Æ¡ng 5: Neural Networks</a></li><li><a href=#2-6-chÆ°Æ¡ng-6-kernel-methods>2.6. ChÆ°Æ¡ng 6: Kernel Methods</a></li><li><a href=#2-7-chÆ°Æ¡ng-7-sparse-kernel-machines>2.7. ChÆ°Æ¡ng 7: Sparse Kernel Machines</a></li><li><a href=#2-8-chÆ°Æ¡ng-8-graphical-models>2.8. ChÆ°Æ¡ng 8: Graphical Models</a></li><li><a href=#2-9-chÆ°Æ¡ng-9-mixture-models-and-em>2.9. ChÆ°Æ¡ng 9: Mixture Models and EM</a></li><li><a href=#2-10-chÆ°Æ¡ng-10-approximate-inference>2.10. ChÆ°Æ¡ng 10: Approximate Inference</a></li><li><a href=#2-11-chÆ°Æ¡ng-11-sampling-methods>2.11. ChÆ°Æ¡ng 11: Sampling Methods</a></li><li><a href=#2-12-chÆ°Æ¡ng-12-continuous-latent-variables>2.12. ChÆ°Æ¡ng 12: Continuous Latent Variables</a></li><li><a href=#2-13-chÆ°Æ¡ng-13-sequential-data>2.13. ChÆ°Æ¡ng 13: Sequential Data</a></li><li><a href=#2-14-chÆ°Æ¡ng-14-combining-models>2.14. ChÆ°Æ¡ng 14: Combining Models</a></li></ul></li></ul></nav><h1 id=1-cÃ¡c-cáº¥p-Ä‘á»™>1. CÃ¡c cáº¥p Ä‘á»™</h1><table><thead><tr><th>Cáº¥p Ä‘á»™</th><th>Giáº£i thÃ­ch</th></tr></thead><tbody><tr><td>ğŸ˜„<br><span class="highlight-text cyan">SÆ¡ cáº¥p</span></td><td>LÃ½ thuyáº¿t vÃ  phÆ°Æ¡ng phÃ¡p cÆ¡ báº£n phÃ¹ há»£p vá»›i ngÆ°á»i nháº­p mÃ´n. NÃªn Ä‘á»c nhá»¯ng pháº§n nÃ y trÆ°á»›c Ä‘á»ƒ náº¯m Ä‘Æ°á»£c ná»™i dung cÆ¡ báº£n.</td></tr><tr><td>ğŸ˜Š<br><span class="highlight-text green">Trung cáº¥p</span></td><td>CÆ¡ báº£n vá» suy luáº­n Bayes vÃ  má»™t sá»‘ ná»™i dung hÆ¡i nÃ¢ng cao má»™t chÃºt. NgoÃ i ra, cÃ²n cÃ³ má»™t sá»‘ phÆ°Æ¡ng phÃ¡p há»¯u dá»¥ng trong má»™t sá»‘ trÆ°á»ng há»£p Ä‘áº·c biá»‡t. Pháº§n nÃ y phÃ¹ há»£p vá»›i báº­c há»c tiáº¿n sÄ© vá»›i ná»™i dung hÆ¡i huÆ°á»›ng nÃ¢ng cao.</td></tr><tr><td>ğŸ˜°<br><span class="highlight-text yellow">NÃ¢ng cao</span></td><td>Ná»™i dung nÃ¢ng cao bao gá»“m cáº£ giáº£i thÃ­ch lÃ½ thuyáº¿t. Pháº§n nÃ y phÃ¹ há»£p vá»›i trÃ¬nh Ä‘á»™ tiáº¿n sÄ©, nhÃ  nghiÃªn vÃ  cÃ¡c kÄ© sÆ° há»c mÃ¡y.</td></tr></tbody></table><h1 id=2-cÃ¡c-chÆ°Æ¡ng>2. CÃ¡c chÆ°Æ¡ng</h1><h2 id=2-1-chÆ°Æ¡ng-1-introduction>2.1. ChÆ°Æ¡ng 1: Introduction</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜„ <span class="highlight-text cyan">1</span></td><td>Introduction</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.1</span></td><td>Example: Polynomial Curve Fitting</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.2</span></td><td>Probability Theory</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.2.1</span></td><td>Probability densities</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.2.2</span></td><td>Expectations and covariances</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.2.3</span></td><td>Bayesian probabilities</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.2.4</span></td><td>The Gaussian distribution</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.2.5</span></td><td>Curve fitting re-visited</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">1.2.6</span></td><td>Bayesian curve fitting</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.3</span></td><td>Model Selection</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.4</span></td><td>The Curse of Dimensionality</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.5</span></td><td>Decision Theory</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.5.1</span></td><td>Minimizing the misclassification rate</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.5.2</span></td><td>Minimizing the expected loss</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">1.5.3</span></td><td>The reject option</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">1.5.4</span></td><td>Inference and decision</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.5.5</span></td><td>Loss functions for regression</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.6</span></td><td>Information Theory</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">1.6.1</span></td><td>Relative entropy and mutual information</td></tr></tbody></table><h2 id=2-2-chÆ°Æ¡ng-2-probability-distributions>2.2. ChÆ°Æ¡ng 2: Probability Distributions</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜„ <span class="highlight-text cyan">2</span></td><td>Probability Distributions</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.1</span></td><td>Binary Variables</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.1.1</span></td><td>The beta distribution</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.2</span></td><td>Multinomial Variables</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.2.1</span></td><td>The Dirichlet distribution</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.3</span></td><td>The Gaussian Distribution</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.3.1</span></td><td>Conditional Gaussian distributions</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.3.2</span></td><td>Marginal Gaussian distributions</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.3.3</span></td><td>Bayesâ€™ theorem for Gaussian variables</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.3.4</span></td><td>Maximum likelihood for the Gaussian</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">2.3.5</span></td><td>Sequential estimation</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">2.3.6</span></td><td>Bayesian inference for the Gaussian</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">2.3.7</span></td><td>Studentâ€™s t-distribution</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">2.3.8</span></td><td>Periodic variables</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.3.9</span></td><td>Mixtures of Gaussians</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.4</span></td><td>The Exponential Family</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.4.1</span></td><td>Maximum likelihood and sufficient statistics</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">2.4.2</span></td><td>Conjugate priors</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">2.4.3</span></td><td>Noninformative priors</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.5</span></td><td>Nonparametric Methods</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.5.1</span></td><td>Kernel density estimators</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">2.5.2</span></td><td>Nearest-neighbour methods</td></tr></tbody></table><h2 id=2-3-chÆ°Æ¡ng-3-linear-models-for-regression>2.3. ChÆ°Æ¡ng 3: Linear Models for Regression</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜„ <span class="highlight-text cyan">3</span></td><td>Linear Models for Regression</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">3.1</span></td><td>Linear Basis Function Models</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">3.1.1</span></td><td>Maximum likelihood and least squares</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">3.1.2</span></td><td>Geometry of least squares</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">3.1.3</span></td><td>Sequential learning</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">3.1.4</span></td><td>Regularized least squares</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">3.1.5</span></td><td>Multiple outputs</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">3.2</span></td><td>The Bias-Variance Decomposition</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">3.3</span></td><td>Bayesian Linear Regression</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">3.3.1</span></td><td>Parameter distribution</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">3.3.2</span></td><td>Predictive distribution</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">3.3.3</span></td><td>Equivalent kernel</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">3.4</span></td><td>Bayesian Model Comparison</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">3.5</span></td><td>The Evidence Approximation</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">3.5.1</span></td><td>Evaluation of the evidence function</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">3.5.2</span></td><td>Maximizing the evidence function</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">3.5.3</span></td><td>Effective number of parameters</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">3.6</span></td><td>Limitations of Fixed Basis Functions</td></tr></tbody></table><h2 id=2-4-chÆ°Æ¡ng-4-linear-models-for-classification>2.4. ChÆ°Æ¡ng 4: Linear Models for Classification</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜„ <span class="highlight-text cyan">4</span></td><td>Linear Models for Regression</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.1</span></td><td>Discriminant Functions</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.1.1</span></td><td>Two classes</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.1.2</span></td><td>Multiple classes</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.1.3</span></td><td>Lest squares for classification</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.1.4</span></td><td>Fisher&rsquo;s linear discriminant</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.1.5</span></td><td>Relation to least squares</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.1.6</span></td><td>Fisher&rsquo;s discriminant for multiple classes</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.1.7</span></td><td>The perceptron algorithm</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.2</span></td><td>Probabilistic Generative Models</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.2.1</span></td><td>Continuous inputs</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.2.2</span></td><td>Maximum likelihood solution</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.2.3</span></td><td>Discrete features</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.2.4</span></td><td>Exponential family</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.3</span></td><td>Probabilistic Discriminant Models</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.3.1</span></td><td>Fixed basis functions</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.3.2</span></td><td>Logistic regression</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.3.3</span></td><td>Interative reweighted least squares</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">4.3.4</span></td><td>Multiclass logistic regression</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">4.3.5</span></td><td>Probit regression</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">4.3.6</span></td><td>Canonical link functions</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">4.4</span></td><td>The Laplace Approximation</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">4.4.1</span></td><td>Model comparison and BIC</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">4.5</span></td><td>Bayesian Logistic Regression</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">4.5.1</span></td><td>Laplace approximation</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">4.5.2</span></td><td>Predictive distribution</td></tr></tbody></table><h2 id=2-5-chÆ°Æ¡ng-5-neural-networks>2.5. ChÆ°Æ¡ng 5: Neural Networks</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜„ <span class="highlight-text cyan">5</span></td><td>Neural Networks</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.1</span></td><td>Feed-forward Networks Functions</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.1.1</span></td><td>Weight-space symmetries</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.2</span></td><td>Network Training</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.2.1</span></td><td>Parameter optimization</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.2.2</span></td><td>Local quadratic approximation</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.2.3</span></td><td>Use of gradient information</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.2.4</span></td><td>Gradient descent optimization</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.3</span></td><td>Error Backpropagation</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.3.1</span></td><td>Evaluation of error-function derivatives</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.3.2</span></td><td>A simple example</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">5.3.3</span></td><td>Efficiency of backpropagation</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.3.4</span></td><td>The Jacobian matrix</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.4</span></td><td>The Hessian Matrix</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.4.1</span></td><td>Diagonal approximation</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.4.2</span></td><td>Outer product approximation</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.4.3</span></td><td>Inverse Hessian</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.4.4</span></td><td>Finite differences</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.4.5</span></td><td>Exact evaluation of the Hessian</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.4.6</span></td><td>Fast multiplication by the Hessian</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">5.5</span></td><td>Regularization in Neural Networks</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">5.5.1</span></td><td>Consistent Gaussian priors</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">5.5.2</span></td><td>Early stopping</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.5.3</span></td><td>Invariances</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.5.4</span></td><td>Tangent propagation</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.5.5</span></td><td>Training with transformed data</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.5.6</span></td><td>Convolutional networks</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.5.7</span></td><td>Soft weight sharing</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.6</span></td><td>Mixture Density Networks</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.7</span></td><td>Bayesian Neural Networks</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.7.1</span></td><td>Posterior parameter distribution</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.7.2</span></td><td>Hyperparameter optimization</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">5.7.3</span></td><td>Bayesian neural networks for classification</td></tr></tbody></table><h2 id=2-6-chÆ°Æ¡ng-6-kernel-methods>2.6. ChÆ°Æ¡ng 6: Kernel Methods</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜„ <span class="highlight-text cyan">6</span></td><td>Kernel Methods</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">6.1</span></td><td>Dual Representaions</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">6.3</span></td><td>Constructing Kernels</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">6.3</span></td><td>Radial Basis Function Networks</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">6.3.1</span></td><td>Nadaraya-Watson model</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">6.4</span></td><td>Gaussian Processes</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">6.4.1</span></td><td>Linear regression revisited</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">6.4.2</span></td><td>Gaussian processes for regression</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">6.4.3</span></td><td>Learning the hyperparameter</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">6.4.4</span></td><td>Automatic relevance determination</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">6.4.5</span></td><td>Gaussian processes for classification</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">6.4.6</span></td><td>Laplace approximation</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">6.4.7</span></td><td>Connection to neural networks</td></tr></tbody></table><h2 id=2-7-chÆ°Æ¡ng-7-sparse-kernel-machines>2.7. ChÆ°Æ¡ng 7: Sparse Kernel Machines</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜„ <span class="highlight-text cyan">7</span></td><td>Sparse Kernel Machines</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">7.1</span></td><td>Maximum Margin Classifiers</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">7.1.1</span></td><td>Overlapping class distributions</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">7.1.2</span></td><td>Relation to logistic regression</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">7.1.3</span></td><td>Multiclass SVMs</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">7.1.4</span></td><td>SVMs for regression</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">7.1.5</span></td><td>Computational learning theory</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">7.2</span></td><td>Relevance Vector Machines</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">7.2.1</span></td><td>RVM for regression</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">7.2.2</span></td><td>Analysis of sparsity</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">7.2.3</span></td><td>RVM for classification</td></tr></tbody></table><h2 id=2-8-chÆ°Æ¡ng-8-graphical-models>2.8. ChÆ°Æ¡ng 8: Graphical Models</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜Š <span class="highlight-text green">8</span></td><td>Graphical Models</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.1</span></td><td>Bayesian Networks</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.1.1</span></td><td>Example: Polynomial regression</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.1.2</span></td><td>Generative models</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.1.3</span></td><td>Discrete variables</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.1.4</span></td><td>Linear-Gaussian models</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.2</span></td><td>Conditional Independence</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.2.1</span></td><td>Three example graphs</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.2.2</span></td><td>D-separation</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.3</span></td><td>Markov Random Fields</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.3.1</span></td><td>Conditional independence properties</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.3.2</span></td><td>Factorization properties</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.3.3</span></td><td>Illustration: Image de-noising</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.3.4</span></td><td>Relation to directed graphs</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.4</span></td><td>inference in Graphical Models</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.4.1</span></td><td>Inference on a chain</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.4.2</span></td><td>Trees</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.4.3</span></td><td>Factor graphs</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.4.4</span></td><td>The sum-product algorithm</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">8.4.5</span></td><td>The max-sum algorithm</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">8.4.6</span></td><td>Exact inference in general graphs</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">8.4.7</span></td><td>Loopy belief propagation</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">8.4.8</span></td><td>Learning the graph structure</td></tr></tbody></table><h2 id=2-9-chÆ°Æ¡ng-9-mixture-models-and-em>2.9. ChÆ°Æ¡ng 9: Mixture Models and EM</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜„ <span class="highlight-text cyan">9</span></td><td>Mixture Models and EM</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">9.1</span></td><td>K-means Clustering</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">9.1.1</span></td><td>Image segmentation and compression</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">9.2</span></td><td>Mixtures of Gaussians</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">9.2.1</span></td><td>Maximum likelihood</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">9.2.2</span></td><td>EM for Gaussian mixtures</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">9.3</span></td><td>An Alternative View of EM</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">9.3.1</span></td><td>Gaussian mixtures revisited</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">9.3.2</span></td><td>Relation to K-means</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">9.3.3</span></td><td>Mixtures of Bernoulli distributions</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">9.3.4</span></td><td>EM for Bayesian linear regression</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">9.4</span></td><td>The EM Algorithm in General</td></tr></tbody></table><h2 id=2-10-chÆ°Æ¡ng-10-approximate-inference>2.10. ChÆ°Æ¡ng 10: Approximate Inference</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜Š <span class="highlight-text green">10</span></td><td>Approximate Inference</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">10.1</span></td><td>Variational Inference</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">10.1.1</span></td><td>Factorized distributions</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">10.1.2</span></td><td>Properties of factorized approximations</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">10.1.3</span></td><td>Example: The univariate Gaussian</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">10.1.4</span></td><td>Model comparison</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">10.2</span></td><td>Illustration: Variational Mixture of Gaussians</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">10.2.1</span></td><td>Variational distribution</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">10.2.2</span></td><td>Variational lower bound</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">10.2.3</span></td><td>Predictive density</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.2.4</span></td><td>Determining the number of components</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.2.5</span></td><td>Induced factorizations</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.3</span></td><td>Variational Linear Regression</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.3.1</span></td><td>Variational distribution</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.3.2</span></td><td>Predictive distribution</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.3.3</span></td><td>Lower bound</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.4</span></td><td>Exponential Family Distributions</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.4.1</span></td><td>Variational message passing</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.5</span></td><td>Local Variational Methods</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.6</span></td><td>Variational Logistic Regression</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.6.1</span></td><td>Variational posterior distribution</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.6.2</span></td><td>Optimizing the variational parameters</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.6.3</span></td><td>Inference of hyperparameters</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.7</span></td><td>Expectation Propagation</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.7.1</span></td><td>Example: The clutter problem</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">10.7.2</span></td><td>Expectation propagation of graphs</td></tr></tbody></table><h2 id=2-11-chÆ°Æ¡ng-11-sampling-methods>2.11. ChÆ°Æ¡ng 11: Sampling Methods</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜Š <span class="highlight-text green">11</span></td><td>Sampling Methods</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">11.1</span></td><td>Basis Sampling Algorithms</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">11.1.1</span></td><td>Standard distributions</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">11.1.2</span></td><td>Rejection sampling</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">11.1.3</span></td><td>Adaptive rejection sampling</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">11.1.4</span></td><td>Importance sampling</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">11.1.5</span></td><td>Sampling-importance-resampling</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">11.1.6</span></td><td>Sampling and EM algorithm</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">11.2</span></td><td>Markov Chain Monte Carlo</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">11.2.1</span></td><td>Markov chains</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">11.2.2</span></td><td>The Metropolis-Hastings algorithm</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">11.3</span></td><td>Gibbs Sampling</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">11.4</span></td><td>Slice Sampling</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">11.5</span></td><td>The Hybrid Monte Carlo Algorithm</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">11.5.1</span></td><td>Dynamical systems</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">11.5.2</span></td><td>Hybrid Monte Carlo</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">11.6</span></td><td>Estimating the Partition Function</td></tr></tbody></table><h2 id=2-12-chÆ°Æ¡ng-12-continuous-latent-variables>2.12. ChÆ°Æ¡ng 12: Continuous Latent Variables</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜„ <span class="highlight-text cyan">12</span></td><td>Continuous Latent Variables</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">12.1</span></td><td>Principal Component Analysis</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">12.1.1</span></td><td>Maximum variance formulation</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">12.1.2</span></td><td>Minimum-error formulation</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">12.1.3</span></td><td>Applications of peA</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">12.1.4</span></td><td>PCA for high-dimensional data</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">12.2</span></td><td>Probabilistic p e A</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">12.2.1</span></td><td>Maximum likelihood peA</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">12.2.2</span></td><td>EM algorithm for peA</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">12.2.3</span></td><td>Bayesian peA</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">12.2.4</span></td><td>Factor analysis</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">12.3</span></td><td>Kernel PCA</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">12.4</span></td><td>Nonliear Latent Variable Models</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">12.4.1</span></td><td>Independent component analysis</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">12.4.2</span></td><td>Autoassociative neural networks</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">12.4.3</span></td><td>Modelling nonlinear manifolds</td></tr></tbody></table><h2 id=2-13-chÆ°Æ¡ng-13-sequential-data>2.13. ChÆ°Æ¡ng 13: Sequential Data</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜Š <span class="highlight-text green">13</span></td><td>Sequential Data</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">13.1</span></td><td>Markov Models</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">13.2</span></td><td>Hidden Markov Models</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">13.2.1</span></td><td>Maximum likelihood for the HMM</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">13.2.2</span></td><td>The forward-backward algorithm</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">13.2.3</span></td><td>The sum-product algorithm for the HMM</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">13.2.4</span></td><td>Scaling factors</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">13.2.5</span></td><td>The Viterbi algorithm</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">13.2.6</span></td><td>Extensions of the hidden Markov model</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">13.3</span></td><td>Linear Dynamical Systems</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">13.3.1</span></td><td>Inference in LDS</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">13.3.2</span></td><td>Learning in LDS</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">13.3.3</span></td><td>Extensions of LDS</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">13.3.4</span></td><td>Particle filters</td></tr></tbody></table><h2 id=2-14-chÆ°Æ¡ng-14-combining-models>2.14. ChÆ°Æ¡ng 14: Combining Models</h2><table><thead><tr><th>Má»¥c</th><th>TiÃªu Ä‘á»</th></tr></thead><tbody><tr><td>ğŸ˜Š <span class="highlight-text green">14</span></td><td>Combining Models</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">14.1</span></td><td>Bayesian Model Averaging</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">14.2</span></td><td>Committees</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">14.3</span></td><td>Boosting</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">14.3.1</span></td><td>Minimizing exponential error</td></tr><tr><td>ğŸ˜Š <span class="highlight-text green">14.3.2</span></td><td>Error functions for boosting</td></tr><tr><td>ğŸ˜„ <span class="highlight-text cyan">14.4</span></td><td>Tree-based Models</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">14.5</span></td><td>Conditional Mixture Models</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">14.5.1</span></td><td>Mixtures of linear regression models</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">14.5.2</span></td><td>Mixtures of logistic models</td></tr><tr><td>ğŸ˜° <span class="highlight-text yellow">14.5.3</span></td><td>Mixtures of experts</td></tr></tbody></table></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THáºº ÄÃNH Dáº¤U</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Há»c MÃ¡y</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/ data-tooltip="[ML] Há»“i quy tuyáº¿n tÃ­nh (Linear Regression)"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiáº¿p</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-intro/ data-tooltip="[ML] Há»c mÃ¡y lÃ  gÃ¬?"><span class="hide-xs hide-sm text-small icon-mr">TrÆ°á»›c</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiáº¿ng Viá»‡t" value=vi selected>Tiáº¿ng Viá»‡t (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=æ—¥æœ¬èª value=ja>æ—¥æœ¬èª (ja)</option></select></div></div><div id=topic><label>Chá»§ Ä‘á»</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Láº­p TrÃ¬nh</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Há»c MÃ¡y</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>ToÃ¡n</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>XÃ¡c Suáº¥t</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>SÃ¡ch</a></li></ul></div><div id=contact><label>LiÃªn há»‡</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gá»­i tin nháº¯n</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2017 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/ data-tooltip="[ML] Há»“i quy tuyáº¿n tÃ­nh (Linear Regression)"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiáº¿p</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-intro/ data-tooltip="[ML] Há»c mÃ¡y lÃ  gÃ¬?"><span class="hide-xs hide-sm text-small icon-mr">TrÆ°á»›c</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-prml%2F"><i class="fa fa-facebook-official"></i><span>Chia sáº» vá»›i Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-prml%2F"><i class="fa fa-twitter"></i><span>Chia sáº» vá»›i Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-prml%2F"><i class="fa fa-google-plus"></i><span>Chia sáº» vá»›i Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="áº¢nh Ä‘áº¡i diá»‡n"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-GR8SEkOO1rBN/jnOcQDFcFmwXAevSLx7/Io9Ps1rkxWp983ZIuUGfxivlF/5f5eJ src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.js></script><script crossorigin=anonymous integrity=sha384-cXpztMJlr2xFXyDSIfRWYSMVCXZ9HeGXvzyKTYrn03rsMAlOtIQVzjty5ULbaP8L src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/12\/ml-prml\/';this.page.identifier='\/vi\/2017\/12\/ml-prml\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>