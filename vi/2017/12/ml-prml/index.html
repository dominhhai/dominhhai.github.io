<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.26 with theme Tranquilpeak 0.4.1-BETA"><title>Pattern Recognition and Machine Learning</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Học Máy,Pattern Recognition and Machine Learning,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/12/ml-prml/><meta name=description content="Được coi là sách giáo khoa cho những người làm học máy, cuốn sách này viết về các giải thuật và lý thuyết xây dựng các giải thuật nhận dạng mẫu và học máy. Tuy nhiên lúc mới đọc thì thấy khá khó nhằn nên tôi đã tìm hiểu độ khó các phần đề biết đường mà đọc."><meta property=og:type content=website><meta property=og:title content="Pattern Recognition and Machine Learning"><meta property=og:url content=https://dominhhai.github.io/vi/2017/12/ml-prml/><meta property=og:description content="Được coi là sách giáo khoa cho những người làm học máy, cuốn sách này viết về các giải thuật và lý thuyết xây dựng các giải thuật nhận dạng mẫu và học máy. Tuy nhiên lúc mới đọc thì thấy khá khó nhằn nên tôi đã tìm hiểu độ khó các phần đề biết đường mà đọc."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="Pattern Recognition and Machine Learning"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/12/ml-prml/><meta name=twitter:description content="Được coi là sách giáo khoa cho những người làm học máy, cuốn sách này viết về các giải thuật và lý thuyết xây dựng các giải thuật nhận dạng mẫu và học máy. Tuy nhiên lúc mới đọc thì thấy khá khó nhằn nên tôi đã tìm hiểu độ khó các phần đề biết đường mà đọc."><meta name=twitter:creator content=@minhhai3b><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/ml/prml.jpg><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/ml/prml.jpg><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-fpbzgxsy0kgmdvyrj5ykkg6ratccrk3gocmaqn4xpcjywmv5dteilzucro4f.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.css integrity=sha384-8QOKbPtTFvh/lMY0qPVbXj9hDh+v8US0pD//FcoYFst2lCIf0BmT58+Heqj0IGyx><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/#about><i class="sidebar-button-icon fa fa-lg fa-address-card"></i><span class=sidebar-button-desc>Thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>Pattern Recognition and Machine Learning</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-12-12T16:36:56&#43;09:00>12 tháng 12, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Được coi là sách giáo khoa cho những người làm học máy, cuốn sách này viết về các giải thuật và lý thuyết xây dựng các giải thuật nhận dạng mẫu và học máy. Tuy nhiên lúc mới đọc thì thấy khá khó nhằn nên tôi đã tìm hiểu độ khó các phần đề biết đường mà đọc.</p><p>Độ khó này không phải do tôi nghĩ ra mà là lấy từ trang <a href=http://ibisforest.org/index.php?PRML%2Fcourse target=_blank _ rel="noopener noreferrer">trợ giúp sách này của Nhật</a>. Tôi mới đọc hết 1 phần cuốn này và thấy đánh giá ở trang đó có vẻ đúng. Tôi thì ưu tiên đọc phần dễ và vừa trước rồi mới tới các phần nâng cao. Tuy nhiên không phải là không đọc tí nào phần nâng cao mà có đọc 1 chút dù không hiểu gì cũng mặc.</p><h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-các-cấp-độ>1. Các cấp độ</a></li><li><a href=#2-các-chương>2. Các chương</a><ul><li><a href=#2-1-chương-1-introduction>2.1. Chương 1: Introduction</a></li><li><a href=#2-2-chương-2-probability-distributions>2.2. Chương 2: Probability Distributions</a></li><li><a href=#2-3-chương-3-linear-models-for-regression>2.3. Chương 3: Linear Models for Regression</a></li><li><a href=#2-4-chương-4-linear-models-for-classification>2.4. Chương 4: Linear Models for Classification</a></li><li><a href=#2-5-chương-5-neural-networks>2.5. Chương 5: Neural Networks</a></li><li><a href=#2-6-chương-6-kernel-methods>2.6. Chương 6: Kernel Methods</a></li><li><a href=#2-7-chương-7-sparse-kernel-machines>2.7. Chương 7: Sparse Kernel Machines</a></li><li><a href=#2-8-chương-8-graphical-models>2.8. Chương 8: Graphical Models</a></li><li><a href=#2-9-chương-9-mixture-models-and-em>2.9. Chương 9: Mixture Models and EM</a></li><li><a href=#2-10-chương-10-approximate-inference>2.10. Chương 10: Approximate Inference</a></li><li><a href=#2-11-chương-11-sampling-methods>2.11. Chương 11: Sampling Methods</a></li><li><a href=#2-12-chương-12-continuous-latent-variables>2.12. Chương 12: Continuous Latent Variables</a></li><li><a href=#2-13-chương-13-sequential-data>2.13. Chương 13: Sequential Data</a></li><li><a href=#2-14-chương-14-combining-models>2.14. Chương 14: Combining Models</a></li></ul></li></ul></nav><h1 id=1-các-cấp-độ>1. Các cấp độ</h1><table><thead><tr><th>Cấp độ</th><th>Giải thích</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">Sơ cấp</span></td><td>Lý thuyết và phương pháp cơ bản phù hợp với người nhập môn. Nên đọc những phần này trước để nắm được nội dung cơ bản.</td></tr><tr><td><span class="highlight-text green">Trung cấp</span></td><td>Cơ bản về suy luận Bayes và một số nội dung hơi nâng cao một chút. Ngoài ra, còn có một số phương pháp hữu dụng trong một số trường hợp đặc biệt. Phần này phù hợp với bậc học tiến sĩ với nội dung hơi huướng nâng cao.</td></tr><tr><td><span class="highlight-text yellow">Nâng cao</span></td><td>Nội dung nâng cao bao gồm cả giải thích lý thuyết. Phần này phù hợp với trình độ tiến sĩ, nhà nghiên và các kĩ sư học máy.</td></tr></tbody></table><h1 id=2-các-chương>2. Các chương</h1><h2 id=2-1-chương-1-introduction>2.1. Chương 1: Introduction</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">1</span></td><td>Introduction</td></tr><tr><td><span class="highlight-text cyan">1.1</span></td><td>Example: Polynomial Curve Fitting</td></tr><tr><td><span class="highlight-text cyan">1.2</span></td><td>Probability Theory</td></tr><tr><td><span class="highlight-text cyan">1.2.1</span></td><td>Probability densities</td></tr><tr><td><span class="highlight-text cyan">1.2.2</span></td><td>Expectations and covariances</td></tr><tr><td><span class="highlight-text cyan">1.2.3</span></td><td>Bayesian probabilities</td></tr><tr><td><span class="highlight-text cyan">1.2.4</span></td><td>The Gaussian distribution</td></tr><tr><td><span class="highlight-text cyan">1.2.5</span></td><td>Curve fitting re-visited</td></tr><tr><td><span class="highlight-text green">1.2.6</span></td><td>Bayesian curve fitting</td></tr><tr><td><span class="highlight-text cyan">1.3</span></td><td>Model Selection</td></tr><tr><td><span class="highlight-text cyan">1.4</span></td><td>The Curse of Dimensionality</td></tr><tr><td><span class="highlight-text cyan">1.5</span></td><td>Decision Theory</td></tr><tr><td><span class="highlight-text cyan">1.5.1</span></td><td>Minimizing the misclassification rate</td></tr><tr><td><span class="highlight-text cyan">1.5.2</span></td><td>Minimizing the expected loss</td></tr><tr><td><span class="highlight-text yellow">1.5.3</span></td><td>The reject option</td></tr><tr><td><span class="highlight-text green">1.5.4</span></td><td>Inference and decision</td></tr><tr><td><span class="highlight-text cyan">1.5.5</span></td><td>Loss functions for regression</td></tr><tr><td><span class="highlight-text cyan">1.6</span></td><td>Information Theory</td></tr><tr><td><span class="highlight-text cyan">1.6.1</span></td><td>Relative entropy and mutual information</td></tr></tbody></table><h2 id=2-2-chương-2-probability-distributions>2.2. Chương 2: Probability Distributions</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">2</span></td><td>Probability Distributions</td></tr><tr><td><span class="highlight-text cyan">2.1</span></td><td>Binary Variables</td></tr><tr><td><span class="highlight-text cyan">2.1.1</span></td><td>The beta distribution</td></tr><tr><td><span class="highlight-text cyan">2.2</span></td><td>Multinomial Variables</td></tr><tr><td><span class="highlight-text cyan">2.2.1</span></td><td>The Dirichlet distribution</td></tr><tr><td><span class="highlight-text cyan">2.3</span></td><td>The Gaussian Distribution</td></tr><tr><td><span class="highlight-text cyan">2.3.1</span></td><td>Conditional Gaussian distributions</td></tr><tr><td><span class="highlight-text cyan">2.3.2</span></td><td>Marginal Gaussian distributions</td></tr><tr><td><span class="highlight-text cyan">2.3.3</span></td><td>Bayes’ theorem for Gaussian variables</td></tr><tr><td><span class="highlight-text cyan">2.3.4</span></td><td>Maximum likelihood for the Gaussian</td></tr><tr><td><span class="highlight-text green">2.3.5</span></td><td>Sequential estimation</td></tr><tr><td><span class="highlight-text green">2.3.6</span></td><td>Bayesian inference for the Gaussian</td></tr><tr><td><span class="highlight-text green">2.3.7</span></td><td>Student’s t-distribution</td></tr><tr><td><span class="highlight-text yellow">2.3.8</span></td><td>Periodic variables</td></tr><tr><td><span class="highlight-text cyan">2.3.9</span></td><td>Mixtures of Gaussians</td></tr><tr><td><span class="highlight-text cyan">2.4</span></td><td>The Exponential Family</td></tr><tr><td><span class="highlight-text cyan">2.4.1</span></td><td>Maximum likelihood and sufficient statistics</td></tr><tr><td><span class="highlight-text green">2.4.2</span></td><td>Conjugate priors</td></tr><tr><td><span class="highlight-text green">2.4.3</span></td><td>Noninformative priors</td></tr><tr><td><span class="highlight-text cyan">2.5</span></td><td>Nonparametric Methods</td></tr><tr><td><span class="highlight-text cyan">2.5.1</span></td><td>Kernel density estimators</td></tr><tr><td><span class="highlight-text cyan">2.5.2</span></td><td>Nearest-neighbour methods</td></tr></tbody></table><h2 id=2-3-chương-3-linear-models-for-regression>2.3. Chương 3: Linear Models for Regression</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">3</span></td><td>Linear Models for Regression</td></tr><tr><td><span class="highlight-text cyan">3.1</span></td><td>Linear Basis Function Models</td></tr><tr><td><span class="highlight-text cyan">3.1.1</span></td><td>Maximum likelihood and least squares</td></tr><tr><td><span class="highlight-text yellow">3.1.2</span></td><td>Geometry of least squares</td></tr><tr><td><span class="highlight-text green">3.1.3</span></td><td>Sequential learning</td></tr><tr><td><span class="highlight-text green">3.1.4</span></td><td>Regularized least squares</td></tr><tr><td><span class="highlight-text yellow">3.1.5</span></td><td>Multiple outputs</td></tr><tr><td><span class="highlight-text cyan">3.2</span></td><td>The Bias-Variance Decomposition</td></tr><tr><td><span class="highlight-text green">3.3</span></td><td>Bayesian Linear Regression</td></tr><tr><td><span class="highlight-text green">3.3.1</span></td><td>Parameter distribution</td></tr><tr><td><span class="highlight-text green">3.3.2</span></td><td>Predictive distribution</td></tr><tr><td><span class="highlight-text green">3.3.3</span></td><td>Equivalent kernel</td></tr><tr><td><span class="highlight-text yellow">3.4</span></td><td>Bayesian Model Comparison</td></tr><tr><td><span class="highlight-text yellow">3.5</span></td><td>The Evidence Approximation</td></tr><tr><td><span class="highlight-text yellow">3.5.1</span></td><td>Evaluation of the evidence function</td></tr><tr><td><span class="highlight-text yellow">3.5.2</span></td><td>Maximizing the evidence function</td></tr><tr><td><span class="highlight-text yellow">3.5.3</span></td><td>Effective number of parameters</td></tr><tr><td><span class="highlight-text green">3.6</span></td><td>Limitations of Fixed Basis Functions</td></tr></tbody></table><h2 id=2-4-chương-4-linear-models-for-classification>2.4. Chương 4: Linear Models for Classification</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">3</span></td><td>Linear Models for Regression</td></tr><tr><td><span class="highlight-text cyan">3.1</span></td><td>Linear Basis Function Models</td></tr><tr><td><span class="highlight-text cyan">3.1.1</span></td><td>Maximum likelihood and least squares</td></tr><tr><td><span class="highlight-text yellow">3.1.2</span></td><td>Geometry of least squares</td></tr><tr><td><span class="highlight-text green">3.1.3</span></td><td>Sequential learning</td></tr><tr><td><span class="highlight-text green">3.1.4</span></td><td>Regularized least squares</td></tr><tr><td><span class="highlight-text yellow">3.1.5</span></td><td>Multiple outputs</td></tr><tr><td><span class="highlight-text cyan">3.2</span></td><td>The Bias-Variance Decomposition</td></tr><tr><td><span class="highlight-text green">3.3</span></td><td>Bayesian Linear Regression</td></tr><tr><td><span class="highlight-text green">3.3.1</span></td><td>Parameter distribution</td></tr><tr><td><span class="highlight-text green">3.3.2</span></td><td>Predictive distribution</td></tr><tr><td><span class="highlight-text green">3.3.3</span></td><td>Equivalent kernel</td></tr><tr><td><span class="highlight-text yellow">3.4</span></td><td>Bayesian Model Comparison</td></tr><tr><td><span class="highlight-text yellow">3.5</span></td><td>The Evidence Approximation</td></tr><tr><td><span class="highlight-text yellow">3.5.1</span></td><td>Evaluation of the evidence function</td></tr><tr><td><span class="highlight-text yellow">3.5.2</span></td><td>Maximizing the evidence function</td></tr><tr><td><span class="highlight-text yellow">3.5.3</span></td><td>Effective number of parameters</td></tr><tr><td><span class="highlight-text green">3.6</span></td><td>Limitations of Fixed Basis Functions</td></tr></tbody></table><h2 id=2-5-chương-5-neural-networks>2.5. Chương 5: Neural Networks</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">5</span></td><td>Neural Networks</td></tr><tr><td><span class="highlight-text cyan">5.1</span></td><td>Feed-forward Networks Functions</td></tr><tr><td><span class="highlight-text cyan">5.1.1</span></td><td>Weight-space symmetries</td></tr><tr><td><span class="highlight-text cyan">5.2</span></td><td>Network Training</td></tr><tr><td><span class="highlight-text cyan">5.2.1</span></td><td>Parameter optimization</td></tr><tr><td><span class="highlight-text cyan">5.2.2</span></td><td>Local quadratic approximation</td></tr><tr><td><span class="highlight-text cyan">5.2.3</span></td><td>Use of gradient information</td></tr><tr><td><span class="highlight-text cyan">5.2.4</span></td><td>Gradient descent optimization</td></tr><tr><td><span class="highlight-text cyan">5.3</span></td><td>Error Backpropagation</td></tr><tr><td><span class="highlight-text cyan">5.3.1</span></td><td>Evaluation of error-function derivatives</td></tr><tr><td><span class="highlight-text cyan">5.3.2</span></td><td>A simple example</td></tr><tr><td><span class="highlight-text cyan">5.3.3</span></td><td>Efficiency of backpropagation</td></tr><tr><td><span class="highlight-text yellow">5.3.4</span></td><td>The Jacobian matrix</td></tr><tr><td><span class="highlight-text yellow">5.4</span></td><td>The Hessian Matrix</td></tr><tr><td><span class="highlight-text yellow">5.4.1</span></td><td>Diagonal approximation</td></tr><tr><td><span class="highlight-text yellow">5.4.2</span></td><td>Outer product approximation</td></tr><tr><td><span class="highlight-text yellow">5.4.3</span></td><td>Inverse Hessian</td></tr><tr><td><span class="highlight-text yellow">5.4.4</span></td><td>Finite differences</td></tr><tr><td><span class="highlight-text yellow">5.4.5</span></td><td>Exact evaluation of the Hessian</td></tr><tr><td><span class="highlight-text yellow">5.4.6</span></td><td>Fast multiplication by the Hessian</td></tr><tr><td><span class="highlight-text green">5.5</span></td><td>Regularization in Neural Networks</td></tr><tr><td><span class="highlight-text green">5.5.1</span></td><td>Consistent Gaussian priors</td></tr><tr><td><span class="highlight-text green">5.5.2</span></td><td>Early stopping</td></tr><tr><td><span class="highlight-text yellow">5.5.3</span></td><td>Invariances</td></tr><tr><td><span class="highlight-text yellow">5.5.4</span></td><td>Tangent propagation</td></tr><tr><td><span class="highlight-text yellow">5.5.5</span></td><td>Training with transformed data</td></tr><tr><td><span class="highlight-text yellow">5.5.6</span></td><td>Convolutional networks</td></tr><tr><td><span class="highlight-text yellow">5.5.7</span></td><td>Soft weight sharing</td></tr><tr><td><span class="highlight-text yellow">5.6</span></td><td>Mixture Density Networks</td></tr><tr><td><span class="highlight-text yellow">5.7</span></td><td>Bayesian Neural Networks</td></tr><tr><td><span class="highlight-text yellow">5.7.1</span></td><td>Posterior parameter distribution</td></tr><tr><td><span class="highlight-text yellow">5.7.2</span></td><td>Hyperparameter optimization</td></tr><tr><td><span class="highlight-text yellow">5.7.3</span></td><td>Bayesian neural networks for classification</td></tr></tbody></table><h2 id=2-6-chương-6-kernel-methods>2.6. Chương 6: Kernel Methods</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">6</span></td><td>Kernel Methods</td></tr><tr><td><span class="highlight-text cyan">6.1</span></td><td>Dual Representaions</td></tr><tr><td><span class="highlight-text cyan">6.3</span></td><td>Constructing Kernels</td></tr><tr><td><span class="highlight-text cyan">6.3</span></td><td>Radial Basis Function Networks</td></tr><tr><td><span class="highlight-text cyan">6.3.1</span></td><td>Nadaraya-Watson model</td></tr><tr><td><span class="highlight-text yellow">6.4</span></td><td>Gaussian Processes</td></tr><tr><td><span class="highlight-text yellow">6.4.1</span></td><td>Linear regression revisited</td></tr><tr><td><span class="highlight-text yellow">6.4.2</span></td><td>Gaussian processes for regression</td></tr><tr><td><span class="highlight-text yellow">6.4.3</span></td><td>Learning the hyperparameter</td></tr><tr><td><span class="highlight-text yellow">6.4.4</span></td><td>Automatic relevance determination</td></tr><tr><td><span class="highlight-text yellow">6.4.5</span></td><td>Gaussian processes for classification</td></tr><tr><td><span class="highlight-text yellow">6.4.6</span></td><td>Laplace approximation</td></tr><tr><td><span class="highlight-text yellow">6.4.7</span></td><td>Connection to neural networks</td></tr></tbody></table><h2 id=2-7-chương-7-sparse-kernel-machines>2.7. Chương 7: Sparse Kernel Machines</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">7</span></td><td>Sparse Kernel Machines</td></tr><tr><td><span class="highlight-text cyan">7.1</span></td><td>Maximum Margin Classifiers</td></tr><tr><td><span class="highlight-text cyan">7.1.1</span></td><td>Overlapping class distributions</td></tr><tr><td><span class="highlight-text green">7.1.2</span></td><td>Relation to logistic regression</td></tr><tr><td><span class="highlight-text green">7.1.3</span></td><td>Multiclass SVMs</td></tr><tr><td><span class="highlight-text green">7.1.4</span></td><td>SVMs for regression</td></tr><tr><td><span class="highlight-text green">7.1.5</span></td><td>Computational learning theory</td></tr><tr><td><span class="highlight-text yellow">7.2</span></td><td>Relevance Vector Machines</td></tr><tr><td><span class="highlight-text yellow">7.2.1</span></td><td>RVM for regression</td></tr><tr><td><span class="highlight-text yellow">7.2.2</span></td><td>Analysis of sparsity</td></tr><tr><td><span class="highlight-text yellow">7.2.3</span></td><td>RVM for classification</td></tr></tbody></table><h2 id=2-8-chương-8-graphical-models>2.8. Chương 8: Graphical Models</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text green">8</span></td><td>Graphical Models</td></tr><tr><td><span class="highlight-text green">8.1</span></td><td>Bayesian Networks</td></tr><tr><td><span class="highlight-text green">8.1.1</span></td><td>Example: Polynomial regression</td></tr><tr><td><span class="highlight-text green">8.1.2</span></td><td>Generative models</td></tr><tr><td><span class="highlight-text green">8.1.3</span></td><td>Discrete variables</td></tr><tr><td><span class="highlight-text green">8.1.4</span></td><td>Linear-Gaussian models</td></tr><tr><td><span class="highlight-text green">8.2</span></td><td>Conditional Independence</td></tr><tr><td><span class="highlight-text green">8.2.1</span></td><td>Three example graphs</td></tr><tr><td><span class="highlight-text green">8.2.2</span></td><td>D-separation</td></tr><tr><td><span class="highlight-text green">8.3</span></td><td>Markov Random Fields</td></tr><tr><td><span class="highlight-text green">8.3.1</span></td><td>Conditional independence properties</td></tr><tr><td><span class="highlight-text green">8.3.2</span></td><td>Factorization properties</td></tr><tr><td><span class="highlight-text green">8.3.3</span></td><td>Illustration: Image de-noising</td></tr><tr><td><span class="highlight-text green">8.3.4</span></td><td>Relation to directed graphs</td></tr><tr><td><span class="highlight-text green">8.4</span></td><td>inference in Graphical Models</td></tr><tr><td><span class="highlight-text green">8.4.1</span></td><td>Inference on a chain</td></tr><tr><td><span class="highlight-text green">8.4.2</span></td><td>Trees</td></tr><tr><td><span class="highlight-text green">8.4.3</span></td><td>Factor graphs</td></tr><tr><td><span class="highlight-text green">8.4.4</span></td><td>The sum-product algorithm</td></tr><tr><td><span class="highlight-text green">8.4.5</span></td><td>The max-sum algorithm</td></tr><tr><td><span class="highlight-text yellow">8.4.6</span></td><td>Exact inference in general graphs</td></tr><tr><td><span class="highlight-text yellow">8.4.7</span></td><td>Loopy belief propagation</td></tr><tr><td><span class="highlight-text yellow">8.4.8</span></td><td>Learning the graph structure</td></tr></tbody></table><h2 id=2-9-chương-9-mixture-models-and-em>2.9. Chương 9: Mixture Models and EM</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">9</span></td><td>Mixture Models and EM</td></tr><tr><td><span class="highlight-text cyan">9.1</span></td><td>K-means Clustering</td></tr><tr><td><span class="highlight-text cyan">9.1.1</span></td><td>Image segmentation and compression</td></tr><tr><td><span class="highlight-text cyan">9.2</span></td><td>Mixtures of Gaussians</td></tr><tr><td><span class="highlight-text cyan">9.2.1</span></td><td>Maximum likelihood</td></tr><tr><td><span class="highlight-text cyan">9.2.2</span></td><td>EM for Gaussian mixtures</td></tr><tr><td><span class="highlight-text cyan">9.3</span></td><td>An Alternative View of EM</td></tr><tr><td><span class="highlight-text cyan">9.3.1</span></td><td>Gaussian mixtures revisited</td></tr><tr><td><span class="highlight-text cyan">9.3.2</span></td><td>Relation to K-means</td></tr><tr><td><span class="highlight-text yellow">9.3.3</span></td><td>Mixtures of Bernoulli distributions</td></tr><tr><td><span class="highlight-text yellow">9.3.4</span></td><td>EM for Bayesian linear regression</td></tr><tr><td><span class="highlight-text green">9.4</span></td><td>The EM Algorithm in General</td></tr></tbody></table><h2 id=2-10-chương-10-approximate-inference>2.10. Chương 10: Approximate Inference</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text green">10</span></td><td>Approximate Inference</td></tr><tr><td><span class="highlight-text green">10.1</span></td><td>Variational Inference</td></tr><tr><td><span class="highlight-text green">10.1.1</span></td><td>Factorized distributions</td></tr><tr><td><span class="highlight-text green">10.1.2</span></td><td>Properties of factorized approximations</td></tr><tr><td><span class="highlight-text green">10.1.3</span></td><td>Example: The univariate Gaussian</td></tr><tr><td><span class="highlight-text green">10.1.4</span></td><td>Model comparison</td></tr><tr><td><span class="highlight-text green">10.2</span></td><td>Illustration: Variational Mixture of Gaussians</td></tr><tr><td><span class="highlight-text green">10.2.1</span></td><td>Variational distribution</td></tr><tr><td><span class="highlight-text green">10.2.2</span></td><td>Variational lower bound</td></tr><tr><td><span class="highlight-text green">10.2.3</span></td><td>Predictive density</td></tr><tr><td><span class="highlight-text yellow">10.2.4</span></td><td>Determining the number of components</td></tr><tr><td><span class="highlight-text yellow">10.2.5</span></td><td>Induced factorizations</td></tr><tr><td><span class="highlight-text yellow">10.3</span></td><td>Variational Linear Regression</td></tr><tr><td><span class="highlight-text yellow">10.3.1</span></td><td>Variational distribution</td></tr><tr><td><span class="highlight-text yellow">10.3.2</span></td><td>Predictive distribution</td></tr><tr><td><span class="highlight-text yellow">10.3.3</span></td><td>Lower bound</td></tr><tr><td><span class="highlight-text yellow">10.4</span></td><td>Exponential Family Distributions</td></tr><tr><td><span class="highlight-text yellow">10.4.1</span></td><td>Variational message passing</td></tr><tr><td><span class="highlight-text yellow">10.5</span></td><td>Local Variational Methods</td></tr><tr><td><span class="highlight-text yellow">10.6</span></td><td>Variational Logistic Regression</td></tr><tr><td><span class="highlight-text yellow">10.6.1</span></td><td>Variational posterior distribution</td></tr><tr><td><span class="highlight-text yellow">10.6.2</span></td><td>Optimizing the variational parameters</td></tr><tr><td><span class="highlight-text yellow">10.6.3</span></td><td>Inference of hyperparameters</td></tr><tr><td><span class="highlight-text yellow">10.7</span></td><td>Expectation Propagation</td></tr><tr><td><span class="highlight-text yellow">10.7.1</span></td><td>Example: The clutter problem</td></tr><tr><td><span class="highlight-text yellow">10.7.2</span></td><td>Expectation propagation of graphs</td></tr></tbody></table><h2 id=2-11-chương-11-sampling-methods>2.11. Chương 11: Sampling Methods</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text green">11</span></td><td>Sampling Methods</td></tr><tr><td><span class="highlight-text green">11.1</span></td><td>Basis Sampling Algorithms</td></tr><tr><td><span class="highlight-text green">11.1.1</span></td><td>Standard distributions</td></tr><tr><td><span class="highlight-text green">11.1.2</span></td><td>Rejection sampling</td></tr><tr><td><span class="highlight-text yellow">11.1.3</span></td><td>Adaptive rejection sampling</td></tr><tr><td><span class="highlight-text yellow">11.1.4</span></td><td>Importance sampling</td></tr><tr><td><span class="highlight-text yellow">11.1.5</span></td><td>Sampling-importance-resampling</td></tr><tr><td><span class="highlight-text yellow">11.1.6</span></td><td>Sampling and EM algorithm</td></tr><tr><td><span class="highlight-text green">11.2</span></td><td>Markov Chain Monte Carlo</td></tr><tr><td><span class="highlight-text green">11.2.1</span></td><td>Markov chains</td></tr><tr><td><span class="highlight-text green">11.2.2</span></td><td>The Metropolis-Hastings algorithm</td></tr><tr><td><span class="highlight-text green">11.3</span></td><td>Gibbs Sampling</td></tr><tr><td><span class="highlight-text yellow">11.4</span></td><td>Slice Sampling</td></tr><tr><td><span class="highlight-text yellow">11.5</span></td><td>The Hybrid Monte Carlo Algorithm</td></tr><tr><td><span class="highlight-text yellow">11.5.1</span></td><td>Dynamical systems</td></tr><tr><td><span class="highlight-text yellow">11.5.2</span></td><td>Hybrid Monte Carlo</td></tr><tr><td><span class="highlight-text yellow">11.6</span></td><td>Estimating the Partition Function</td></tr></tbody></table><h2 id=2-12-chương-12-continuous-latent-variables>2.12. Chương 12: Continuous Latent Variables</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text cyan">12</span></td><td>Continuous Latent Variables</td></tr><tr><td><span class="highlight-text cyan">12.1</span></td><td>Principal Component Analysis</td></tr><tr><td><span class="highlight-text cyan">12.1.1</span></td><td>Maximum variance formulation</td></tr><tr><td><span class="highlight-text cyan">12.1.2</span></td><td>Minimum-error formulation</td></tr><tr><td><span class="highlight-text cyan">12.1.3</span></td><td>Applications of peA</td></tr><tr><td><span class="highlight-text cyan">12.1.4</span></td><td>PCA for high-dimensional data</td></tr><tr><td><span class="highlight-text yellow">12.2</span></td><td>Probabilistic p e A</td></tr><tr><td><span class="highlight-text yellow">12.2.1</span></td><td>Maximum likelihood peA</td></tr><tr><td><span class="highlight-text yellow">12.2.2</span></td><td>EM algorithm for peA</td></tr><tr><td><span class="highlight-text yellow">12.2.3</span></td><td>Bayesian peA</td></tr><tr><td><span class="highlight-text yellow">12.2.4</span></td><td>Factor analysis</td></tr><tr><td><span class="highlight-text green">12.3</span></td><td>Kernel PCA</td></tr><tr><td><span class="highlight-text yellow">12.4</span></td><td>Nonliear Latent Variable Models</td></tr><tr><td><span class="highlight-text yellow">12.4.1</span></td><td>Independent component analysis</td></tr><tr><td><span class="highlight-text yellow">12.4.2</span></td><td>Autoassociative neural networks</td></tr><tr><td><span class="highlight-text yellow">12.4.3</span></td><td>Modelling nonlinear manifolds</td></tr></tbody></table><h2 id=2-13-chương-13-sequential-data>2.13. Chương 13: Sequential Data</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text green">13</span></td><td>Sequential Data</td></tr><tr><td><span class="highlight-text green">13.1</span></td><td>Markov Models</td></tr><tr><td><span class="highlight-text green">13.2</span></td><td>Hidden Markov Models</td></tr><tr><td><span class="highlight-text green">13.2.1</span></td><td>Maximum likelihood for the HMM</td></tr><tr><td><span class="highlight-text green">13.2.2</span></td><td>The forward-backward algorithm</td></tr><tr><td><span class="highlight-text yellow">13.2.3</span></td><td>The sum-product algorithm for the HMM</td></tr><tr><td><span class="highlight-text yellow">13.2.4</span></td><td>Scaling factors</td></tr><tr><td><span class="highlight-text green">13.2.5</span></td><td>The Viterbi algorithm</td></tr><tr><td><span class="highlight-text yellow">13.2.6</span></td><td>Extensions of the hidden Markov model</td></tr><tr><td><span class="highlight-text green">13.3</span></td><td>Linear Dynamical Systems</td></tr><tr><td><span class="highlight-text green">13.3.1</span></td><td>Inference in LDS</td></tr><tr><td><span class="highlight-text green">13.3.2</span></td><td>Learning in LDS</td></tr><tr><td><span class="highlight-text yellow">13.3.3</span></td><td>Extensions of LDS</td></tr><tr><td><span class="highlight-text yellow">13.3.4</span></td><td>Particle filters</td></tr></tbody></table><h2 id=2-14-chương-14-combining-models>2.14. Chương 14: Combining Models</h2><table><thead><tr><th>Mục</th><th>Tiêu đề</th></tr></thead><tbody><tr><td><span class="highlight-text green">14</span></td><td>Combining Models</td></tr><tr><td><span class="highlight-text green">14.1</span></td><td>Bayesian Model Averaging</td></tr><tr><td><span class="highlight-text green">14.2</span></td><td>Committees</td></tr><tr><td><span class="highlight-text green">14.3</span></td><td>Boosting</td></tr><tr><td><span class="highlight-text green">14.3.1</span></td><td>Minimizing exponential error</td></tr><tr><td><span class="highlight-text green">14.3.2</span></td><td>Error functions for boosting</td></tr><tr><td><span class="highlight-text cyan">14.4</span></td><td>Tree-based Models</td></tr><tr><td><span class="highlight-text yellow">14.5</span></td><td>Conditional Mixture Models</td></tr><tr><td><span class="highlight-text yellow">14.5.1</span></td><td>Mixtures of linear regression models</td></tr><tr><td><span class="highlight-text yellow">14.5.2</span></td><td>Mixtures of logistic models</td></tr><tr><td><span class="highlight-text yellow">14.5.3</span></td><td>Mixtures of experts</td></tr></tbody></table></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/ data-tooltip="[ML] Hồi quy tuyến tính (Linear Regression)"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-intro/ data-tooltip="[ML] Học máy là gì?"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2017 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/ data-tooltip="[ML] Hồi quy tuyến tính (Linear Regression)"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-intro/ data-tooltip="[ML] Học máy là gì?"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-prml/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-prml%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-prml%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-prml%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=algolia-search-modal class=modal-container><div class=modal><div class=modal-header><span class=close-button><i class="fa fa-close"></i></span><a href=https://algolia.com target=_blank rel=noopener class="searchby-algolia text-color-light link-unstyled"><span class="searchby-algolia-text text-color-light text-small">by</span>
<img class=searchby-algolia-logo src=https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg></a>
<i class="search-icon fa fa-search"></i><form id=algolia-search-form><input id=algolia-search-input name=search class="form-control input--large search-input" placeholder="Tìm kiếm"></form></div><div class=modal-body><div class="no-result text-color-light text-center">không tìm thấy kết quả</div><div class=results><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-gd/><h3 class=media-heading>[ML] Tối ưu hàm lỗi với Gradient Descent</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Mặc dù sử dụng công thức chuẩn để tìm tham số là có thể thực hiện được, nhưng với tập dữ liệu lớn nhiều chiều trong thực tế thì với máy tính lại không thể thực hiện được do các ràng buộc của bộ nhớ cũng như khả năng tính toán. Chưa kể với nhiều bài toán việc giải được đạo hàm để tìm ra công thức chuẩn là rất khó khăn. Nên trong thực tế giải thuật thay thế là <strong>Gradient Descent</strong> thường được sử dụng.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/><h3 class=media-heading>[ML] MLE của hồi quy tuyến tính</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (<em>mean squared error</em>). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng <a href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/#2-2-mle>MLE (<em>Maximum Likelihood Esitmation</em>)</a> xem sao.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/><h3 class=media-heading>[ML] Hồi quy tuyến tính (Linear Regression)</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Học có giám sát (<em>Supervised Learning</em>) được chia ra làm 2 dạng lớn là <strong>hồi quy</strong> (<em>regression</em>) và <strong>phân loại</strong> (<em>classification</em>) dựa trên tập dữ liệu mẫu - tập huấn luyện (<em>training data</em>). Với bài đầu tiên này ta sẽ bắt đầu bằng bài toán hồi quy mà cụ thể là hồi quy tuyến tính (<em>linear regression</em>).</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-prml/><h3 class=media-heading>Pattern Recognition and Machine Learning</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Được coi là sách giáo khoa cho những người làm học máy, cuốn sách này viết về các giải thuật và lý thuyết xây dựng các giải thuật nhận dạng mẫu và học máy. Tuy nhiên lúc mới đọc thì thấy khá khó nhằn nên tôi đã tìm hiểu độ khó các phần đề biết đường mà đọc.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-intro/><h3 class=media-heading>[ML] Học máy là gì?</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Thời gian gần đây AI nổi lên mạnh mẽ xâm nhập vào rất nhiều lĩnh vực trong cuộc sống như tự động dịch thuật, nhận dạng giọng nói, điều khiển tự động, v.v. Nó giờ được coi là xu hướng công nghệ thế giới và nhiều người cho rằng đó là cuộc cách mạng công nghiệp lần thứ 4.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/seiko-no-yotei/><h3 class=media-heading>Điểm cốt lõi để thành công</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather">Cuốn sách tổng hợp nội dung của một số buổi nói chuyện của ông Inamori Kazuo về kinh doanh, làm việc, nhân sinh. Vẫn phong cách quen thuộc về cách nhìn cuộc sống, cách suy nghĩ, cách hành động như trong các cuốn sách khác mà ông đã viết, nhưng trong cuốn này đặc biệt ở chỗ tổng hợp được nhiều nội dung khá cô động mà vẫn không thiếu sót nội dung.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/vbnet-oracle-version/><h3 class=media-heading>[.NET] Sài nhiều phiên bản Oracle khi thực thi</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Thông thường khi ta build ứng dụng thì phiên bản Oracle DB ở môi trường phát triển và môi trường thực thi là giống nhau nên không xảy ra vấn đề gì cả. Nhưng nếu ở môi trường phát triển và thực thi khác nhau thì sao?</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/what-is-http2/><h3 class=media-heading>[Web] HTTP2 là gì?</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Nhân tiện bản <code>Node v9x</code> mới ra cho phép ta có thể sử dụng ngay API thử nghiệm <code>HTTP/2</code> nên cũng tò mò tìm hiểu đôi chút xem kiến trúc, đặc điểm và cách sử dụng thế nào.
Sau 2 năm ra chính thức ra lò, phiên bản tiếp theo của <code>HTTP</code> này dần được nhiều máy chủ Web lẫn trình duyệt hỗ trợ bởi tính vượt trội của nó so với phiên bản <code>HTTP/1.1</code>.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/about-git/><h3 class=media-heading>[Git] Mô tả về GIT của Linus Torvalds</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Đây là mô tả về GIT mà chủ nhân của nó - ông Linus Torvalds đã viết khi công khai mã nguồn. Cụ thể bài này được copy lại từ <a href=https://github.com/git/git/tree/e83c5163316f89bfbde7d9ab23ca2e25604af290 target=_blank _ rel="noopener noreferrer">Github</a>.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/change-filename-bat/><h3 class=media-heading>[Windows] Đổi tên file với .bat file</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather">Gần đây Gmail không cho phép gửi các file có đuôi là mã nguồn ngôn ngữ lập trình như .js, .vb chẳng hạn. Ngay cả việc đổi đuôi của các file nén cũng không có hiệu quả như trước, nên buộc phải tìm cách đổi toàn bộ đuôi 1 phát.
Bài viết này sẽ nói về cách thay đổi toàn bộ đuôi file bằng .bat file của Windows, tuy nhiên hoàn toàn có thể sử dụng để làm những chuyện khác với các file này như đổi tên chẳng hạn.</div></div><div style=clear:both></div><hr></div></div></div><div class=modal-footer><p class="results-count text-medium" data-message-zero="không tìm thấy kết quả" data-message-one="tìm thấy 1 kết quả" data-message-other="tìm thấy {n} kết quả">55 posts found</p></div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js integrity="sha256-IFHWFEbU2/+wNycDECKgjIRSirRNIDp2acEB5fvdVRU=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js integrity="sha256-+mpyNVJsNt4rVXCw0F+pAOiB3YxmHgrbJsx4ecPuUaI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js integrity="sha256-vMxgR/7FtLovVA+IPrR7+xTgIgARH7y9VZQnmmi0HDI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.js integrity="sha256-N0qFUh7/9vLvia87dDndewmsgsyYoNkdA212tPc+2NI=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-GR8SEkOO1rBN/jnOcQDFcFmwXAevSLx7/Io9Ps1rkxWp983ZIuUGfxivlF/5f5eJ src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.js></script><script crossorigin=anonymous integrity=sha384-cXpztMJlr2xFXyDSIfRWYSMVCXZ9HeGXvzyKTYrn03rsMAlOtIQVzjty5ULbaP8L src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/12\/ml-prml\/';this.page.identifier='\/vi\/2017\/12\/ml-prml\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>