<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.41 with theme Tranquilpeak 0.4.1-BETA"><title>[ML] Hồi quy logistic (Logistic Regression)</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Học Máy,Machine Learning,logistic regression,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/><meta name=description content="Trong các phần trước ta đã tìm hiểu về phương pháp hồi quy tuyến tính để dự đoán đầu ra liên tục, phần này ta sẽ tìm hiểu thêm một thuật toán nữa trong học có giám sát là hồi quy logistic (Logistic Regression) nhằm mục đính phân loại dữ liệu."><meta property=og:type content=website><meta property=og:title content="[ML] Hồi quy logistic (Logistic Regression)"><meta property=og:url content=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/><meta property=og:description content="Trong các phần trước ta đã tìm hiểu về phương pháp hồi quy tuyến tính để dự đoán đầu ra liên tục, phần này ta sẽ tìm hiểu thêm một thuật toán nữa trong học có giám sát là hồi quy logistic (Logistic Regression) nhằm mục đính phân loại dữ liệu."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="[ML] Hồi quy logistic (Logistic Regression)"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/><meta name=twitter:description content="Trong các phần trước ta đã tìm hiểu về phương pháp hồi quy tuyến tính để dự đoán đầu ra liên tục, phần này ta sẽ tìm hiểu thêm một thuật toán nữa trong học có giám sát là hồi quy logistic (Logistic Regression) nhằm mục đính phân loại dữ liệu."><meta name=twitter:creator content=@minhhai3b><meta property=fb:app_id content=333198270561466><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/talk/><i class="sidebar-button-icon fa fa-lg fa-child"></i><span class=sidebar-button-desc>Chém gió</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[ML] Hồi quy logistic (Logistic Regression)</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-12-28T11:19:53&#43;09:00>28 tháng 12, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Trong các phần trước ta đã tìm hiểu về phương pháp hồi quy tuyến tính để dự đoán đầu ra liên tục, phần này ta sẽ tìm hiểu thêm một thuật toán nữa trong học có giám sát là <strong>hồi quy logistic</strong> (<em>Logistic Regression</em>) nhằm mục đính phân loại dữ liệu.<h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-định-nghĩa>1. Định nghĩa</a></li><li><a href=#2-mô-hình>2. Mô hình</a></li><li><a href=#3-ước-lượng-tham-số>3. Ước lượng tham số</a><ul><li><a href=#3-1-phương-pháp-gd>3.1. Phương pháp GD</a></li><li><a href=#3-2-phương-pháp-newton-raphson>3.2. Phương pháp Newton-Raphson</a></li></ul></li><li><a href=#4-lập-trình>4. Lập trình</a></li><li><a href=#5-phân-loại-nhiều-nhóm>5. Phân loại nhiều nhóm</a><ul><li><a href=#5-1-dựa-theo-phương-pháp-2-nhóm>5.1. Dựa theo phương pháp 2 nhóm</a></li><li><a href=#5-2-dựa-theo-mô-hình-xác-suất-nhiều-nhóm>5.2. Dựa theo mô hình xác suất nhiều nhóm</a></li></ul></li><li><a href=#6-over-fitting>6. Over-fitting</a></li><li><a href=#7-kết-luận>7. Kết luận</a></li></ul></nav></p><h1 id=1-định-nghĩa>1. Định nghĩa</h1><p>Phương pháp hồi quy logistic là một mô hình hồi quy nhằm dự đoán giá trị đầu ra <em>rời rạc</em> (<em>discrete target variable</em>) $y$ ứng với một véc-tơ đầu vào $\mathbf{x}$. Việc này tương đương với chuyện phân loại các đầu vào $\mathbf{x}$ vào các nhóm $y$ tương ứng.</p><p>Ví dụ, xem một bức ảnh có chứa một con mèo hay không. Thì ở đây ta coi đầu ra $y=1$ nếu bước ảnh có một con mèo và $y=0$ nếu bức ảnh không có con mèo nào. Đầu vào $\mathbf{x}$ ở đây sẽ là các pixel một bức ảnh đầu vào.</p><div class="figure center"><a class=fancybox href=https://res.cloudinary.com/dominhhai/image/upload/ml/logistic-regression_ex2_ret_1.png title="Classification with 2 groups" data-fancybox-group><img class=fig-img src=https://res.cloudinary.com/dominhhai/image/upload/ml/logistic-regression_ex2_ret_1.png alt="Classification with 2 groups"></a>
<span class=caption>Classification with 2 groups</span></div><p>Để đơn giản, trước tiên ta sẽ cùng đi tìm hiểu mô hình và cách giải quyết cho bài toán phân loại nhị phân tức là $y=\{0,1\}$. Sau đó ta mở rộng cho trường hợp nhiều nhóm sau.</p><h1 id=2-mô-hình>2. Mô hình</h1><p>Sử dụng phương pháp thống kê ta có thể coi rằng khả năng một đầu vào $\mathbf{x}$ nằm vào một nhóm $y_0$ là xác suất nhóm $y_0$ khi biết $\mathbf{x}$: $p(y_0|\mathbf{x})$. Dựa vào công thức xác xuất hậu nghiệm ta có:</p><p>$$
\begin{aligned}
p(y_0|\mathbf{x}) &amp;= \dfrac{p(\mathbf{x}|y_0)p(y_0)}{p(\mathbf{x})}
\cr\ &amp;= \dfrac{p(\mathbf{x}|y_0)p(y_0)}{p(\mathbf{x}|y_0)p(y_0) + p(\mathbf{x}|y_1)p(y_1)}
\end{aligned}
$$</p><p>Đặt:
$$a=\ln\dfrac{p(\mathbf{x}|y_0)p(y_0)}{p(\mathbf{x}|y_1)p(y_1)}$$</p><p>Ta có:
$$p(y_0|\mathbf{x})=\dfrac{1}{1+\exp(-a)}=\sigma(a)$$</p><p>Hàm $\sigma(a)$ ở đây được gọi là <strong>hàm sigmoid</strong> (<em>logistic sigmoid function</em>). Hình dạng chữ S bị chặn 2 đầu của nó rất đặt biệt ở chỗ dạng phân phối đều ra và rất mượt.
<canvas id=sigmoid></canvas></p><p>Ở đây tôi không chứng minh, nhưng vận dụng thuyết phân phối chuẩn, ta có thể chỉ ra rằng:
$$a = \mathbf{w}^{\intercal}\mathbf{x} + w_0$$
Đặt: $\mathbf{x}_0=[1,&hellip;,1]$, ta có thể viết gọn lại thành:
$$a = \mathbf{w}^{\intercal}\mathbf{x}$$</p><p>Công thức tính xác suất lúc này:
$$p(y_0|\mathbf{x})=\dfrac{1}{1+\exp(-a)}=\sigma(\mathbf{w}^{\intercal}\mathbf{x})$$</p><p>Trong đó, $\mathbf{x}$ là thuộc tính đầu vào còn $\mathbf{w}$ là trọng số tương ứng.</p><blockquote><p>Lưu ý rằng cũng như phần <a href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/>hồi quy tuyến tính</a> thì $\mathbf{x}$ ở đây không nhất thiết là đầu vào thô của tập dữ liệu mà ta có thể sử dụng các hàm cơ bản $\phi(\mathbf{x})$ để tạo ra nó. Tuy nhiên, ở đây để cho gọn gàng tôi không viết $\phi(\mathbf{x})$ như lần trước nữa.</p></blockquote><p>Có công thức tính được xác suất rồi thì ta có thể sử dụng một ngưỡng $\epsilon\in [0,1]$ để quyết định nhóm tương ứng. Cụ thể:
$$
\begin{cases}
\mathbf{x}\in y_0 &amp;\text{if } p(y_0|\mathbf{x})\ge\epsilon
\cr
\mathbf{x}\in y_1 &amp;\text{if } p(y_0|\mathbf{x})&lt;\epsilon
\end{cases}
$$</p><p>Ví dụ, $\epsilon=0.7$ thì $\mathbf{x}\in y_0$ khi mà xác suất thuộc nhóm $y_0$ của nó là trên 70%, còn dưới 70% thì ta phân nó vào nhóm $y_1$.</p><p>Dựa vào phân tích ở <a href=https://dominhhai.github.io/vi/2017/10/prob-4-ml/#3-gi%E1%BA%A3i-thu%E1%BA%ADt-logistic-regression>ví dụ mẫu phần xác suất</a>, ta cần tối thiểu hoá làm lỗi sau:
$$J(\mathbf{w})=-\frac{1}{m}\sum_{i=1}^m\Big(y^{(i)}log\sigma^{(i)} + (1-y^{(i)})log(1-\sigma^{(i)})\Big)$$</p><p>Trong đó, $m$ là kích cỡ của tập dữ liệu, $y^{(i)}$ lớp tương ứng của dữ liệu thứ $i$ trong tập dữ liệu, $\sigma^{(i)}=\sigma(\mathbf{w}^{\intercal}\mathbf{x}^{(i)})$ là xác suất tương ứng khi tính với mô hình cho dữ liệu thứ $i$.</p><h1 id=3-ước-lượng-tham-số>3. Ước lượng tham số</h1><h2 id=3-1-phương-pháp-gd>3.1. Phương pháp GD</h2><p>Để tối ưu hàm $J(\mathbf{w})$ trên, ta lại sử dụng các phương pháp <a href=https://dominhhai.github.io/vi/2017/12/ml-gd/>Gradient Descent</a> để thực hiện. Ở đây, đạo hàm của hàm log trên <a href=https://dominhhai.github.io/vi/2017/10/prob-4-ml/#3-2-l%C3%BD-thuy%E1%BA%BFt>có thể được tính</a> như sau:
$$
\begin{aligned}
\frac{\partial J(\mathbf{w})}{\partial w_j}&amp;=\frac{1}{m}\sum_{i=1}^m(\sigma_j^{(i)}-y_j^{(i)})\mathbf{x}_j^{(i)}
\cr\ &amp;=\frac{1}{m}\sum_{i=1}^m\big(\sigma(\mathbf{w}^{\intercal}\mathbf{x}_j^{(i)})-y_j^{(i)}\big)\mathbf{x}_j^{(i)}
\cr\ &amp;=\frac{1}{m}\mathbf{X}_j^{\intercal}\big(\mathbf{\sigma}_j-\mathbf{y}_j\big)
\end{aligned}
$$</p><p>Ví dụ, theo phương pháp <a href=https://dominhhai.github.io/vi/2017/12/ml-gd/#1-gradient-descent-l%C3%A0-g%C3%AC>BGD</a>, ta sẽ cập nhập tham số sau mỗi vòng lặp như sau:
$$\mathbf{w}=\mathbf{w}-\eta\frac{1}{m}\mathbf{X}^{\intercal}\big(\mathbf{\sigma}-\mathbf{y}\big)$$</p><h2 id=3-2-phương-pháp-newton-raphson>3.2. Phương pháp Newton-Raphson</h2><p>Phương pháp ở phía trên ta chỉ sử dụng đạo hàm bậc nhất cho phép GD quen thuộc, tuy nhiên ở bài toán này việc sử dụng đạo hàm bậc 2 đem tại tốc độ tốt hơn.</p><p>$$\mathbf{w}=\mathbf{w}-\mathbf{H}^{-1}\nabla J(\mathbf{w})$$</p><p>Trong đó, $\nabla J(\mathbf{w})$ là <a href=https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant target=_blank _ rel="noopener noreferrer">ma trận Jacobi</a> của $J(\mathbf{w})$, còn $\mathbf{H}$ là <a href=https://en.wikipedia.org/wiki/Hessian_matrix target=_blank _ rel="noopener noreferrer">ma trận Hessian</a> của $J(\mathbf{w})$. Hay nói cách khác, $\mathbf{H}$ là ma trận Jacobi của $\nabla J(\mathbf{w})$.</p><p>Phương pháp này có tên chính thức là <em>Newton-Raphson</em>. Phương pháp này không chỉ sử dụng riêng cho bài toán hồi quy logistic mà còn có thể áp dụng cho cả các bài toán hồi quy tuyến tính. Tuy nhiên, việc thực hiện với hồi quy tuyến tính không thực sự phổ biến.</p><p>Ta có:
$$
\begin{aligned}
\nabla J(\mathbf{w})&amp;=\frac{1}{m}\sum_{i=1}^m(\sigma^{(i)}-y^{(i)})\mathbf{x}^{(i)}
\cr\ &amp;=\frac{1}{m}\mathbf{X}^{T}\big(\mathbf{\sigma}-\mathbf{y}\big)
\end{aligned}
$$</p><p>Đạo hàm của hàm sigmoid:
$$\frac{d\sigma}{da}=\sigma(1-\sigma)$$</p><p>Nên:
$$
\begin{aligned}
\mathbf{H}&amp;=\nabla\nabla J(\mathbf{w})
\cr\ &amp;=\frac{1}{m}\sum_{i=1}^m\mathbf{x}^{(i)}{\mathbf{x}^{(i)}}^{\intercal}
\cr\ &amp;=\frac{1}{m}\sum_{i=1}^m\mathbf{X}^{T}\mathbf{X}
\end{aligned}
$$</p><p>Thế vào công thức cập nhập tham số ta có tham số sau mỗi lần cập nhập là:
$$\mathbf{w}=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}$$</p><p>Như vậy, so với cách lấy đạo hàm bậc 1 thì cách này tỏ ra đơn giản và nhanh hơn.</p><h1 id=4-lập-trình>4. Lập trình</h1><p>Dựa vào các phân tích phía trên ta thử lập trình với BGD xem sao. Trong bài viết này tôi chỉ để cập tới đoạn mã chính để thực hiện việc tối ưu, còn toàn bộ mã nguồn bạn có thể xem trên <a href=https://github.com/dominhhai/mldl/blob/master/coursera-ml/ex2.ipynb target=_blank _ rel="noopener noreferrer">Github</a>.</p><p>Tập dữ liệu được sử dụng ở đây là <a href=https://github.com/dominhhai/mldl/blob/master/coursera-ml/ex2data1.csv target=_blank _ rel="noopener noreferrer">dữ liệu bài tập</a> trong khoá học ML của giáo sư Andrew Ng.</p><div class="figure center"><a class=fancybox href=https://res.cloudinary.com/dominhhai/image/upload/ml/logistic-regression_ex2_data_1.png title=Dataset data-fancybox-group><img class=fig-img src=https://res.cloudinary.com/dominhhai/image/upload/ml/logistic-regression_ex2_data_1.png alt=Dataset></a>
<span class=caption>Dataset</span></div><p>Giờ ta sử dụng phương pháp BGD để tối ưu hàm $J(\mathbf{w})$:<figure class="highlight python language-python"><figcaption><span>bgd.py</span><a href=https://github.com/dominhhai/mldl/blob/master/coursera-ml/ex2.ipynb target=_blank rel=external>bgd.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br><span class=line>24</span><br><span class=line>25</span><br><span class=line>26</span><br><span class=line>27</span><br><span class=line>28</span><br><span class=line>29</span><br><span class=line>30</span><br><span class=line>31</span><br><span class=line>32</span><br><span class=line>33</span><br><span class=line>34</span><br><span class=line>35</span><br><span class=line>36</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python># gradient descent max step
INTERATIONS = 200000
# learning rate
ALPHA = 0.001

# calc sigmoid function
def sigmoid(z):
    return 1.0 / (1.0 &#43; np.exp(-z))

# calc J function
def compute_cost(X, y, theta):
    # number of training examples
    m = y.size
    # activation
    h = sigmoid(np.dot(X, theta))
    # cost
    j = - np.sum(y * np.log(h) &#43; (1 -  y) * np.log(1 - h)) / m
    return j

# implement BGD
def gradient_descent(X, y, theta, alpha, num_inters):
    # number of training examples
    m = y.size
    jHistory = np.empty(num_inters)

    for i in range(num_inters):
        delta = np.dot(X.T, sigmoid(np.dot(X, theta))- y) / m
        theta -= alpha * delta
        jHistory[i] = compute_cost(X, y, theta)

    return theta, jHistory

# train
theta, jHistory = gradient_descent(X, y, np.zeros(X.shape[1]), ALPHA, INTERATIONS)
print(theta)
# theta: [-7.45017822  0.06550395  0.05898701]</code></pre></td></tr></tbody></table></figure></p><p>Kết quả thu được:
$$
\begin{cases}
w_0=-7.45017822 \cr
w_1=0.06550395 \cr
w_2=0.05898701
\end{cases}
$$</p><p>Thử vẽ đường phân tách với $\epsilon=0.5$ ta sẽ được:<div class="figure center"><a class=fancybox href=https://res.cloudinary.com/dominhhai/image/upload/ml/logistic-regression_ex2_ret_1.png title="Decision Boundary with ϵ=0.5" data-fancybox-group><img class=fig-img src=https://res.cloudinary.com/dominhhai/image/upload/ml/logistic-regression_ex2_ret_1.png alt="Decision Boundary with ϵ=0.5"></a>
<span class=caption>Decision Boundary with ϵ=0.5</span></div></p><h1 id=5-phân-loại-nhiều-nhóm>5. Phân loại nhiều nhóm</h1><p>Ở phần trên ta vừa phân tích phương pháp phân loại 2 nhóm $y=\{0,1\}$, dựa vào đó ta có thể tổng quát hoá cho bài toán phân loại K nhóm $y=\{1,..,K\}$. Về cơ bản 2 có 2 phương pháp chính là:</p><ul><li>Dựa theo phương pháp 2 nhóm</li><li>Dựa theo mô hình xác suất nhiều nhóm</li></ul><p>Cụ thể ra sao, ta cùng xem chi tiết ngày phần dưới đây.</p><h2 id=5-1-dựa-theo-phương-pháp-2-nhóm>5.1. Dựa theo phương pháp 2 nhóm</h2><p>Ta có thể sử dụng phương pháp phân loại 2 nhóm để phân loại nhiều nhóm bằng cách tính xác xuất của tầng nhóm tương ứng rồi chọn nhóm có xác suất lớn nhất là đích:
$$p(y_k|\mathbf{x})=\max p(y_j|\mathbf{x})~~~,\forall j=\overline{1,K}$$</p><p>Đoạn quyết định nhóm dựa theo ngưỡng $\epsilon$ vẫn hoàn toàn tương tự như vậy. Nếu $p(y_k|\mathbf{x})\ge\epsilon$ thì $\mathbf{x}\in y_k$, còn không thì nó sẽ không thuộc nhóm $y_k$.</p><p>Phương pháp này khá đơn giản và dễ hiểu song việc thực thi có thể rất tốn kém thời gian do ta phải tính xác suất của nhiều nhóm. Bởi vậy ta cùng xem 1 giải pháp khác hiệu quả hơn như dưới đây.</p><h2 id=5-2-dựa-theo-mô-hình-xác-suất-nhiều-nhóm>5.2. Dựa theo mô hình xác suất nhiều nhóm</h2><p>Tương tự như phân loại 2 nhóm, ta có thể mở rộng ra thành nhiều nhóm với cùng phương pháp sử dụng công thức xác suất hậu nghiệm để được hàm <strong>softmax</strong> sau:
$$
\begin{aligned}
p(y_k|\mathbf{x})=p_k&amp;=\frac{p(\mathbf{x}|y_k)p(y_k)}{\sum_jp(\mathbf{x}|y_j)p(y_j)}
\cr\ &amp;=\frac{\exp(a_k)}{\sum_j\exp(a_j)}
\end{aligned}
$$</p><p>Với $a_j=\log\Big(p(\mathbf{x}|y_j)p(y_j)\Big)=\mathbf{w}_j^{\intercal}\mathbf{x}$. Trong đó, $\mathbf{w}_j$ là trọng số tương ứng với nhóm $j$, còn $\mathbf{x}$ là đầu vào dữ liệu. Tập các $\mathbf{w}_j$ sẽ được gom lại bằng một ma trận trọng số $\mathbf{W}$ với mỗi cột tương ứng với trọng số của nhóm tương ứng.</p><p>Ở đây, ta sẽ mã hoá các nhóm của ta thành một véc-to <strong>one-hot</strong> với phần tử ở chỉ số nhóm tương ứng bằng 1 và các phần tử khác bằng 0. Ví dụ: $y_1=[1,0,&hellip;,0], y_3=[0,0,1,0,&hellip;,0]$. Tập hợp các véc-tơ này lại ta sẽ có được một ma trận chéo $\mathbf{Y}$ với mỗi cột tương ứng với 1 nhóm. Ví dụ, ma trận sau biểu diễn cho tập 3 nhóm:
$$
\mathbf{Y}=\begin{bmatrix}
1 &amp; 0 &amp; 0 \cr
0 &amp; 1 &amp; 0 \cr
0 &amp; 0 &amp; 1
\end{bmatrix}
$$</p><p>Như vậy, ta có thể tính xác suất hợp toàn tập với giả sử các tập dữ liệu là độc lập đôi một:
$$
\begin{aligned}
p(\mathbf{Y}|\mathbf{W})&amp;=\prod_{i=1}^m\prod_{k=1}^Kp(y_k|\mathbf{x}_i)^{Y_{ik}}
\cr\ &amp;=\prod_{i=1}^m\prod_{k=1}^Kp_{ik}^{Y_{ik}}
\end{aligned}
$$</p><p>Trong đó, $p_{ik}=p_k(\mathbf{x}_i)$. Lấy log ta được hàm lỗi:
$$J(\mathbf{W})=-\sum_{i=1}^m\sum_{k=1}^KY_{ik}\log p_{ik}$$</p><p>Như vậy, ta có thể thấy đây là công thức tổng quát của hàm lỗi trong trường hợp 2 nhóm. Công thức này còn có tên gọi là <strong>cross-entropy</strong> error function.</p><p>Việc tối ưu hàm lỗi này cũng tương tự như trường hợp 2 nhóm bằng cách lấy đạo hàm:
$$\nabla_{w_j}J(\mathbf{W})=\sum_{i=1}^m\big(p_{ij}-Y_{ij}\big)\mathbf{x}_i$$</p><blockquote><p><a href=https://en.wikipedia.org/wiki/Cross_entropy target=_blank _ rel="noopener noreferrer">cross-entropy</a> là cách đo độ tương tự giữ 2 phân phối xác suất với nhau. Nếu 2 phần phối càng giống nhau thì cross-entropy của chúng càng nhỏ. Như vậy để tìm mô hình gần với mô hình thực của tập dữ liệu, ta chỉ cần tối thiểu hoá cross-entropy của nó.</p></blockquote><h1 id=6-over-fitting>6. Over-fitting</h1><p>Tương tự như phần hồi quy tuyến tính, ta có thể xử lý overfitting bằng phương pháp thêm hệ số <a href=https://dominhhai.github.io/vi/2017/12/ml-overfitting/#4-k%C4%A9-thu%E1%BA%ADt-ch%C3%ADnh-quy-ho%C3%A1>chính quy hoá</a> cho hàm lỗi:
$$J(\mathbf{w})=-\frac{1}{m}\sum_{i=1}^m\Big(y^{(i)}log\sigma^{(i)} + (1-y^{(i)})log(1-\sigma^{(i)})\Big)+\lambda\frac{1}{m}\mathbf{w}^{\intercal}\mathbf{w}$$</p><p>Đạo hàm lúc này sẽ là:
$$\frac{\partial J(\mathbf{w})}{\partial w_j}=\frac{1}{m}\mathbf{X}_j^{\intercal}\big(\mathbf{\sigma}_j-\mathbf{y}_j\big)+\lambda\frac{1}{m}w_j$$</p><h1 id=7-kết-luận>7. Kết luận</h1><p>Bài viết lần này đã tổng kết lại phương pháp phân loại logistic regression dựa vào cách tính xác suất của mỗi nhóm. Phương này khá đơn giản nhưng cho kết quả rất khả quan và được áp dụng rất nhiều trong cuộc sống.</p><p>Với phân loại nhị phân (2 nhóm), ta có cách tính xác suất:
$$p(y_0|\mathbf{x})=\dfrac{1}{1+\exp(-a)}=\sigma(\mathbf{w}^{\intercal}\mathbf{x})$$</p><p>Hàm lỗi tương ứng:
$$J(\mathbf{w})=-\frac{1}{m}\sum_{i=1}^m\Big(y^{(i)}log\sigma^{(i)} + (1-y^{(i)})log(1-\sigma^{(i)})\Big)+\lambda\frac{1}{m}\mathbf{w}^{\intercal}\mathbf{w}$$</p><p>Có đạo hàm:
$$\frac{\partial J(\mathbf{w})}{\partial w_j}=\frac{1}{m}\mathbf{X}_j^{\intercal}\big(\mathbf{\sigma}_j-\mathbf{y}_j\big)+\lambda\frac{1}{m}w_j
$$</p><p>Trong thực tế, ta thường xuyên phải phân loại nhiều nhóm. Việc này có thể áp dụng bằng cách lấy nhóm có xác suất lớn nhất hoặc sử dụng <strong>softmax</strong> để tính xác suất:
$$p(y_k|\mathbf{x})=p_k=\frac{\exp(a_k)}{\sum_j\exp(a_j)}$$
Với $a_j=\mathbf{w}_j^{\intercal}\mathbf{x}$, trong đó véc-tơ $\mathbf{w}_j$ là trọng số tương ứng với mỗi nhóm.</p><script>function fnMain(){var data=[];for(var i=-5;i<=5;i+=0.01){data.push({x:i,y:1/(1+Math.exp(-i))});}
new Chart('sigmoid',{type:'line',data:{datasets:[{label:'sigmoid function',backgroundColor:'rgba(0, 128, 0, 1)',borderColor:'rgba(0, 128, 0, 1)',fill:false,pointRadius:0,data:data}]},options:{title:{display:true,position:'bottom',text:'Hình 1. Đồ thị hàm sigmoid σ(a)'},scales:{xAxes:[{type:'linear',position:'bottom',scaleLabel:{display:true,labelString:'a'}}],yAxes:[{scaleLabel:{display:true,labelString:'σ'}}]}}});}</script></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/01/atom-auto-encoding/ data-tooltip="[Atom] Tự động phát hiện và hiển thị mã hoá"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/20171226-ml-bias-variance-tradeoff/ data-tooltip="[ML] Cân bằng phương sai và độ lệch"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=fb-root></div><script>(function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(d.getElementById(id))return;js=d.createElement(s);js.id=id;js.src='https://connect.facebook.net/vi_VN/sdk.js#xfbml=1&version=v3.1&appId=333198270561466&autoLogAppEvents=1';fjs.parentNode.insertBefore(js,fjs);}(document,'script','facebook-jssdk'));</script><div id=fb-cmt-thread class=fb-comments data-href=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/ data-width=100%></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2018 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2018/01/atom-auto-encoding/ data-tooltip="[Atom] Tự động phát hiện và hiển thị mã hoá"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/20171226-ml-bias-variance-tradeoff/ data-tooltip="[ML] Cân bằng phương sai và độ lệch"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-logistic-regression/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-logistic-regression%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-logistic-regression%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-logistic-regression%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/12\/ml-logistic-regression\/';this.page.identifier='\/vi\/2017\/12\/ml-logistic-regression\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script src=https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.7.1/Chart.min.js></script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>