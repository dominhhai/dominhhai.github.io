<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.41 with theme Tranquilpeak 0.4.1-BETA"><title>[ML] Mô hình quá khớp (Overfitting)</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Học Máy,Machine Learning,Overfitting,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/12/ml-overfitting/><meta name=description content="Lỗi ước lượng tham số có thể được chia thành 2 loại là khớp quá (over-fitting) và chưa khớp (under-fitting) với tập huấn luyện. Trong bài này sẽ nói về cách theo dõi và hạn chế các lỗi này ra sao. Trọng tâm của bài này sẽ tập trung chủ yếu vào kĩ thuật chính quy hoá (regularization) để giải quyết vấn đề khớp quá của tham số."><link rel=publisher href=https://plus.google.com/115106277658014197977><meta property=fb:app_id content=333198270561466><meta property=og:locale content=vi_VN><meta property=og:type content=article><meta property=article:author content="Do Minh Hai"><meta property=og:title content="[ML] Mô hình quá khớp (Overfitting)"><meta property=og:url content=https://dominhhai.github.io/vi/2017/12/ml-overfitting/><meta property=og:description content="Lỗi ước lượng tham số có thể được chia thành 2 loại là khớp quá (over-fitting) và chưa khớp (under-fitting) với tập huấn luyện. Trong bài này sẽ nói về cách theo dõi và hạn chế các lỗi này ra sao. Trọng tâm của bài này sẽ tập trung chủ yếu vào kĩ thuật chính quy hoá (regularization) để giải quyết vấn đề khớp quá của tham số."><meta property=og:site_name content="Hai's Blog"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:creator content=@minhhai3b><meta name=twitter:card content=summary><meta name=twitter:title content="[ML] Mô hình quá khớp (Overfitting)"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/12/ml-overfitting/><meta name=twitter:description content="Lỗi ước lượng tham số có thể được chia thành 2 loại là khớp quá (over-fitting) và chưa khớp (under-fitting) với tập huấn luyện. Trong bài này sẽ nói về cách theo dõi và hạn chế các lỗi này ra sao. Trọng tâm của bài này sẽ tập trung chủ yếu vào kĩ thuật chính quy hoá (regularization) để giải quyết vấn đề khớp quá của tham số."><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/talk/><i class="sidebar-button-icon fa fa-lg fa-child"></i><span class=sidebar-button-desc>Chém gió</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[ML] Mô hình quá khớp (Overfitting)</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-12-25T08:45:04&#43;09:00>25 tháng 12, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Lỗi ước lượng tham số có thể được chia thành 2 loại là <strong>khớp quá</strong> (<em>over-fitting</em>) và <strong>chưa khớp</strong> (<em>under-fitting</em>) với tập huấn luyện. Trong bài này sẽ nói về cách theo dõi và hạn chế các lỗi này ra sao. Trọng tâm của bài này sẽ tập trung chủ yếu vào kĩ thuật <strong>chính quy hoá</strong> (<em>regularization</em>) để giải quyết vấn đề khớp quá của tham số.<h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-giới-thiệu>1. Giới thiệu</a><ul><li><a href=#1-1-chưa-khớp-underfitting>1.1. Chưa khớp (<em>Underfitting</em>)</a></li><li><a href=#1-2-quá-khớp-overfitting>1.2. Quá khớp (<em>Overfitting</em>)</a></li><li><a href=#1-3-vừa-khớp-good-fitting>1.3. Vừa khớp (<em>Good Fitting</em>)</a></li></ul></li><li><a href=#2-theo-dõi-lỗi>2. Theo dõi lỗi</a><ul><li><a href=#2-1-đánh-giá-lỗi>2.1. Đánh giá lỗi</a></li><li><a href=#2-2-phán-định-lỗi>2.2. Phán định lỗi</a></li></ul></li><li><a href=#3-xử-lý-lỗi>3. Xử lý lỗi</a><ul><li><a href=#3-1-điểm-hợp-lý>3.1. Điểm hợp lý</a></li><li><a href=#3-2-chưa-khớp>3.2. Chưa khớp</a></li><li><a href=#3-3-quá-khớp>3.3. Quá khớp</a></li></ul></li><li><a href=#4-kĩ-thuật-chính-quy-hoá>4. Kĩ thuật chính quy hoá</a><ul><li><a href=#4-1-định-nghĩa>4.1. Định nghĩa</a></li><li><a href=#4-2-công-thức-chuẩn>4.2. Công thức chuẩn</a></li><li><a href=#4-3-tính-đạo-hàm>4.3. Tính đạo hàm</a></li><li><a href=#4-4-cài-đặt>4.4. Cài đặt</a></li></ul></li><li><a href=#5-kết-luận>5. Kết luận</a></li></ul></nav></p><h1 id=1-giới-thiệu>1. Giới thiệu</h1><p>Mô hình của ta sau khi huấn luyện có thể đạt hiệu quả không tốt khi dự đoán với một dữ liệu mới. Chuyện này xảy ra là do mô hình của ta chưa tổng quát hoá được với toàn bộ tập dữ liệu. Nguyên nhân cũng khá dễ hiểu khi mà tập huấn luyện của ta chỉ là một tập nhỏ chưa thể đại diện cho toàn thể dữ liệu được và hơn nữa có thể nó còn bị nhiễu nữa. Người ta chia nguyên nhân ra làm 2 loại chính là <em>chưa khớp</em> hoặc <em>quá khớp</em>.</p><div class="figure center"><a class=fancybox href=https://res.cloudinary.com/dominhhai/image/upload/ml/sin2pi.png title="Hình 1: y=sin(2πx) model. Underfit: degree 1 (left); Goodfit: degree 3 (center); Overfit: degree 15 (right)" data-fancybox-group><img class=fig-img src=https://res.cloudinary.com/dominhhai/image/upload/ml/sin2pi.png alt="Hình 1: y=sin(2πx) model. Underfit: degree 1 (left); Goodfit: degree 3 (center); Overfit: degree 15 (right)"></a>
<span class=caption>Hình 1: y=sin(2πx) model. Underfit: degree 1 (left); Goodfit: degree 3 (center); Overfit: degree 15 (right)</span></div><h2 id=1-1-chưa-khớp-underfitting>1.1. Chưa khớp (<em>Underfitting</em>)</h2><p>Mô hình được coi là chưa khớp nếu nó chưa được chưa phù hợp với tập dữ liệu huấn luyện và cả các mẫu mới khi dự đoán. Nguyên nhân có thể là do mô hình chưa đủ độ phức tạp cần thiết để bao quát được tập dữ liệu. Ví dụ như hình 1 phía bên trái ở trên. Tập dữ liệu huấn luyện loanh quanh khúc $y=sin(2\pi x)$ thế nhưng mô hình của ta chỉ là một đường thẳng mà thôi. Rõ ràng như vậy thì nó không những không thể ước lượng được giá trị của $y$ với $x$ mới mà còn không hiệu quả với cả tập dữ liệu $(x,y)$ có sẵn.</p><h2 id=1-2-quá-khớp-overfitting>1.2. Quá khớp (<em>Overfitting</em>)</h2><p>Mô hình rất hợp lý, rất khớp với tập huấn luyện nhưng khi đem ra dự đoán với dữ liệu mới thì lại không phù hợp. Nguyên nhân có thể do ta chưa đủ dữ liệu để đánh giá hoặc do mô hình của ta quá phức tạp. Mô hình bị quá phức tạp khi mà mô hình của ta sử dụng cả những nhiễu lớn trong tập dữ liệu để học, dấn tới mất tính tổng quát của mô hình. Ví dụ như ở hình 1 phía bên phải ở trên. Mô hình của ta gần như mong muốn bao được hết tất cả các điểm làm cho biên độ dao động của nó lớn quá mức. Mô hình này mà dự đoán với 1 giá trị mới của $x$ thì khả năng $y$ sẽ bị lệch đi rất nhiều.</p><h2 id=1-3-vừa-khớp-good-fitting>1.3. Vừa khớp (<em>Good Fitting</em>)</h2><p>Mô hình này nằm giữa 2 mô hình chưa khớp và quá khớp cho ra kết quả hợp lý với cả tập dữ liệu huấn luyện và các giá trị mới, tức là nó mang được tính tổng quát như hình 1 ở giữa phía trên. Lý tưởng nhất là khớp được với nhiều dữ liệu mẫu và cả các dữ liệu mới. Tuy nhiên trên thực tế được mô hình như vậy rất hiếm.</p><h1 id=2-theo-dõi-lỗi>2. Theo dõi lỗi</h1><p>Với định nghĩa như trên ta cần phương pháp để đánh giá được mô hình trước khi có thể đưa ra được biện pháp cải tiến. Trước tiên ta quy định một số thông số lỗi để phục vụ cho việc đánh giá mô hình.</p><h2 id=2-1-đánh-giá-lỗi>2.1. Đánh giá lỗi</h2><p>Ở đây ta sẽ lấy trung bình lỗi của toàn bộ tập dữ liệu để đánh giá:
$$E(\theta)=\frac{1}{m}\sum_{i=1}^m err(\hat y^{(i)},y^{(i)})$$</p><p>Trong đó $E(\theta)$ là lỗi ứng với tham số $\theta$ ước lượng được của tập dữ liệu gồm có $m$ mẫu. $err(\hat y,y)$ thể hiện cho sự khác biệt giữa giá trị dự đoán $\hat y$ và giá trị thực tế $y$. Đương nhiên là nếu $\hat y=y$ thì $err(\hat y^{(i)},y^{(i)})=0$. Thường người ta lấy $err(\hat y^{(i)},y^{(i)})=\Vert \hat y^{(i)}-y^{(i)}\Vert_2^2$ giống như các hàm lỗi của mô hình. Khi đó lỗi của ta được gọi là <strong>lỗi trung bình bình phương</strong> (<em>MSE - Mean Squared Error</em>):
$$E(\theta)=\frac{1}{m}\sum_{i=1}^m\Vert \hat y^{(i)}-y^{(i)}\Vert_2^2$$</p><p>Như đã đề cập trong phần <a href=https://dominhhai.github.io/vi/2017/12/ml-intro/#3-c%C3%A1c-b%C6%B0%E1%BB%9Bc-h%E1%BB%8Dc-m%C3%A1y>các bước của học máy</a> thì dữ liệu của ta sẽ được phân chia làm 3 phần là <strong>tập huấn luyện</strong> (<em>training set</em>) 60%, <strong>tập kiểm chứng</strong> (<em>cross validation set</em>) 20% và <strong>tập kiểm tra</strong> (<em>test set</em>) 20%. Ứng với mỗi phần ta sẽ đưa ra thông số lỗi tương ứng:</p><ul><li><strong>Tập huấn luyện</strong>: $\displaystyle E_{train}(\theta)=\frac{1}{m_{train}}\sum_{i=1}^{m_{train}}err(\hat y_{train}^{(i)},y_{train}^{(i)})$</li><li><strong>Tập kiểm chứng</strong>: $\displaystyle E_{CV}(\theta)=\frac{1}{m_{CV}}\sum_{i=1}^{m_{CV}}err(\hat y_{CV}^{(i)},y_{CV}^{(i)})$</li><li><strong>Tập kiểm tra</strong>: $\displaystyle E_{test}(\theta)=\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(\hat y_{test}^{(i)},y_{test}^{(i)})$</li></ul><p>Với mô hình hồi quy tuyến tính ta có thể lấy luôn hàm lỗi $\displaystyle J(\theta)=\frac{1}{2m}\sum_{i=1}^m(\hat y^{(i)}-y^{(i)})^2$ để đánh giá lỗi. Đương nhiên là ứng với mỗi phần dữ liệu ta phải sử dụng dữ liệu của phần tương ứng để đánh giá:
$$
\begin{cases}
E_{train}(\theta)=\displaystyle\frac{1}{2m_{train}}\sum_{i=1}^{m_{train}}(\hat y_{train}^{(i)}-y_{train}^{(i)})^2 \cr
E_{CV}(\theta)=\displaystyle\frac{1}{2m_{CV}}\sum_{i=1}^{m_{CV}}(\hat y_{CV}^{(i)}-y_{CV}^{(i)})^2 \cr
E_{test}(\theta)=\displaystyle\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(\hat y_{test}^{(i)}-y_{test}^{(i)})^2
\end{cases}
$$</p><h2 id=2-2-phán-định-lỗi>2.2. Phán định lỗi</h2><p>Với cách định nghĩa lỗi như trên thì một mô hình:</p><ul><li><em>Chưa khớp</em>: Cả $E_{train},E_{CV},E_{test}$ đều lớn.</li><li><em>Quá khớp</em>: $E_{train}$ nhỏ còn $E_{CV},E_{test}$ lại lớn.</li><li><em>Vừa khớp</em>: Cả $E_{train},E_{CV},E_{test}$ đều nhỏ.</li></ul><p>Tuy nhiên khi huấn luyện bạn không được phép sờ tới tập kiểm tra, nên ta sử dụng 2 tập huấn luyện và kiểm chứng để dự đoán kiểu lỗi. Ví dụ hình dưới đây mô tả lỗi đồ thị của $E_{train}(\theta)$ và $E_{CV}(\theta)$.</p><div class="figure center"><a class=fancybox href=https://res.cloudinary.com/dominhhai/image/upload/ml/diagnosing_bias_variance.png title="Hình 2: Đồ thị của các lỗi. Source: https://www.coursera.org/learn/machine-learning/" data-fancybox-group><img class=fig-img src=https://res.cloudinary.com/dominhhai/image/upload/ml/diagnosing_bias_variance.png alt="Hình 2: Đồ thị của các lỗi. Source: https://www.coursera.org/learn/machine-learning/"></a>
<span class=caption>Hình 2: Đồ thị của các lỗi. Source: https://www.coursera.org/learn/machine-learning/</span></div><p>Ở hình trên ta thấy rằng, trước điểm $d$ - bậc của đa thức hợp lý thì cả 2 lỗi đều có chiều hướng giảm dần, nhưng vượt qua điểm này thì lỗi tập huấn luyện vẫn tiếp tục nhỏ đi còn tập kiểm chứng lại vọt lên. Điều đó chứng tỏ rằng phía trước $d$ ta thu được lỗi <em>chưa khớp</em> và sau $d$ là lỗi <em>quá khớp</em>, còn ở $d$ là <em>vừa khớp</em>.</p><p>Một cách tổng quát, ta có thể dựa vào sự biến thiên của $E_{train}$ và $E_{CV}$ như trên để có phán định về tính chất của lỗi:</p><ul><li>$E_{train}$ và $E_{CV}$ đều lớn: Chưa khớp</li><li>$E_{train}$ và $E_{CV}$ đều nhỏ: Vừa khớp</li><li>$E_{train}$ nhỏ còn $E_{CV}$ lớn: Khớp quá</li></ul><h1 id=3-xử-lý-lỗi>3. Xử lý lỗi</h1><h2 id=3-1-điểm-hợp-lý>3.1. Điểm hợp lý</h2><p>Đồ thị trên còn cho ta một gợi ý rất quan trọng là ta có thể đoán được điểm hợp lý để dừng lại khi huấn luyện. Điểm dừng ở đây chính là điểm mà đồ thị của $E_{CV}$ đổi hướng. Khi bắt đầu thấy $E_{CV}$ đổi hướng sau một số vòng lặp nào đó thì ta sẽ dừng việc huấn luyện lại và chọn lấy điểm bắt đầu có sự đổi hướng này làm điểm hợp lý cho tham số và siêu tham số. Nếu bạn cần đọc thêm về việc dừng này thì có thể đọc ở <a href=https://dominhhai.github.io/vi/2017/12/ml-gd/#4-%C4%91i%E1%BB%81u-ki%E1%BB%87n-d%E1%BB%ABng>phần điều kiện dừng</a> ở phần tối ưu hàm lỗi.</p><h2 id=3-2-chưa-khớp>3.2. Chưa khớp</h2><p>Như đã đề cập chuyện này xảy ra khi mà mô hình của ta chưa đủ phức tạp. Như vậy ta cần phải tăng độ phức tạp của mô hình lên. Để tăng độ phức tạp ta có thể lấy thêm tính năng cho mẫu bằng cách thêm các $\phi(\mathbf{x})$ khác nhau. Ví dụ, tăng bậc của đa thức lên có thể giúp ta khớp hơn với tập dữ liệu chẳng hạn. Cụ thể thì bạn có xem lại ví dụ 2 của <a href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/#5-2-v%C3%AD-d%E1%BB%A5-2>bài về hồi quy tuyến tính</a>.</p><p>Khi xảy ra lỗi <em>chưa khớp</em> thì ta cần lưu ý tới một điểm quan trọng là <strong>tăng dữ liệu không giúp mô hình tốt hơn</strong>. Tại sao lại thế thì ta sẽ cùng bàn về lý thuyết cân bằng giữa phương sai vào độ lệch ở bài viết sau.</p><h2 id=3-3-quá-khớp>3.3. Quá khớp</h2><p>Khi xảy <em>quá khớp</em> ta có thể bỏ bớt tính năng đi để giảm độ phức tạp mô hình. Hoặc có thể lấy thêm dữ liệu để mô hình có thể học được một cách tổng quát hơn. Thật khó đưa ra được một cách cụ thể ngoài việc kết hợp của tất cả các kiểu xử lý trên lại với nhau sau đó đưa ra đánh giá cụ thể sau.</p><p>Ngoài ra, ta còn có một kĩ thuật nữa rất phổ biến trong học máy là <em>chính quy hoá</em> mà ta sẽ cùng xem xét ở phần ngay dưới đấy.</p><h1 id=4-kĩ-thuật-chính-quy-hoá>4. Kĩ thuật chính quy hoá</h1><h2 id=4-1-định-nghĩa>4.1. Định nghĩa</h2><p>Chính quy hoá (<em>regularization</em>) là một kĩ thuật giúp giảm lỗi khớp quá bằng cách thêm một phần chính quy hoá vào hàm lỗi như sau:
$$J(\theta)=E_X(\theta)+\lambda E_\theta(\theta)$$</p><p>$E_X(\theta)$ là hàm lỗi ban đầu và cụm $\lambda E_\theta(\theta)$ mới thêm vào là số hạng chính quy hoá đóng vai trò như một biện pháp phạt lỗi (<em>penalization</em>).</p><div class="figure center"><a class=fancybox href=https://res.cloudinary.com/dominhhai/image/upload/ml/sin2pi_ridge.png title="Hình 3: y=sin(2πx) L2. Without Ridge: λ=0 (left); Ridge: λ=1e-4 (center); Ridge: λ=1 (right)" data-fancybox-group><img class=fig-img src=https://res.cloudinary.com/dominhhai/image/upload/ml/sin2pi_ridge.png alt="Hình 3: y=sin(2πx) L2. Without Ridge: λ=0 (left); Ridge: λ=1e-4 (center); Ridge: λ=1 (right)"></a>
<span class=caption>Hình 3: y=sin(2πx) L2. Without Ridge: λ=0 (left); Ridge: λ=1e-4 (center); Ridge: λ=1 (right)</span></div><p>Trong đó, hệ số chính quy hoá $\lambda$ được chọn từ trước để cân bằng giữa $E_X(\theta)$ và $E_\theta(\theta)$. $\lambda$ càng lớn thì ta càng coi trọng $E_\theta(\theta)$, ít coi trọng tham số cho hàm lỗi ban đầu hơn, dẫn tới việc các tham số $\theta$ ít có ảnh hưởng tới mô hình hơn. Hay nói cách khác là mô hình bớt phức tạp đi giúp ta đỡ việc lỗi <em>quá khớp</em>.</p><p>$E_\theta(\theta)$ ở đây sẽ không bao gồm độ lệch $\theta_0$ và thường có dạng như sau:
$$E_\theta(\theta)=\frac{1}{p}\Vert\theta\Vert_p^p=\frac{1}{p}\sum_{i=1}^n|\theta_i|^p$$</p><p>Khi đó, hàm lỗi có thể viết lại như sau:
$$J(\theta)=E_X(\theta)+\lambda\frac{1}{p}\sum_{i=1}^n|\theta_i|^p$$</p><p>$p$ thường được chọn là 2 (<em>L2 Norm</em>) và 1 (<em>L1 Norm</em> hay còn được gọi là <em>Lasso</em> trong thống kê).</p><p>Với <em>L2</em>, hàm lỗi có dạng:
$$J(\theta)=E_X(\theta)+\frac{\lambda}{2}\theta^{\intercal}\theta$$</p><p>Với <em>L1</em>, hàm lỗi có dạng:
$$J(\theta)=E_X(\theta)+\lambda\sum_{i=1}^n|\theta_i|$$</p><p>Phương pháp chính quy hoá này còn có tên là <strong>cắt trọng số</strong> (<em>weight decay</em>) vì nó làm cho các trọng số (tham số $\theta$) bị tiêu biến dần về 0 trong khi học. Còn trong thống kê, phương pháp này có tên là <strong>co tham số</strong> (<em>parameter shrinkage</em>) vì nó làm co lại các giá trị tham số dần về 0.</p><h2 id=4-2-công-thức-chuẩn>4.2. Công thức chuẩn</h2><p>Với hàm lỗi của hồi quy tuyến tính thì ta thường chia lấy trung bình của toàn mẫu nên số hạng chính quy hoá cũng sẽ được chia tương tự. Ngoài ra ta cũng thường lấy <em>L2</em> để thực hiện việc chính quy hoá, nên:
$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m\Big(\theta^{\intercal}\phi(\mathbf{x}_i)-y_i\Big)^2+\frac{\lambda}{2m}\theta^{\intercal}\theta$$</p><p>Khi đó, công thức chuẩn được viết lại như sau:
$$\hat\theta=(\lambda\mathbf{I}+\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$$</p><h2 id=4-3-tính-đạo-hàm>4.3. Tính đạo hàm</h2><p>Việc tính đạo hàm nhằm thực hiện giải thuật <a href=https://dominhhai.github.io/vi/2017/12/ml-gd/>tối ưu với Gradient Descent</a>.</p><p>Đạo hàm khi có số hạng chính quy hoá với:</p><ul><li><em>L2</em> : $\dfrac{\partial E_X(\theta)}{\partial\theta_i}+\lambda\theta_i$</li><li><em>L1</em> : $\dfrac{\partial E_X(\theta)}{\partial\theta_i}+\lambda\text{sgn}(\theta_i)$</li></ul><blockquote><p>Lưu ý: đạo hàm này <strong>không</strong> tính cho $\theta_0$. Nói cách khác $\theta_0$ không được thêm số hạng chính quy hoá.</p></blockquote><p>Trường hợp của bài toán hồi quy tuyến tính:</p><p>$$\frac{\partial}{\partial\theta_i}=\frac{1}{m}\sum_{j=1}^m(\theta^{\intercal}\phi(\mathbf{x}_j)-y_j)\mathbf{x_j}+\begin{cases}0 &amp;\text{for }i=0\cr\frac{\lambda}{m}\theta_i &amp;\text{for }i&gt;0\end{cases}$$</p><p>Gradient có dạng sau:
$$\Delta_\theta J(\theta)=\frac{1}{m}(\theta^{\intercal}\Phi-y)\Phi+\frac{\lambda}{m}\theta$$</p><p>Đương nhiên là khi tính số hạng chính quy hoá ta gắn $\theta_0\triangleq 0$ để tiêu biến số hạng đó đi.</p><h2 id=4-4-cài-đặt>4.4. Cài đặt</h2><p>Hệ số chính quy hoá $\lambda$ thường nhỏ để không quá ảnh hưởng nhiều tới việc tối ưu lỗi truyền thống. Thường người ta sẽ chọn lấy 1 danh sách các $\lambda$ để huấn luyện và lấy một giá trị tối ưu nhất. Tuy nhiên, lưu ý rằng hệ số này không dùng cho tập <em>kiểm chứng</em> khi đối chiếu để đánh giá mô hình.</p><p>Cụ thể các bước cài đặt như sau:</p><ol><li>Tạo danh sách các $\lambda$.</li><li>Tạo các mô hình tương ứng với các $\phi(\mathbf{x})$ tương ứng. Ví dụ như bậc của đa thức hay co giãn các thuộc tính chẳng hạn.</li><li>Học tham số $\theta$ ứng với từng $\lambda$ một.</li><li>Tính lỗi với tập kiểm chứng $E_{CV}(\theta)$ ứng với tham số $\theta$ học được (lúc này đặt $\lambda=0$).</li><li>Chọn lấy mô hình ứng với tham số và $\lambda$ cho ít lỗi nhất với tập kiểm chứng.</li><li>Lấy $\theta$ và $\lambda$ tương ứng rồi tính lỗi cho tập kiểm tra $E_{test}(\theta)$ và đánh giá mô hình.</li></ol><p>Nếu hứng thú bạn có thể xem ví dụ cài đặt thuật toán với chính quy hoá <a href=https://github.com/dominhhai/mldl/blob/master/code/linear_regression/one_var_sin2pi-regularization.ipynb target=_blank _ rel="noopener noreferrer">tại đây</a> nhé.</p><h1 id=5-kết-luận>5. Kết luận</h1><p>Đánh giá mô hình có thể chia thành 3 dạng <em>chưa khớp</em> khi nó chưa đủ độ phức tạp, <em>quá khớp</em> khi nó quá phức tạp và <em>vừa khớp</em> khi mà nó vừa đủ để tổng quát hoá. Khi huấn luyện ta có thể sử dụng <em>tập huấn luyện</em> và <em>tập kiểm chứng</em> để đánh giá mô hình đang ở tình trạng nào. Nếu $E_{train},E_{CV}$ đều lớn thì ta nói rằng nó <em>chưa khớp</em>, còn $E_{train}$ nhỏ và $E_{CV}$ lớn thì ta nói rằng nó bị <em>quá khớp</em>.</p><p>Bài toán chưa khớp thì ta có thể giải quyết bằng cách phức tạp hoá mô hình lên còn với bài toán quá khớp thì ta có thể sử dụng phương pháp chính quy hoá để giải quyết:
$$J(\theta)=E_X(\theta)+\lambda E_\theta(\theta)$$</p><p>Hệ số $\lambda$ càng lớn thì mô hình sẽ càng đơn giản đi từ đó giúp tránh được chuyện <em>quá khớp</em> nhưng cũng dẫn tới việc <em>chưa khớp</em>. Nên ta cần phải chọn được giá trị $\lambda$ hợp lý. Thường ta sẽ đưa ra 1 danh sách các hệ số $\lambda$ rồi chạy lần lượt và chọn lấy một giá trị tốt nhất. Tuy nhiên ta cần phải nhớ rằng cụm chuẩn hoá này không dùng cho tập <em>kiểm chứng</em> khi huấn luyện.</p><p>Mặc dù qua bài này còn đôi chỗ hơi khó hiểu và mơ hồ nhưng nhìn chung nếu chỉ lập trình thì ta nhớ lấy hệ số $\lambda$ là được. Nếu bạn hứng thú tìm hiểu tận gốc vấn đề thì ta sẽ cùng xem trong bài viết tới về vấn đề cân bằng giữa phương sai và độ lệch của mô hình.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/20171226-ml-bias-variance-tradeoff/ data-tooltip="[ML] Cân bằng phương sai và độ lệch"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-gd/ data-tooltip="[ML] Tối ưu hàm lỗi với Gradient Descent"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-overfitting/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-overfitting/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-overfitting/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=fb-root></div><script>(function(d,s,id){if(window.location.hostname=='localhost')return;var js,fjs=d.getElementsByTagName(s)[0];if(d.getElementById(id))return;js=d.createElement(s);js.id=id;js.src='https://connect.facebook.net/vi_VN/sdk.js#xfbml=1&version=v3.1&appId=333198270561466&autoLogAppEvents=1';fjs.parentNode.insertBefore(js,fjs);}(document,'script','facebook-jssdk'));</script><div id=fb-cmt-thread class=fb-comments data-href=https://dominhhai.github.io/vi/2017/12/ml-overfitting/ data-width=100%></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2018 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/20171226-ml-bias-variance-tradeoff/ data-tooltip="[ML] Cân bằng phương sai và độ lệch"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-gd/ data-tooltip="[ML] Tối ưu hàm lỗi với Gradient Descent"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-overfitting/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-overfitting/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-overfitting/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-overfitting%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-overfitting%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-overfitting%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/12\/ml-overfitting\/';this.page.identifier='\/vi\/2017\/12\/ml-overfitting\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>