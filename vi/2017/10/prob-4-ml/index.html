<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.40.2 with theme Tranquilpeak 0.4.1-BETA"><title>[Xác Suất] Đằng sau giải thuật phân loại</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Probability,Xác Suất,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/10/prob-4-ml/><meta name=description content="Cho tới thời điểm này chúng ta đã có 1 số khái niệm và phương pháp căn bản của xác suất thống kê. Vậy giờ chúng ta áp dụng nó thể nào cho các bài toán học máy?"><meta property=og:type content=website><meta property=og:title content="[Xác Suất] Đằng sau giải thuật phân loại"><meta property=og:url content=https://dominhhai.github.io/vi/2017/10/prob-4-ml/><meta property=og:description content="Cho tới thời điểm này chúng ta đã có 1 số khái niệm và phương pháp căn bản của xác suất thống kê. Vậy giờ chúng ta áp dụng nó thể nào cho các bài toán học máy?"><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="[Xác Suất] Đằng sau giải thuật phân loại"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/10/prob-4-ml/><meta name=twitter:description content="Cho tới thời điểm này chúng ta đã có 1 số khái niệm và phương pháp căn bản của xác suất thống kê. Vậy giờ chúng ta áp dụng nó thể nào cho các bài toán học máy?"><meta name=twitter:creator content=@minhhai3b><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/prob/icon.png><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/prob/icon.png><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/#about><i class="sidebar-button-icon fa fa-lg fa-address-card"></i><span class=sidebar-button-desc>Thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[Xác Suất] Đằng sau giải thuật phân loại</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-10-15T00:00:00Z>15 tháng 10, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/x%c3%a1c-su%e1%ba%a5t>Xác Suất</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/th%e1%bb%91ng-k%c3%aa>Thống Kê</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Cho tới thời điểm này chúng ta đã có 1 số khái niệm và phương pháp căn bản của xác suất thống kê. Vậy giờ chúng ta áp dụng nó thể nào cho các bài toán học máy?</p><p>Bài viết này sẽ chỉ ra lý thuyết đằng sau bài toán phân loại để có thể hiểu sâu hơn về cách áp dụng lý thuyết xác suất thống kê cho các bài toán học máy.</p><h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-bài-toán-phân-loại>1. Bài toán phân loại</a></li><li><a href=#2-giải-thuật-naive-bayes>2. Giải thuật Naive Bayes</a><ul><li><a href=#2-1-tóm-tắt-giải-thuật>2.1. Tóm tắt giải thuật</a></li><li><a href=#2-2-lý-thuyết>2.2. Lý thuyết</a></li></ul></li><li><a href=#3-giải-thuật-logistic-regression>3. Giải thuật Logistic Regression</a><ul><li><a href=#3-1-tóm-tắt-giải-thuật>3.1. Tóm tắt giải thuật</a></li><li><a href=#3-2-lý-thuyết>3.2. Lý thuyết</a></li></ul></li><li><a href=#4-kết-luận>4. Kết luận</a></li></ul></nav><h1 id=1-bài-toán-phân-loại>1. Bài toán phân loại</h1><p>Trong học máy, bài toán phân loại là bài toán gắn nhãn cho mỗi đầu vào. Do các nhãn này là biết trước nên ta có thể biểu diễn chúng qua một tập hợp $ Y $ hữu hạn. Còn các đầu vào sẽ được coi là một tập hợp $ X $. Như vậy, thực chất bài toán dãn nhãn là đi tìm $ y \in Y $ sao cho xác suất của nó đối với 1 đầu vào $ x \in X $ là lớn nhất. Tập dữ liệu huấn luyện của ta là một tập các cặp $ (x, y) $ đã biết trước.</p><p>Về mặt toán học, ta có thể biểu diễn mỗi cặp $(x, y)$ bằng một hàm số dự đoán: $\hat y = f(x)$. Như vậy bài toán của ta có thể biểu diễn thành dạng: $\hat{y}=f(x)=\underset{y}{\mathrm{argmax}}\hat{P}(Y = y|X)$.</p><blockquote><p>$\underset{y}{\mathrm{argmax}}$ là hàm trả ra giá trị của tham số $y$ mà tại đó khiến hàm đạt được giá trị lớn nhất.</p></blockquote><p>Lưu ý rằng mỗi nhãn $ y \in Y \subset \mathbb{N} $ là một nhãn riêng biệt và ta quy ước nó thuộc trường số tự nhiên. Còn mỗi đầu vào $ x \in X \subset \mathbb{R^m} $ là một véc-tơ tương ứng với số thuộc tính của $ x $, hay nói cách khác nếu mỗi đầu vào có $m$ thuộc tính thì $x$ là một véc-tơ $m$ chiều với mỗi phần tử đại diện cho 1 thuộc tính.</p><h1 id=2-giải-thuật-naive-bayes>2. Giải thuật Naive Bayes</h1><h2 id=2-1-tóm-tắt-giải-thuật>2.1. Tóm tắt giải thuật</h2><p>Giải thuật Naive Bayes được trình bày như sau:</p><ul><li><strong>Huấn luyện</strong><br>Mục tiêu của quá trình huấn luyện là đánh giá xác suất $ P(Y) $ và $ P(X_i|Y) $ cho mọi thuộc tính $ i=\overline{1, m} $. Để làm việc này, ta có thể sử dụng 1 trong 2 cách đánh giá sau:<ul><li>MLE (Maximum Likelihood Estimation)
$$ \hat{p}(X_i=x_i|Y=y) = \frac{(\text{training examples where Xi = xi and Y = y})}{(\text{training examples where Y = y})} $$</li><li>Laplace MAP (Maximum A Posteriori probability)
$$ \hat{p}(X_i=x_i|Y=y) = \frac{(\text{training examples where Xi = xi and Y = y}) + 1}{(\text{training examples where Y = y}) + 2} $$</li></ul></li><li><strong>Dự đoán</strong><br>Với mỗi đầu vào $ x $, ta đánh giá $ y $ như sau:
$$
\begin{aligned}
\hat{y} &amp;= \underset{y}{\mathrm{argmax}}\hat{P}(X|Y)\hat{P}(Y)
\cr
\ &amp;= \underset{y}{\mathrm{argmax}}\prod_{i=1}^m\hat{p}(X_i=x_i|Y=y)\hat{p}(Y=y)
\cr
\ &amp;= \underset{y}{\mathrm{argmax}}\sum_{i=1}^m\log\hat{p}(X_i=x_i|Y=y) + \log\hat{p}(Y=y)
\end{aligned}
$$</li></ul><p>Ở đây, ta lấy phiên bản $\log$ nhằm tránh việc triệt tiêu số khi nhân vì xác suất luôn nằm trong khoảng $[0, 1]$ nên khi nhân vào sẽ ra số rất bé dẫn tới chuyện triệt tiêu mất số khi tính toán. Tuy nhiên nếu số lượng tập huấn luyện và thuộc tính đầu vào là ít, ta vẫn có thể sử dụng được phiên bản nhân xác suất.</p><h2 id=2-2-lý-thuyết>2.2. Lý thuyết</h2><p>Về mặt lý thuyết ta có thể giải bài toán phân loại bằng cách sử dụng phương pháp vét cạn. Để làm như vậy ta có thể tối ưu xác suất kết hợp $\hat{P}(Y, X)$. Vì sao ta có thể nói như vậy?</p><p>Quay lại lý thuyết của bài toán phân loại rằng ta cần tìm $y$ sao cho:
$$\hat{y} = f(x) = \underset{y}{\mathrm{argmax}}\hat{P}(Y = y|X)$$.
Do đây là xác suất có điều kiện nên:
$$\hat{y} = \underset{y}{\mathrm{argmax}}\frac{\hat{P}(X,Y)}{\hat{P}(X)}$$
Vì $\hat{P}(X)$ là hằng số do $X$ là tập đã biết trước, nên ta có:
$$\hat{y} = \underset{y}{\mathrm{argmax}}\hat{P}(X,Y)$$</p><p>Vì lượng thuộc tính của $x \in X$ là $m$ nên khi tính xác suất hợp này ta có tới $2^m$ tham số! Tức không gian nhớ cần để thực hiện phương pháp này là $\Theta({2^m})$, thật khủng khiếp phải không?</p><p>Tuy nhiên bằng giả thiết Naive Bayes, ta có thể giảm không gian này xuống còn $\Theta{(m)}$. Giả thuyết này cho rằng mỗi thuộc tính của $x$ là độc lập đối với mỗi nhãn $y$ bất kì. Giải thuyết này là sai nhưng cực kì hữu ích và vẫn thực hiện tốt việc phân loại trong rất nhiều bài toán thực tế. Với giả thuyết này ta sẽ có:
$$
\begin{aligned}
\hat{y} &amp;= \underset{y}{\mathrm{argmax}}\hat{P}(X,Y)
\cr
\ &amp;= \underset{y}{\mathrm{argmax}}\hat{P}(X|Y)\hat{P}(Y)
\cr
\ &amp;= \underset{y}{\mathrm{argmax}}\prod_{i=1}^m\hat{p}(X_i=x_i|Y=y)\hat{p}(Y=y)
\cr
\end{aligned}
$$</p><p>Bằng ước lượng kiểu này mà giải thuật của ta có thể chạy nhanh và ổn định cho cả 2 việc huấn luyện và dự đoán. Naive Bayes là một dạng đơn giản của mô hình đồ họa xác suất (Probabilistic Graphical Models) trong lĩnh vực học máy. Với bài toán dạng này, ta có thể vẽ một đồ thị thể hiện mối tương quan giữa các biến từ đó đưa ra giả thuyết nhằm đánh giá, dự đoán bằng phép kết hợp xác suất có điều kiện.</p><h1 id=3-giải-thuật-logistic-regression>3. Giải thuật Logistic Regression</h1><h2 id=3-1-tóm-tắt-giải-thuật>3.1. Tóm tắt giải thuật</h2><p>Logistic Regression là một giải thuật phân loại bằng cách học một hàm ước lượng xác suất $P(X|Y)$. Với giả thuyết trọng tâm rằng xác suất này có thể biểu diễn bằng một hàm $\mathrm{sigmoid}$ với tham số là một kết hợp tuyến tính của các thuộc tính đầu vào. Việc hiểu giải thuật này là rất cần thiết vì nó là bước cơ bản nhất của một mạng trí tuệ nhân tạo, nên bạn đừng vội vàng bỏ qua lý thuyết đằng sau nó nhé.</p><blockquote><p>Kết hợp tuyết tính có thể hiểu đơn giản là một hàm tuyến tính (linear function) hay còn gọi là hàm bậc nhất.</p></blockquote><p>Về mặt toán học, với mỗi điểm dữ liệu huấn luyện (x, y), Logistic Regression giả sử rằng:
$$P(Y=1|X=x) \triangleq \sigma(z) ~~~, \text{where } z = \theta_0 + \sum_{i=1}^m\theta_ix_i $$</p><p>Đặt $x_0 = 1$, giả thuyết này có thể được viết dưới các dạng tương đương sau:
$$
\begin{aligned}
P(Y=1|X=x) &amp;= \sigma(\theta^\intercal x)
\cr
P(Y=0|X=x) &amp;= 1 - \sigma(\theta^\intercal x)
\end{aligned}
$$</p><p>Như vậy bài toán của ta lúc này sẽ là đi tìm các $\theta$ sao cho xác suất cho tập dữ liệu của ta là lớn nhất có thể.</p><p>Tương tự như giải thuật Naive Bayes, ta cũng sẽ sử dụng ước lượng theo hàm $\log$ để tìm $\theta$ (<em>Log Likelihood</em>) như sau:</p><p>$$LL(\theta) = \sum_{i=1}^n y^{(i)}\log\sigma(\theta^\intercal x) + (1-y^{(i)})\log\big(1-\sigma(\theta^\intercal x)\big) $$</p><p>Về mặt toán học để tìm cực đại của hàm này là không đơn giản chút nào, nhưng ta có thể thực hiện bằng máy tính bằng phương pháp leo đồi - Gradient Ascent Optimization. Ý tưởng đằng sau giải thuật này là nếu ta tiến từng bước nhỏ theo hướng của gradient thì ta sẽ đạt được cực đại cục bộ. Nhưng trong bài toán Logistic Regression ta có thể chứng minh rằng cực đại này chính là cực đại toàn cục. Mỗi bước tiến nhỏ như vậy được được dịch chuyển bởi 1 độ học $ \eta $ đủ nhỏ để ta không bị vượt quá giá trị cực trị nhưng cũng không quá nhỏ khiến giải thuật ta bị chậm đi. Tại mỗi bước đó, ta cập nhập các tham số $\theta$ như sau:</p><p>$$\theta_j^\text{new} = \theta_j^\text{old} + \eta\frac{\partial LL(\theta^\text{old})}{\partial\theta_j^\text{old}}$$</p><p>Do gradient của $LL(\theta)$ theo mỗi véc-tơ $\theta_j$ là $\sum_{i=1}^n\Big(y^{(i)}-\sigma(\theta^\intercal x)\Big)x_j^{(i)}$, nên ta có:</p><p>$$\theta_j^\text{new} = \theta_j^\text{old} + \eta\sum_{i=1}^n\Big(y^{(i)}-\sigma(\theta^\intercal x)\Big)x_j^{(i)}$$</p><p>Giải thuật tối ưu hoàn chỉnh có thể viết dưới dạng giả mã như sau:</p><pre><code>// Init theta
for i = 0 to m
    theta_j := 0
// optimize
do (many times)
    // calc gradient
    for j = 0 to m
        gradient[j] := 0
    for each training data (x, y)
        for j = 0 to m
            z := transpose(theta) ・ x
            gradient[j] += x_j(y - sigmoid(z))
    // gradient ascent
    for j = 0 to m
        theta_j += eta * gradient[j]
</code></pre><h2 id=3-2-lý-thuyết>3.2. Lý thuyết</h2><p>Để giải quyết bài toán phân loại này ta lại sử dụng phương pháp đánh giá MLE (Maximum Likelihood Estimation) bằng 2 bước:</p><ul><li>Biểu diễn hàm tối ưu dạng hàm log-likelihood của $\theta$</li><li>Tìm $\theta$ để hàm này đạt cực đại</li></ul><p>Với giả thuyết ta chỉ có 2 nhãn $y = 0$ hoặc $y=1$, ta có thể sử dụng hàm xác suất Béc-nu-li $Y \sim \mathrm{Bern}(p)$ để ước lượng xác suất của mỗi nhãn như sau:</p><p>$$P(Y=y|X=x) = p^y (1-p)^{(1-y)} $$</p><p>Trong đó: $p = \sigma(\theta^\intercal x)$. Từ đây ta có tích xác suất hợp của cả tập dữ liệu như sau:</p><p>$$
\begin{aligned}
L(\theta) &amp;= \prod_{i=1}^n P(Y=y^{(i)}|X=x^{(i)})
\cr
\ &amp;= \sigma(\theta^\intercal x)^{y^{(i)}} \big(1-\sigma(\theta^\intercal x)\big)^{1-y^{(i)}}
\end{aligned}
$$</p><p>Giờ ta lấy $\log$ để phân rã phép nhân để có hàm Log-Likelihood như sau:</p><p>$$LL(\theta) = \sum_{i=1}^n y^{(i)}\log\sigma(\theta^\intercal x)+(1-y^{(i)})\log(1-\sigma(\theta^\intercal x))$$</p><p>Bước tiếp theo là tìm $\theta$ để hàm này đạt cực đại bằng phương pháp Gradient Ascent đề cập ở trên. Để tìm được gradient đối với mỗi $\theta_j$, ta có thể sử dụng phương pháp đạo hàm của hàm hợp (quy tắc chuỗi) như sau:</p><p>$$\frac{\partial LL(\theta)}{\partial \theta_j} = \frac{\partial LL(\theta)}{\partial p}\frac{\partial p}{\partial z}\frac{\partial z}{\partial \theta_j}$$</p><p>Trong đó: $z=\theta^\intercal x$ và $p=\sigma(z)$. Giờ sẽ tính đạo hàm của từng thành phần rồi sau ghép lại với nhau.</p><ul><li><p>$\displaystyle\frac{\partial LL(\theta)}{\partial p}$<br>Ta có $LL(\theta)=y\log p + (1-y)\log(1-p)$ nên đạo hàm sẽ là:
$$
\begin{aligned}
\frac{\partial LL(\theta)}{\partial p} &amp;= \frac{y}{p}-\frac{1-y}{1-p}
\cr
\ &amp;= \frac{y-p}{p(1-p)}
\end{aligned}
$$</p></li><li><p>$\displaystyle\frac{\partial p}{\partial z}$<br>Ta có $p=\sigma(z)$ nên đạo hàm sẽ là:
$$
\begin{aligned}
\frac{\partial p}{\partial z} &amp;= \sigma(z)(1-\sigma(z))
\cr
\ &amp;= p(1-p)
\end{aligned}
$$</p></li><li><p>$\displaystyle\frac{\partial z}{\partial \theta_j}$<br>Ta có $z=\theta^\intercal x$ nên đạo hàm sẽ là:
$$\frac{\partial z}{\partial \theta_j} = x_j$$</p></li></ul><p>Kết hợp chúng lại với nhau ta sẽ được:
$$
\begin{aligned}
\frac{\partial LL(\theta)}{\partial \theta_j} &amp;= \frac{\partial LL(\theta)}{\partial p}\frac{\partial p}{\partial z}\frac{\partial z}{\partial \theta_j}
\cr
\ &amp;= \frac{y-p}{p(1-p)} p(1-p) x_j
\cr
\ &amp;= (y-p) x_j
\cr
\ &amp;= (y - \sigma(\theta^\intercal x)) x_j
\end{aligned}
$$</p><p>Như vậy gradient của hàm Log-Likelihood trên là:
$$\frac{\partial LL(\theta)}{\partial \theta_j} = (y - \sigma(\theta^\intercal x)) x_j$$</p><p>Với đạo hàm như vậy ta có thể thực hiện được hàm thuật toán leo đồi như trên rồi.</p><h1 id=4-kết-luận>4. Kết luận</h1><p>Qua bài toán đơn giản này hi vọng bạn có được cái nhìn về cách áp dụng xác suất thống kê cho các bài toán học máy cũng như tầm quan trọng của nó trong việc xây dựng các thuật toán học máy.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/what-is-rnn/ data-tooltip="[RNN] RNN là gì?"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/ data-tooltip="[Xác Suất] Mẫu thống kê và ước lượng tham số"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/10/prob-4-ml/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/10/prob-4-ml/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/10/prob-4-ml/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2018 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/what-is-rnn/ data-tooltip="[RNN] RNN là gì?"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/ data-tooltip="[Xác Suất] Mẫu thống kê và ước lượng tham số"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/10/prob-4-ml/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/10/prob-4-ml/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/10/prob-4-ml/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fprob-4-ml%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fprob-4-ml%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fprob-4-ml%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/10\/prob-4-ml\/';this.page.identifier='\/vi\/2017\/10\/prob-4-ml\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>