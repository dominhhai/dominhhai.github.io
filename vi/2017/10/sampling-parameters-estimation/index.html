<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.41 with theme Tranquilpeak 0.4.1-BETA"><title>[Xác Suất] Mẫu thống kê và ước lượng tham số</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Probability,Xác Suất,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/><meta name=description content="Trong các phần trước ta đã tìm hiểu cơ bản về xác suất và thống kê xác suất cũng như một số mô hình thống kê thông dụng, dựa vào đó ta tiếp tục lấn sang 1 phần quan trọng là thống kê và ước lượng các tham số cho các bài toán thực tế."><meta property=og:type content=website><meta property=og:title content="[Xác Suất] Mẫu thống kê và ước lượng tham số"><meta property=og:url content=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/><meta property=og:description content="Trong các phần trước ta đã tìm hiểu cơ bản về xác suất và thống kê xác suất cũng như một số mô hình thống kê thông dụng, dựa vào đó ta tiếp tục lấn sang 1 phần quan trọng là thống kê và ước lượng các tham số cho các bài toán thực tế."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="[Xác Suất] Mẫu thống kê và ước lượng tham số"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/><meta name=twitter:description content="Trong các phần trước ta đã tìm hiểu cơ bản về xác suất và thống kê xác suất cũng như một số mô hình thống kê thông dụng, dựa vào đó ta tiếp tục lấn sang 1 phần quan trọng là thống kê và ước lượng các tham số cho các bài toán thực tế."><meta name=twitter:creator content=@minhhai3b><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/prob/icon.png><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/prob/icon.png><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.1.0/css/all.css integrity=sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/talk/><i class="sidebar-button-icon fa fa-lg fa-chalkboard-teacher"></i><span class=sidebar-button-desc>Thuyết Trình</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fab fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fab fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[Xác Suất] Mẫu thống kê và ước lượng tham số</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-10-14T00:00:00Z>14 tháng 10, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/x%c3%a1c-su%e1%ba%a5t>Xác Suất</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/th%e1%bb%91ng-k%c3%aa>Thống Kê</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Trong các phần trước ta đã tìm hiểu cơ bản về xác suất và thống kê xác suất cũng như một số mô hình thống kê thông dụng, dựa vào đó ta tiếp tục lấn sang 1 phần quan trọng là thống kê và ước lượng các tham số cho các bài toán thực tế.</p><h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-mẫu-thống-kê>1. Mẫu thống kê</a><ul><li><a href=#1-1-mẫu-ngẫu-nhiên>1.1. Mẫu ngẫu nhiên</a></li><li><a href=#1-2-thống-kê>1.2. Thống kê</a></li><li><a href=#1-3-đặc-trưng-mẫu>1.3. Đặc trưng mẫu</a><ul><li><a href=#1-3-1-trung-bình-mẫu>1.3.1. Trung bình mẫu</a></li><li><a href=#1-3-2-phương-sai-mẫu>1.3.2. Phương sai mẫu</a></li></ul></li></ul></li><li><a href=#2-ước-lượng-tham-số>2. Ước lượng tham số</a><ul><li><a href=#2-1-tham-số>2.1. Tham số</a></li><li><a href=#2-2-mle>2.2. MLE</a><ul><li><a href=#2-2-1-khái-niệm>2.2.1. Khái niệm</a></li><li><a href=#2-2-2-ví-dụ>2.2.2. Ví dụ</a></li></ul></li><li><a href=#2-3-map>2.3. MAP</a><ul><li><a href=#2-3-1-khái-niệm>2.3.1. Khái niệm</a></li><li><a href=#2-3-2-siêu-tham-số>2.3.2. Siêu tham số</a></li><li><a href=#2-3-3-ví-dụ>2.3.3. Ví dụ</a></li></ul></li></ul></li><li><a href=#3-kết-luận>3. Kết luận</a></li></ul></nav><h1 id=1-mẫu-thống-kê>1. Mẫu thống kê</h1><p>Trong thực tế khi muốn thống kê để tìm quan hệ giữa các yếu tố ngẫu nhiên ta thường xuyên phải làm việc với các tập dữ liệu rất lớn tới mức không đủ thời gian và chi phí để làm việc. Vậy nên việc chọn lấy 1 tập mẫu nhỏ trong đó để mô phỏng là rất cần thiết. Quá trình lấy mẫu này đòi hỏi nhiều kĩ thuật sao cho mẫu lấy ra có thể đại diện được cho toàn bộ tập dữ liệu. Tuy nhiên, bài viết này sẽ không tập trung vào quá trình lấy mẫu đó mà sẽ tập trung vào việc tìm các đặc tính của tập mẫu đó.</p><h2 id=1-1-mẫu-ngẫu-nhiên>1.1. Mẫu ngẫu nhiên</h2><p>Mẫu ngẫu nhiên là tập các mẫu độc lập và có cùng một thống kê xác suất (<em>I.I.D - Independent, Identically Distributed</em>). Ví dụ ta cần thống kê mức độ xinh gái ảnh hưởng thế nào tới trí thông minh của chị em. Thì ta có thể coi độ xinh gái là một biến ngẫu nhiên. Lúc này ta lấy mẫu $n$ người và mỗi người sẽ có độ xinh gái là $X_i$ tương ứng. Khi đó ta có thể coi rằng $X_i$ là độc lập đôi một với nhau và
chúng có cùng một phân phối xác suất. Tập các mẫu này là mẫu ngẫu nhiên $X=[X_1,X_2,&hellip;,X_n]$ kích thước $n$.</p><p>Như vậy nếu gọi $p_X(x)$ là hàm trọng lượng xác suất đồng thời nếu $X_i$ là rời rạc và $f_X(x)$ là hàm mật độ xác suất đồng thời nếu $X_i$ là liên tục thì ta sẽ có:
$$p_X(x)=\prod_{i=i}^np_{X_i}(x_i)$$
và
$$f_X(x)=\prod_{i=i}^nf_{X_i}(x_i)$$</p><h2 id=1-2-thống-kê>1.2. Thống kê</h2><p>Ta đã chọn ra được mẫu ngẫu nhiên rồi và giờ là lúc ta cần xem quan hệ của chúng ra sao. Phép lấy quan hệ như vậy được gọi là thống kê. Về mặt hình thức, ta có thể định nghĩa một hàm $Y=g(X)$ bất kì là một thống kê phụ thuộc vào mẫu ngẫu nhiên $X$.</p><p>Ví dụ: $\displaystyle\overline X=g(X)=\frac{1}{n}\sum_{i=1}^nX_i$ có thể coi là một thống kê. Thống kê này có tên là trung bình mẫu.</p><h2 id=1-3-đặc-trưng-mẫu>1.3. Đặc trưng mẫu</h2><p>Ở đây ta sẽ xét một số thống kê cơ bản cho mẫu ngẫu nhiên và gọi chúng là các đặc trưng mẫu. Giả sử ta có mẫu ngẫu nhiên $X=[X_1,X_2,&hellip;,X_n]$ kích thước $n$ tuân theo một phân phối có kỳ vọng là $\mu$ và phương sai là $\sigma^2$.</p><h3 id=1-3-1-trung-bình-mẫu>1.3.1. Trung bình mẫu</h3><p>Trung bình mẫu (<em>Mean</em>) hay còn gọi là kỳ vọng mẫu (<em>Expectation</em>) của một mẫu ngẫu nhiên là giá trị trung bình của mẫu đó:
$$\overline X=\frac{1}{n}\sum_{i=1}^nX_i$$</p><p>Rõ ràng $\overline X$ cũng sẽ là một biến ngẫu nhiên và ta có thể tính được các đặc trưng của biến ngẫu nhiên này như:</p><table><thead><tr><th>Đặc trưng</th><th>Giá trị</th></tr></thead><tbody><tr><td>Kỳ vọng - $E[\overline X]$</td><td>$\mu$</td></tr><tr><td>Phương sai - $Var(\overline X)$</td><td>$\dfrac{\sigma^2}{n}$</td></tr></tbody></table><p>Chứng minh:
$$
\begin{aligned}
E[\overline X] &amp;= E\bigg[\frac{1}{n}\sum_{i=1}^nX_i\bigg]
\cr\ &amp;= \frac{1}{n}E\bigg[\sum_{i=1}^nX_i\bigg]
\cr\ &amp;= \frac{1}{n}\sum_{i=1}^nE[X_i]
\cr\ &amp;= \frac{1}{n}\sum_{i=1}^n\mu
\cr\ &amp;= \mu
\end{aligned}
$$</p><p>$$
\begin{aligned}
Var[\overline X] &amp;= Var\bigg[\frac{1}{n}\sum_{i=1}^nX_i\bigg]
\cr\ &amp;= \frac{1}{n^2}Var\bigg[\sum_{i=1}^nX_i\bigg]
\cr\ &amp;= \frac{1}{n^2}\sum_{i=1}^nVar[X_i]
\cr\ &amp;= \frac{1}{n^2}\sum_{i=1}^n\sigma^2
\cr\ &amp;= \frac{\sigma^2}{n}
\end{aligned}
$$</p><p>Như vậy là ta có thể thấy rằng giá trị kỳ vọng của biến ngẫu nhiên trung bình luôn là hằng số và bằng kỳ vọng của mẫu ngẫu nhiên. Tức là nếu ta lấy mẫu ngẫu nhiên từ 1 tập mẫu ra thì các tập mẫu ngẫu nhiên này luôn có cùng giá trị trung bình. Nói cách khác trung bình mẫu là không lệch (<em>unbiased</em>).</p><h3 id=1-3-2-phương-sai-mẫu>1.3.2. Phương sai mẫu</h3><p>Phương sai mẫu $S^2$ là giá trị trung bình của phương sai của mẫu ngẫu nhiên:
$$S^2=\frac{1}{n}\sum_{i=1}^n(X_i-\overline X)^2$$</p><p>Kỳ vọng của biến ngẫu nhiên $S^2$ sẽ là: $E[S^2]=\dfrac{n-1}{n}\sigma^2$. Như vậy là nó không còn bằng với phương sai của $X$ nữa, nên người ta thương lấy một dạng phương sai khác sao cho kỳ vọng của nó là bằng $\sigma^2$. Khái niệm này gọi là phương sai hiệu chỉnh, kí hiệu là $s^2$:
$$s^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline X)^2$$</p><p>Chú ý rằng trong nhiều tài liệu người ta coi luôn phương sai hiệu chỉnh là phương sai mẫu.</p><p>Giá trị kỳ vọng của biến ngẫu nhiên phương sai hiệu chỉnh luôn là hằng số và bằng phương sai của mẫu ngẫu nhiên. Tức là nếu ta lấy mẫu ngẫu nhiên từ 1 tập mẫu ra thì các tập mẫu ngẫu nhiên này luôn có cùng giá trị kỳ vọng của phương sai. Nói cách khác phương sai hiệu chỉnh mẫu là không lệch (<em>unbiased</em>).</p><h1 id=2-ước-lượng-tham-số>2. Ước lượng tham số</h1><p>Là quá trình đi tìm tham số để mô tả quan hệ của các biến ngẫu nhiên. Trong phần <a href=https://dominhhai.github.io/vi/2017/10/prob-rand-mulvar/>hợp nhiều biến ngẫu nhiên</a> ta đã nói về khái niệm tương quan của các biến ngẫu nhiên và hệ số tương quan của chúng. Khi đó ta cũng đã nói qua về mô hình hồi quy (hay sự phụ thuộc tuyến tính) giữa 2 biến ngẫu nhiên $X,Y$: $Y=a+bX$. Tuy nhiên đó chỉ là một ví dụ đơn giản về việc tìm tham số $a,b$. Trong thực tế ta thường phải tìm tham số cho các mô hình xác suất phức tạp hơn nhiều như mô hình phân phối chuẩn chẳng hạn.</p><p>Quá trình ước lượng tham số này cũng chính là ý tưởng bên dưới của các bài toán học máy và nó được gọi là quá trình huấn luyện. Một bài toán học máy có chung 2 giai đoạn:</p><ul><li>① Mô hình hoá tập mẫu (dữ liệu huấn luyện) bằng một mô hình xác suất với các tham số tương ứng</li><li>② Tìm các tham số đó bằng tập mẫu đã có. Hay còn gọi là học các tham số đó bằng dữ liệu huấn luyện.</li></ul><p>Trong phần này ta sẽ coi tập mẫu là mẫu ngẫu nhiên và ta tìm hiểu 2 phương pháp tìm tham số chính là MLE và MAP.</p><h2 id=2-1-tham-số>2.1. Tham số</h2><p>Tham số là các giá trị quyết định sự phụ thuộc xác suất của các biến ngẫu nhiên trong mô hình thống kê tương ứng. Các tham số này được kí hiệu là $\theta$ và nó là một véc-to có số chiều bằng với số lượng tham số thành phần. Ví dụ:</p><table><thead><tr><th>Mô hình</th><th>Tham số</th></tr></thead><tbody><tr><td>Phân phối đều - $X \sim \mathcal{Unif}(a, b)$</td><td>$\theta=[a,b]$</td></tr><tr><td>Phân phối Béc-nu-li - $X \sim \mathcal{Bern}(p)$</td><td>$\theta=b$</td></tr><tr><td>Phân phối nhị thức - $X \sim \mathcal{Bin}(n,p)$</td><td>$\theta=[n,p]$</td></tr><tr><td>Phân phối Poa-xông - $X \sim \mathcal{Poi}(\lambda)$</td><td>$\theta=\lambda$</td></tr><tr><td>Phân phối hình học - $X \sim \mathcal{Geo}(p)$</td><td>$\theta=p$</td></tr><tr><td>Phân phối nhị thức âm - $X \sim \mathcal{NegBin}(r,p)$</td><td>$\theta=[r,p]$</td></tr><tr><td>Phân phối chuẩn - $X \sim \mathcal{N}(\mu, \sigma^2)$</td><td>$\theta=[\mu,\sigma^2]$</td></tr><tr><td>Phân phối mũ - $X \sim \mathcal{Exp}(\beta)$</td><td>$\theta=\beta$</td></tr></tbody></table><p>Có nhiều phương pháp để ước lượng các tham số này từ tập mẫu ta có nhưng được đề cập nhiều nhất là 2 phương pháp:</p><ul><li>MLE (Maximum Likelihood Estimation): Ước lượng hợp lý cực đại</li><li>MAP (Maximum A Posteriori): Cực đại xác suất hậu nghiệm</li></ul><p>Về cái nào hơn cái nào thì không có đánh giá chính thức nên chỉ có cách là áp dụng và tự đánh giá.</p><h2 id=2-2-mle>2.2. MLE</h2><h3 id=2-2-1-khái-niệm>2.2.1. Khái niệm</h3><p>Ý tưởng của MLE là chọn tham số $\theta$ sao cho đầu ra của mô hình gần nhất với tập mẫu quan sát được. Ví dụ ta cần tìm tham số cho mô hình thống kê của bài toán xinh gái ảnh hưởng thế nào tới thông minh. Thì ta sẽ tìm tham số $\theta$ sao cho các biến các đầu vào là <em>&ldquo;xinh gái&rdquo;</em> - $X$ có kết quả gần với đầu ra <em>&ldquo;thông minh&rdquo;</em> - $Y$ nhất có thể.</p><p>Vì các mẫu quan sát được hay nói cách khác là đã xảy ra rồi nên chúng có xác suất là 1, giờ nếu ta muốn đầu ra của ta gần với chúng nhất thì xác suất của đầu ra của ta phải gần với 1 nhất. Hay nói cách khác ta phải tìm tham số sao cho xác suất đầu ra của mô hình là lớn nhất có thể.</p><p>Để mô tả xác suất đầu ra ta sử dụng một <em>hàm hợp lý</em> (<em>Likelihood function</em>) như sau:
$$L(\theta)=\prod_{i=1}^nf(X_i|\theta)$$</p><p>Bạn đang nhìn thấy nó giống với xác suất đồng thời? Yes, chính nó đấy! Tuy nhiên ta phải phân biệt ra một chút để hiểu cho đúng về <em>hàm hợp lý</em>:</p><ul><li>$X$ là rời rạc: <em>hàm hợp lý</em> là hàm trọng lượng xác suất (<em>PMF</em>)</li><li>$X$ là liên tục: <em>hàm hợp lý</em> là hàm mật độ xác suất (<em>PDF</em>)</li></ul><p>Ta biểu diễn $f(X|\theta)$ với ý nghĩa rằng ta đầu ra của ta sẽ phụ thuộc vào tham số mô hình, mỗi tham số khác nhau sẽ cho đầu ra là khác nhau. Ngoài ra, ta cần lấy xác suất đồng thời bởi ta cần phải lấy mức độ giống nhau tổng thể của toàn bộ tập mẫu.</p><p>Vấn đề của ta bây giờ là làm sao có thể tìm được tham số $\theta$ sao cho xác suất đầu ra là lớn nhất có thể, tức:
$$\hat\theta=\underset{\theta}{\mathrm{argmax}}L(\theta)$$</p><blockquote><p>$\underset{\theta}{\mathrm{argmax}}$ là hàm trả ra giá trị của tham số $\theta$ mà tại đó khiến hàm đạt được giá trị lớn nhất.</p></blockquote><p>Tuy nhiên do các $f(X_i|\theta)$ là nhỏ (có thể là bé hơn 1) nên với tập mẫu lớn $L(\theta)$ rất có thể sẽ rất nhỏ và khó khăn để xử lý sai số. Hơn nữa việc tối ưu tích là khó khăn hơn so với tối ưu tổng nên nếu ta có thể biến đổi tương đương thành tổng thì quá nhẹ nhàng. May mắn là nếu ta lấy $\log$ của nó thì tham số vẫn không thay đổi mà phép nhân của ta có thể biến thành phép cộng, nên trong thực tế ta sẽ sử dụng phiên bản $log$ của <em>hàm hợp lý</em> (<em>Log Likelihood function</em>):
$$LL(\theta)=\log L(\theta)=\log\prod_{i=1}^nf(X_i|\theta)=\sum_{i=1}^n\log f(X_i|\theta)$$</p><p>Và ta sẽ tìm $\theta$ để tối ưu hoá hàm này:
$$\hat\theta=\underset{\theta}{\mathrm{argmax}}LL(\theta)$$</p><p>Để tối ưu hoá hàm này ta có thể sử dụng nhiều phương pháp khác nhau, một trong các phương pháp phổ biến là sử dụng đạo hàm bậc nhất kết hợp chạy chương trình trên máy tính.</p><h3 id=2-2-2-ví-dụ>2.2.2. Ví dụ</h3><p>Ví dụ 1: Giả sử ta có tập dữ liệu $X=[X_1,X_2,&hellip;,X_n]$ tuân theo luật phân phối Bec-nu-li $X \sim \mathcal{Bern}(p)$ với tham số $p$. Giờ ta sẽ sử dụng phương pháp MLE để tìm tham số $p$.</p><p>Hàm hợp lý của ta lúc này sẽ có dạng:
$$L(\theta)=\prod_{i=1}^nf(X_i|\theta)=\prod_{i=1}^np^{X_i}(1-p)^{1-X_i}$$</p><p>Phiên bản $\log$:
$$
\begin{aligned}
LL(\theta)&amp;=\sum_{i=1}^n\log\Big(p^{X_i}(1-p)^{1-X_i}\Big)
\cr\ &amp;=\sum_{i=1}^n\log\Big(p^{X_i}\Big)+\log\Big((1-p)^{1-X_i}\Big)
\cr\ &amp;=\sum_{i=1}^nX_i\log(p)+(1-X_i)\log(1-p)
\end{aligned}
$$</p><p>Đặt $Y=\sum_{i=1}^nX_i$, ta có:
$$LL(\theta)=Y\log(p)+(n-Y)\log(1-p)$$</p><p>Giờ ta cần chọn $\hat p$ sao cho hàm trên đạt giá trị lớn nhất:
$$\hat p=\underset{p}{\mathrm{argmax}}\Big(Y\log(p)+(n-Y)\log(1-p)\Big)$$</p><p>Như ta đã biết hàm này đạt cực trị tại điểm có đạo hàm bằng 0, tức là:
$$
\begin{aligned}
\ &amp;LL(p)^{\prime}=0
\cr \iff &amp; Y\frac{1}{p}+(n-Y)\frac{-1}{1-p} = 0
\cr \iff &amp; p=\frac{Y}{n}
\end{aligned}
$$</p><p>Từ đây ta sẽ có điểm $\hat p=\dfrac{\sum_{i=1}^nX_i}{n}$ là giá trị ước lượng cần tìm.</p><h2 id=2-3-map>2.3. MAP</h2><h3 id=2-3-1-khái-niệm>2.3.1. Khái niệm</h3><p>Ý tưởng của MAP là chọn tham số $\theta$ sao chúng gần với tham số thực của dữ liệu nhất có thể. Như vậy không giống như việc khớp dữ liệu quan sát được của MLE thì MAP lại đi khớp với tham số thực của tập dữ liệu quan sát được rồi.</p><p>Do có mẫu quan sát được rồi nên xác suất của chúng và xác suất của các tham số thực tương ứng là đã được xác định và bằng 1. Nên, giờ vấn đề của là làm sao để cho xác suất các tham số ước lượng khi đã biết mẫu quan sát được là gần với 1 nhất có thể. Hay nói cách khác xác suất của tham số ước lượng khi biết xác suất của mẫu quan sát và tham số thực phải lớn nhất có thể.</p><p>Tương tự như MLE, ta cũng sẽ biểu diễn xác suất này như sau:
$$P(\theta)=f(\theta|X)$$</p><blockquote><p>$X$ ở đây là vec-to $X=[X_1,&hellip;,X_n]^{\intercal}$</p></blockquote><p>Giờ ta sử dụng <a href=https://dominhhai.github.io/vi/2017/10/what-is-prob/#2-4-xác-suất-hậu-nghiệm-bayes>thuyết Bayes</a> để biến đổi công thức trên một chút:
$$
\begin{aligned}
P(\theta)&amp;=f(\theta|X)
\cr\ &amp;= \dfrac{f(X|\theta)g(\theta)}{h(X)}
\cr\ &amp;= \dfrac{g(\theta)\prod_{i=1}^nf(X_i|\theta)}{h(X)}
\end{aligned}
$$</p><blockquote><p>$g, h$ ở đây cũng mang ý nghĩa tương tự như $f$. Tức, chúng là PMF nếu biến là rời rạc và PDF nếu biến là liên tục.</p></blockquote><p>Ta cũng sẽ tìm được tham số $\theta$ sao cho xác suất trên là lớn nhất, tức:
$$
\begin{aligned}
\hat\theta&amp;=\underset{\theta}{\mathrm{argmax}}P(\theta)
\cr\ &amp;=\underset{\theta}{\mathrm{argmax}}\dfrac{g(\theta)\prod_{i=1}^nf(X_i|\theta)}{h(X)}
\cr\ &amp;=\underset{\theta}{\mathrm{argmax}}\prod_{i=1}^nf(X_i|\theta)g(\theta)
\end{aligned}
$$</p><p>Ở trên ta giản lược được $h(X)$ là do xác suất của mẫu quan sát được rồi là không đổi. Giờ ta lại lấy $\log$ tương tự như MLE:
$$
\begin{aligned}
\hat\theta&amp;=\underset{\theta}{\mathrm{argmax}}\prod_{i=1}^nf(X_i|\theta)g(\theta)
\cr\ &amp;=\underset{\theta}{\mathrm{argmax}}\log\bigg(\prod_{i=1}^nf(X_i|\theta)g(\theta)\bigg)
\cr\ &amp;=\underset{\theta}{\mathrm{argmax}}\bigg(\log g(\theta) + \sum_{i=1}^n\log f(X_i|\theta)\bigg)
\end{aligned}
$$</p><p>Từ công thức trên ta thấy rằng MAP chỉ khác MLE ở chỗ thêm $\log g(\theta)$ hay còn gọi là xác suất tiền nghiệm vào hàm mục tiêu. Nói vậy thôi nhưng vấn đề lại phát sinh rồi! Ta hiện giờ đã biết được mô hình phân phối của tập mẫu $f(X)$ nhưng lại chưa biết mô hình phân phối của tham số $g(\theta)$ nên việc tính toán có vẻ bất khả thi.</p><h3 id=2-3-2-siêu-tham-số>2.3.2. Siêu tham số</h3><p>Tiếp tục với vấn đề chọn mô hình phân phối nào cho các tham số $\theta$. Trong thực tế người ta chọn mô hình của tham số $\theta$ sao cho cùng dạng với mô hình của tham số $\theta$ có điều kiện $X$. Hiểu theo nghĩa của Bayes là mô hình thống kê xác suất tiền nghiệm và hậu nghiệm là cùng họ với nhau. Mô hình thống kê kiểu này được gọi là <strong>xác suất tiền nghiệm liên hợp</strong> (<a href=https://en.wikipedia.org/wiki/Conjugate_prior target=_blank _ rel="noopener noreferrer"><em>conjugate prior</em></a>) của hàm khả năng (<em>likelihood function</em>).</p><p>Ví dụ, nếu mẫu của ta tuân theo phân phối Béc-nu-li $X \sim \mathcal{Bern}(p)$ với tham số $\theta=p$ thì phân phối xác suất tiền nghiệm liên hợp của nó là phân phối Beta $\theta \sim \mathcal{Beta}(\alpha,\beta)$ bởi khi kết hợp với Beta thì xác suất hậu nghiệm của nó cũng sẽ là Beta. Ở đây tôi không đề cập sâu về dạng mô hình này nhưng bạn nên đọc thêm để hiểu về nó cũng nhưng các ứng dụng của nó.</p><p>Do các phân phối của tham số cũng được quy định bởi các tham số của phân phối tương ứng. Tức là các tham số $\theta$ cần tìm của ta giờ phải phụ thuộc vào cả các tham số của phân phối xác suất của nó. Để phân biệt người ta gọi các tham số này là <strong>siêu tham số</strong> (<em>hyperparameters</em>).</p><p>Khi làm việc các siêu tham số này được thiết lập <em>dựa vào cảm quan</em> của người giải quyết bài toán. Việc chọn được siêu tham số hợp lý là một việc vô cùng cần thiết để thu được tham số $\theta$ tốt. Chính vì vậy mà các bài toán học máy sau này, ta thường xuyên phải xem xét nhiều bộ siêu tham số để được kết quả mong đợi là thế.</p><p>Để thuận tiện khi làm việc, ta liệt kê một số xác suất tiền nghiệm liên hợp như dưới đây:</p><table><thead><tr><th>Tham số</th><th>Liên hợp</th></tr></thead><tbody><tr><td>Béc-nu-li - $X \sim \mathcal{Bern}(p)$</td><td>Beta - $\theta \sim \mathcal{Beta}(\alpha,\beta)$</td></tr><tr><td>Nhị thức - $X \sim \mathcal{Bin}(n,p)$</td><td>Beta - $\theta \sim \mathcal{Beta}(\alpha,\beta)$</td></tr><tr><td>Poa-xông - $X \sim \mathcal{Poi}(\lambda)$</td><td>Gamma - $\theta \sim \mathcal{Gama}(\alpha,\beta)$</td></tr><tr><td>Phân phối hình học - $X \sim \mathcal{Geo}(p)$</td><td>Beta - $\theta \sim \mathcal{Beta}(\alpha,\beta)$</td></tr><tr><td>Phân phối nhị thức âm - $X \sim \mathcal{NegBin}(r,p)$</td><td>Beta - $\theta \sim \mathcal{Beta}(\alpha,\beta)$</td></tr><tr><td>Phân phối chuẩn - $X \sim \mathcal{N}(\mu,\_)$</td><td>Phân phối chuẩn - $\theta \sim \mathcal{N}(\mu_0,\sigma_0^2)$</td></tr><tr><td>Phân phối chuẩn - $X \sim \mathcal{N}(\_,\sigma^2)$</td><td>Gamma đảo - $\theta \sim \mathcal{InvGama}(\alpha,\beta)$</td></tr><tr><td>Phân phối mũ - $X \sim \mathcal{Exp}(\beta)$</td><td>Gamma - $\theta \sim \mathcal{Gama}(\alpha,\beta)$</td></tr></tbody></table><p>Tới đây thì ta có thể sử dụng các phương pháp tối ưu hàm mục tiêu hệt như với MLE chỉ khác là ta thêm các siêu tham số vào khi tính toán. Các siêu tham số này sẽ được thiết lập từ trước dựa vào cảm quan của người quan sát.</p><h3 id=2-3-3-ví-dụ>2.3.3. Ví dụ</h3><p>Ta xét lại ví dụ tìm $p$ của $X \sim \mathcal{Bern}(p)$ ở trên bằng phương pháp MAP. Vì đây là phân phối Bec-nu-li nên ta chọn phân phối Beta làm phân phối xác suất tiền nghiệm liên hợp $\theta \sim \mathcal{Beta}(\alpha,\beta)$ cho tham số $\theta=p$.</p><p>Như vậy ước lượng $\hat p$ cần tìm là:
$$
\begin{aligned}
\hat p&amp;=\underset{\theta}{\mathrm{argmax}}\bigg(\log Beta(p)+\sum_{i=1}^n\log Bern(X_i|p)\bigg)
\cr\ &amp;=\underset{\theta}{\mathrm{argmax}}\bigg(\log\Big(\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}p^{\alpha-1}(1-p)^{\beta-1}\Big)+Y\log(p)+(n-Y)\log(1-p)\bigg)
\end{aligned}
$$</p><p>Bỏ đi những thành phần hằng số (không phụ thuộc vào $p$) ta sẽ có:
$$
\begin{aligned}
\hat p&amp;=\underset{\theta}{\mathrm{argmax}}\bigg(\log\Big(p^{\alpha-1}(1-p)^{\beta-1}\Big)+Y\log(p)+(n-Y)\log(1-p)\bigg)
\cr\ &amp;=\underset{\theta}{\mathrm{argmax}}\bigg((\alpha-1)log(p)+(\beta-1)\log(1-p)+ Y\log(p)+(n-Y)\log(1-p)\bigg)
\cr\ &amp;=\underset{\theta}{\mathrm{argmax}}\bigg((\alpha-1+Y)log(p)+(\beta-1+n-Y)\log(1-p)\bigg)
\end{aligned}
$$</p><p>Giải bằng cách lấy đạo hàm tương tự như trên ta sẽ được:
$$\hat p=\frac{\alpha-1+\sum_{i=1}^nX_i}{n+\alpha+\beta-2}$$</p><p>Các siêu tham số $\alpha,\beta$ lúc này sẽ được chọn từ trước. Tuỳ vào giá trị của siêu tham số mà $\hat p$ thể hiện khác nhau dẫn tới kết quả của mô hình là khác nhau. Việc chọn $\alpha,\beta$ thế nào ta sẽ cùng xem xét sau trong các bài về học máy.</p><h1 id=3-kết-luận>3. Kết luận</h1><p>Chọn mẫu là một quá trình rất quan trọng để tìm ra quan hệ giữa các sự kiện và tính chất của dữ liệu. Trong thực tế ta thường chỉ làm việc với các mẫu ngẫu nhiên tức là các mẫu độc lập đôi một và cùng phân phối (<em>I.I.D</em>) rồi đi tìm tham số của các mô hình thống kê với mẫu ngẫu nhiên.</p><p>Việc tìm tham số hay còn gọi là quá trình học tham số là ý tưởng chính của các bài toán học máy nhằm tìm được mối tương quan giữa các đầu vào và đầu ra dựa trên tập dữ liệu huấn luyện. Có 2 phương pháp chính để tìm tham số là <strong>MLE</strong> (Maximum Likelihood Estimation) và <strong>MAP</strong> (Maximum A Posteriori). Ý tưởng của MLE là tìm tham số sao cho đầu ra của mô hình là khớp với tập dữ liệu quan sát được nhất có thể, còn của MAP là tìm tham số sao cho gần với tham số thực tế của tập dữ liệu nhất có thể.</p><p>Cả 2 phương pháp này đều lấy hàm mục tiêu phiên bản $\log$ (<em>Log Likehood function</em>) để cực đại hoá. Việc tìm tham số để tối ưu hàm này có thể thực hiện bằng các phương pháp của giải thích như đạo hàm cấp 1 kết hợp với sức mạnh tính toán của máy tính để giải quyết.</p><p>Từ các tham số và mô hình đó, ta có thể khai phá và dự đoán được các sự kiện trong tương lai. Nghe vẫn hơi mông lung phải không? Không sao cả, bài kế tiếp ta sẽ cùng bàn về cách áp dụng các kiến thức xác suất cho bài toán học máy ra sao.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/prob-4-ml/ data-tooltip="[Xác Suất] Đằng sau giải thuật phân loại"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/prob-com-var/ data-tooltip="[Xác Suất] Một số phân phối phổ biến"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fab fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fab fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2018 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/prob-4-ml/ data-tooltip="[Xác Suất] Đằng sau giải thuật phân loại"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/prob-com-var/ data-tooltip="[Xác Suất] Một số phân phối phổ biến"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fsampling-parameters-estimation%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fsampling-parameters-estimation%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fsampling-parameters-estimation%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/10\/sampling-parameters-estimation\/';this.page.identifier='\/vi\/2017\/10\/sampling-parameters-estimation\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>