<!doctype html><html lang=vi><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.26 with theme Tranquilpeak 0.4.1-BETA"><title>[RNN] Cài đặt GRU/LSTM</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Mạng RNN,Học Sâu,Deep Learning,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/><meta name=description content="Bài giới thiệu RNN cuối cùng này được dịch lại từ trang blog WILDML.


Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi Sepp Hochreiter và Jürgen Schmidhuber.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM."><meta property=og:type content=website><meta property=og:title content="[RNN] Cài đặt GRU/LSTM"><meta property=og:url content=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/><meta property=og:description content="Bài giới thiệu RNN cuối cùng này được dịch lại từ trang blog WILDML.


Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi Sepp Hochreiter và Jürgen Schmidhuber.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="[RNN] Cài đặt GRU/LSTM"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/><meta name=twitter:description content="Bài giới thiệu RNN cuối cùng này được dịch lại từ trang blog WILDML.


Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi Sepp Hochreiter và Jürgen Schmidhuber.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM."><meta name=twitter:creator content=@minhhai3b><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-fpbzgxsy0kgmdvyrj5ykkg6ratccrk3gocmaqn4xpcjywmv5dteilzucro4f.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.css integrity=sha384-8QOKbPtTFvh/lMY0qPVbXj9hDh+v8US0pD//FcoYFst2lCIf0BmT58+Heqj0IGyx><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-105333519-1','auto');ga('send','pageview');</script><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/#about><i class="sidebar-button-icon fa fa-lg fa-address-card"></i><span class=sidebar-button-desc>Thông tin</span></a><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[RNN] Cài đặt GRU/LSTM</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-10-23T00:00:00Z>23 tháng 10, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-s%c3%a2u>Học Sâu</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/rnn>RNN</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><blockquote><p>Bài giới thiệu RNN cuối cùng này được dịch lại từ trang <a href=http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/ target=_blank rel="noopener noreferrer">blog WILDML</a>.</blockquote><p>Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi <a href=http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf target=_blank rel="noopener noreferrer">Sepp Hochreiter và Jürgen Schmidhuber</a>.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM.<p>Đây là bài cuối trong chuỗi bài giới thiệu về RNN:<ul><li>1. <a href=https://dominhhai.github.io/vi/2017/10/what-is-rnn/>Giới thiệu RNN</a><li>2. <a href=https://dominhhai.github.io/vi/2017/10/implement-rnn-with-python/>Cài đặt RNN với Python và Theano</a><li>3. <a href=https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/>Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm</a><li>4. Cài đặt GRU/LSTM (bài này)</ul><h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-mạng-lstm>1. Mạng LSTM</a><li><a href=#2-mạng-gru>2. Mạng GRU</a><li><a href=#3-gru-vs-lstm>3. GRU vs LSTM</a><li><a href=#4-cài-đặt>4. Cài đặt</a><ul><li><a href=#4-1-cập-nhập-tham-số-với-rmsprop>4.1. Cập nhập tham số với rmsprop</a><li><a href=#4-2-thêm-một-tầng-nhúng>4.2. Thêm một tầng nhúng</a><li><a href=#4-3-thêm-tầng-gru-thứ-2>4.3. Thêm tầng GRU thứ 2</a><li><a href=#4-4-hiệu-năng>4.4. Hiệu năng</a></ul><li><a href=#5-kết-quả>5. Kết quả</a></ul></nav><h1 id=1-mạng-lstm>1. Mạng LSTM</h1><p>LSTM được thiết kế nhằm tránh cho đạo hàm bị triệt tiêu như đã mô tả <a href=https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/#2-vấn-đề-mất-mát-đạo-hàm target=_blank rel="noopener noreferrer">trong phần 3</a> của chuỗi bài viết.
Về cơ bản, LSMT có kiến trúc như mạng RNN thuần nhưng khác nhau ở cách tính toán các trạng thái ẩn ($ \circ $ là kí hiệu của phép nhân poitwise - hay còn gọi là phép nhân Hadamard):<p>$$
\begin{aligned}
i &amp;= \sigma(x_t U^i + s_{t-1} W^i) \cr
f &amp;= \sigma(x_t U^f + s_{t-1} W^f) \cr
o &amp;= \sigma(x_t U^o + s_{t-1} W^o) \cr
g &amp;= \tanh(x_t U^g + s_{t-1} W^g) \cr
c_t &amp;= {c_{t-1} \circ f} + {g \circ i} \cr
s_t &amp;= \tanh(c_t) \circ o
\end{aligned}
$$<p>Những công thức trên nhìn khá phức tạp, nhưng chúng thực sự không khó.
Với mạng RNN thuần, các trạng thái ẩn được tính toán dựa vào $ s_t = \tanh(U x_t + W s_{t-1}) $
với $ s_t $ là trạng thái ẩn mới, $ s_{t-1} $ là trạng thái ẩn phía trước và $ x_t $ là đầu vào của bước đó. Như vậy, đầu vào và đầu ra của LSTM cũng không khác gì so với RNN thuần, chúng chỉ khác cách tính toán mà thôi.
Chính cách tính toán đặc biệt này giúp cho LSTM tránh được tình trạng đạo hàm bị triệt tiêu ở các bước phụ thuộc xa.<div class="figure center"><a class=fancybox href=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/gru-lstm.png data-fancybox-group><img class=fig-img src=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/gru-lstm.png></a></div><p>Chi tiết về cách LSTM tránh được chuyện đó bạn có thể đọc bài viết của anh Chirs Olah tại <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/ target=_blank rel="noopener noreferrer">đây</a> (bản dịch tại <a href=https://dominhhai.github.io/vi/2017/10/what-is-lstm/ target=_blank rel="noopener noreferrer">đây</a>).
Về cơ bản ta có thể tóm tắt LSTM như sau:<ul><li>$ i, f, o $ lần lượt được gọi là cổng vào, cổng quên và cổng ra.
Từ công thức ở trên, ta có thể thấy giống hệt nhau và chỉ khác nhau ở tham số ma trận.
Chúng được gọi là cổng bởi nó dùng để lọc thông tin đi qua đó.
Với đặc điểm của hàm sigmoid nằm trong khoảng $ [0, 1] $ khi nhân với một véc-tơ thì ta có thể quyết định được có bao nhiêu thông tin được giữ lại.
Ví dụ, với $ 0 $ thì phép nhân sẽ làm triệt tiêu véc-tơ tương đương với việc không có thông tin nào đi qua cổng được.
Còn với $ 1 $ thì phép nhân không làm thay đổi véc-tơ đi qua, nên ta nói rằng toàn bộ thông tin qua nó được được bảo đảm.
Cổng vào giúp ta chỉ định được bao nhiêu thông tin của đầu vào sẽ ảnh hưởng tới trạng thái mới.
Cổng quên thì giúp ta bỏ đi bao nhiêu lượng thông tin ở trạng thái trước đó.
Còn cổng ra sẽ điều chỉnh lượng thông tin trạng thái trong có thể ra ngoài và truyền tới các nút mạng tiếp theo.
Ở đây, toàn bộ các cổng có cùng một kích cỡ và bằng số lượng trại thái ẩn của bạn: $ d_s $.<li>$ g $ là trạng thái ẩn ứng cử được tính toán dựa trên đầu vào hiện tại và trạng thái trước.
Công thức tính của nó không khác gì so với RNN thuần (ta chỉ đổi tên ở công thức trên: $ U = U_g $ và $ W = W_g $).
Tuy nhiên, thay vì lấy giá trị đó làm trạng thái đầu ra như RNN thuần thì ta sẽ lọc thông tin của nó bằng cổng vào trước khi đưa nó làm trạng thái ẩn mới.<li>$ c_t $ là bộ nhớ trong của LSTM. Nhìn vào công thức trên ta có thể thấy rằng nó là tổng của bộ nhớ trước đã được lọc bởi cổng quên và trạng thái ẩn ứng cử được lọc bởi cổng vào.
Nói nôm na là nó là sự kết hợp của bộ nhớ trước và đầu vào hiện tại.<li>Sau khi có được $ c_t $ rồi, ta sẽ đưa nó qua cổng ra để lọc thông tin một lần nữa để có được trạng thái mới $ s_t $.</ul><div class="figure center"><a class=fancybox href=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.00.55-AM.png title="LSTM Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)" data-fancybox-group><img class=fig-img src=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.00.55-AM.png alt="LSTM Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)"></a>
<span class=caption>LSTM Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)</span></div><p>RNN thuần có thể coi là một trường hợp đặc biệt của LSTM.
Ở sơ đồ trên, nếu ta để giá trị đầu ra của cổng vào luôn là 1 và đầu ra của cổng quên luôn là 0 (không nhớ trạng thái trước), thì ta sẽ được mô hình RNN thuần.
Cơ chế cổng của LSTM chính là chìa khóa giúp cho nó không bị mất mát đạo hàm, hay nói cách khác là có thể học được cả phụ thuộc xa.<p>Lưu ý rằng, mô hình LSTM ở trên chỉ là kiến trúc cơ bản của LSTM mà thôi.
Trong thực tế có nhiều kiến trúc LSTM đã được xây dựng để giải quyết từng vấn đề cụ thể.
Nếu bạn cần tìm hiểu sự khác nhau của chúng thì có thể đọc <a href=http://arxiv.org/pdf/1503.04069.pdf target=_blank rel="noopener noreferrer">bài này của Odyssey</a>.
Một kiến trúc phổ biến của LSTM là sử dụng các kết nối <em>peephole</em> nhằm giúp các cổng có thể sử dụng được cả trạng thái trong $ c_{t-1} $ để đưa ra phán đoán hợp lý hơn.<h1 id=2-mạng-gru>2. Mạng GRU</h1><p>Ý tưởng của GRU cũng khá giống với LSTM:<p>$$
\begin{aligned}
z &amp;= \sigma(x_t U^z + s_{t-1} W^z) \cr
r &amp;= \sigma(x_t U^r + s_{t-1} W^r) \cr
h &amp;= \tanh(x_t U^h + (s_{t-1} \circ r) W^h) \cr
s_t &amp;= {(1 - z) \circ h} + {z \circ s_{t-1}}
\end{aligned}
$$<p>GRU chỉ có 2 cổng: cổng thiết lập lại $ r $ và cổng cập nhập $ z $.
Cổng thiết lập lại sẽ quyết định cách kết hợp giữa đầu vào hiện tại với bộ nhớ trước,
còn cổng cập nhập sẽ chỉ định có bao nhiêu thông tin về bộ nhớ trước nên giữa lại.
Như vậy RNN thuần cũng là một dạng đặc biệt của GRU, với đầu ra của cổng thiết lập lại là 1 và cổng cập nhập là 0.
Cùng chung ý tưởng sử dụng cơ chế cổng điều chỉnh thông tin, nhưng chúng khác nhau ở mấy điểm sau:<ul><li>GRU có 2 cổng, còn LSTM có tới 3 cổng.<li>GRU không có bộ nhớ trong $ c_t $ và không có cổng ra như LSTM.<li>2 cổng vào và cổng quên được kết hợp lại thành cổng cập nhập $ z $ và cổng thiết lập lại $ r $ sẽ được áp dụng trực tiếp cho trạng thái ẩn trước.<li>GRU không sử dụng một hàm phi tuyến tính để tính đầu ra như LSTM.</ul><div class="figure center"><a class=fancybox href=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.36.51-AM.png title="GRU Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)" data-fancybox-group><img class=fig-img src=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.36.51-AM.png alt="GRU Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)"></a>
<span class=caption>GRU Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)</span></div><h1 id=3-gru-vs-lstm>3. GRU vs LSTM</h1><p>Cả 2 kiến trúc này đều có thể giải quyết được vấn đề mất mát đạo hàm, nhưng cái nào ngon hơn cái nào?
GRU còn khá trẻ tuổi (2014) so với ông chú LSTM của mình (1997) và tiềm năng của nó vẫn chưa được khám phá hết.
Tuy nhiên thông qua một số đánh giá thì không cái nào thực sự là ăn được hẳn cái nào.
Nhiều bài toán, việc điều chỉnh các siêu tham số (hyperparameters) như số tầng chẳng hạn lại có ý nghĩa hơn là việc chọn kiến trúc LSTM hay GRU.
Nhưng cũng có những bài toán mà GRU được chọn bởi nó nhanh hơn hoặc cần ít dữ liệu hơn do GRU ít tham số hơn.
Cũng có những lúc nếu bạn có đủ dữ liệu thì LSTM lại tỏ ra mạnh mẽ hơn và đạt được kết quả tốt hơn.
Để tìm hiểu thêm về một số đánh giá so sánh giữa 2 mô hình này, bạn có thể tham khảo tại <a href=http://arxiv.org/abs/1412.3555 target=_blank rel="noopener noreferrer">đây</a> và cả <a href=http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf target=_blank rel="noopener noreferrer">đây</a> nữa.<h1 id=4-cài-đặt>4. Cài đặt</h1><p>Ta sẽ dựa vào đoạn mã bữa trước ta đã xây dựng với Theano để cài đặt LSTM/GRU.
Lô-gic chương trình sẽ không thay đổi vì LSTM hay GRU chỉ đơn giản là thay đổi cách tính trạng thái ẩn mà thôi.
Nên ta chỉ cần thay đổi đoạn mã tính toán đó dựa và các công thức phía trên là được.
Đoạn mã bên dưới đây sẽ chỉ mô ta việc tính toán đó, còn toàn bộ mã nguồn đầy đủ các bạn có thể xem trên <a href=https://github.com/dennybritz/rnn-tutorial-gru-lstm target=_blank rel="noopener noreferrer">Github</a>.<figure class="highlight python"><figcaption><span>gru.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br></pre><td class=code><pre class="python code-highlight">
def forward_prop_step(x_t, s_t1_prev):
      # This is how we calculated the hidden state in a simple RNN. No longer!
      # s_t = T.tanh(U[:,x_t] &#43; W.dot(s_t1_prev))

      # Get the word vector
      x_e = E[:,x_t]

      # GRU Layer
      z_t1 = T.nnet.hard_sigmoid(U[0].dot(x_e) &#43; W[0].dot(s_t1_prev) &#43; b[0])
      r_t1 = T.nnet.hard_sigmoid(U[1].dot(x_e) &#43; W[1].dot(s_t1_prev) &#43; b[1])
      c_t1 = T.tanh(U[2].dot(x_e) &#43; W[2].dot(s_t1_prev * r_t1) &#43; b[2])
      s_t1 = (T.ones_like(z_t1) - z_t1) * c_t1 &#43; z_t1 * s_t1_prev

      # Final output calculation
      # Theano&#39;s softmax returns a matrix with one row, we only need the row
      o_t = T.nnet.softmax(V.dot(s_t1) &#43; c)[0]

      return [o_t, s_t1]</pre></table></figure><p>Nhìn khá đơn giản phải không? Thế còn việc tính đạo hàm thì sao?
Cũng như phần trước ta có thể tính đạo hàm với <code>E</code>, <code>W</code>, <code>U</code>, <code>b</code> và <code>c</code> một cách tương tự bằng quy tắc chuỗi vi phân.
Tuy nhiên, ở đây tôi sử dụng luôn thư viện Theano để tính đạo hàm cho tiện.<figure class="highlight python"><figcaption><span>gru.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br></pre><td class=code><pre class="python code-highlight">
# Gradients using Theano
dE = T.grad(cost, E)
dU = T.grad(cost, U)
dW = T.grad(cost, W)
db = T.grad(cost, b)
dV = T.grad(cost, V)
dc = T.grad(cost, c)</pre></table></figure><p>Giờ thì chương trình của ta đã khá đẹp rồi, nhưng để đạt được kết quả tốt thì cần một số mẹo nữa.<h2 id=4-1-cập-nhập-tham-số-với-rmsprop>4.1. Cập nhập tham số với rmsprop</h2><p>Giải thuật SGD (Stochastic Gradient Descent) thường sẽ không tìm được điểm tối ưu nếu độ học (learning rate) của ta lớn và sẽ rất chậm nếu độ học nhỏ.
Để giải quyết vấn đề đó, hàng loạt các biến thể khác nhau của SGD đã được ra đời như
<a href=http://www.cs.toronto.edu/~fritz/absps/momentum.pdf target=_blank rel="noopener noreferrer">Momentum Method</a>,
<a href=http://www.magicbroom.info/Papers/DuchiHaSi10.pdf target=_blank rel="noopener noreferrer">AdaGrad</a>,
<a href=http://arxiv.org/abs/1212.5701 target=_blank rel="noopener noreferrer">AdaDelta</a>,
<a href=http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf target=_blank rel="noopener noreferrer">rmsprop</a>&hellip;
Để tìm hiểu thêm các giải thuật này khác nhau ra sao bạn có thể đọc bài <a href=http://cs231n.github.io/neural-networks-3/#update target=_blank rel="noopener noreferrer">so sánh này</a> để có một cái nhìn tổng quan về chúng.
Trong phần này tôi chọn <code>rmsprop</code> để thực hiện việc tối ưu tham số.
Ý tưởng cơ bản của giải thuật này là thay đổi độ học theo từng tham số một dựa vào tổng các đạo hàm trước.
Một cách trừu tượng, ta có thể nói rằng đối với các thuộc tính thường xảy ra hơn thì sẽ có độ học nhỏ hơn do tổng đạo hàm của chúng lớn hơn, còn các thuộc tính ít xảy ra thì sẽ có độ học lớn hơn.<p>Việc cài đặt <code>rmsprop</code> khá đơn giản. Với mỗi tham số ta tạo một biến để lưu tạm tham số và sẽ cập nhập dần tham số và biến đó trong quá trình giảm đạo hàm như sau:<figure class="highlight python"><figcaption><span>gru.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br></pre><td class=code><pre class="python code-highlight">
# for W parameter
cacheW = decay * cacheW &#43; (1 - decay) * dW ** 2
W = W - learning_rate * dW / np.sqrt(cacheW &#43; 1e-6)</pre></table></figure><p><code>decay</code> thường là 0.9 hoặc 0.95, còn 1e-6 được cộng thêm vào để tránh việc chia cho 0 khi <code>cacheW</code> bằng 0.<h2 id=4-2-thêm-một-tầng-nhúng>4.2. Thêm một tầng nhúng</h2><p>Sử dụng các từ nhúng như <a href=https://code.google.com/p/word2vec/ target=_blank rel="noopener noreferrer">word2vec</a> và <a href=http://nlp.stanford.edu/projects/glove/>GloVe</a>
là một phương pháp phổ biến để cài thiện độ chính xác của mô hình.
Thay vì sử dụng các véc-tơ one-hot để biểu diễn các từ thì ta sử dụng các véc-tơ có kích cỡ nhỏ như word2vec hay GloVe có mang ngữ nghĩa sẽ mang lại hiệu năng tốt hơn.
Sử dụng các véc-tơ này tương đương với việc ta sử dụng các đầu vào đã được <em>huấn luyện trước</em> (pre-training), nên độ chính xác có thể được cải thiện.
Một cách trừu tượng, bạn cho mạng nơ-ron biết được các từ nào là tương tự nhau có thể giúp nó hiểu được ngôn ngữ hơn và việc học sẽ được cắt giảm bớt đi.
Sử dụng các véc-tơ được huấn luyện trước này còn có lợi khi bạn có ít dữ liệu vì nó cho phép mạng có thể sinh ra được nhiều từ mà bạn chưa có trong tập dữ liệu dựa vào các từ đồng nghĩa của véc-tơ.
Ở đây tôi không thêm tầng nhúng vào, nhưng việc thêm này cũng không khó vì chỉ đơn giản là thay thế ma trận <code>E</code> trong đoạn mã của ta là xong.<h2 id=4-3-thêm-tầng-gru-thứ-2>4.3. Thêm tầng GRU thứ 2</h2><p>Thêm một tầng thứ 2 có thể giúp mô hình của ta tương tác được ở mức độ cao hơn.
Bạn có thể thêm nhiều tầng hơn nữa, nhưng chắc chắn rằng đừng để mô hình của bạn bị khớp quá (overfitting) khi dữ liệu của bạn không đủ lớn.
Ở đây tôi không có nhiều dữ liệu, nên tôi cũng chỉ muốn mô hình của mình trả ra kết quả ngay sau 2, 3 tầng mạng.<div class="figure center"><a class=fancybox href=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/gru-lstm-2-layer.png data-fancybox-group><img class=fig-img src=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/gru-lstm-2-layer.png></a></div><p>Việc tính toán ở các tầng là tương tự nhau, nên ta chỉ cần thêm đoạn mã tính cho tầng vừa thêm là được.<figure class="highlight python"><figcaption><span>gru.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre class="python code-highlight">
# GRU Layer 1
z_t1 = T.nnet.hard_sigmoid(U[0].dot(x_e) &#43; W[0].dot(s_t1_prev) &#43; b[0])
r_t1 = T.nnet.hard_sigmoid(U[1].dot(x_e) &#43; W[1].dot(s_t1_prev) &#43; b[1])
c_t1 = T.tanh(U[2].dot(x_e) &#43; W[2].dot(s_t1_prev * r_t1) &#43; b[2])
s_t1 = (T.ones_like(z_t1) - z_t1) * c_t1 &#43; z_t1 * s_t1_prev

# GRU Layer 2
z_t2 = T.nnet.hard_sigmoid(U[3].dot(s_t1) &#43; W[3].dot(s_t2_prev) &#43; b[3])
r_t2 = T.nnet.hard_sigmoid(U[4].dot(s_t1) &#43; W[4].dot(s_t2_prev) &#43; b[4])
c_t2 = T.tanh(U[5].dot(s_t1) &#43; W[5].dot(s_t2_prev * r_t2) &#43; b[5])
s_t2 = (T.ones_like(z_t2) - z_t2) * c_t2 &#43; z_t2 * s_t2_prev</pre></table></figure><p>Mã đầy đủ tôi có để trên <a href=https://github.com/dennybritz/rnn-tutorial-gru-lstm/blob/master/gru_theano.py target=_blank rel="noopener noreferrer">Github</a>, nếu hứng thú các bạn có thể tham khảo trên đó.<h2 id=4-4-hiệu-năng>4.4. Hiệu năng</h2><p>Đoạn mã tôi xây dựng ở đây chỉ dành cho mục đích học tập, không phải dành cho phát triển sản phẩm, bởi vậy hiệu năng thực sự là không tốt.
Để hoàn thiện hơn thì ta cần <a href=http://svail.github.io/ target=_blank rel="noopener noreferrer">nhiều mẹo khác</a> để tối ưu hiệu năng của RNN,
nhưng có lẽ quan trọng nhất là cập nhập cùng lúc nhiều tham số.
Thay vì học từng câu một, ta có thể nhóm các câu có cùng độ dài với nhau (thậm chí có thể thêm các kí tự vào để được các câu có cùng độ dài),
sau đó thực hiện phép nhân ma trận và cộng tổng đạo hàm lại cùng lúc.
Vì thực hiện phép nhân một ma trận cỡ lớn có thể thực hiện rất hiệu quả với GPU,
chứ không cần phải chia nhỏ ra để xử lý sẽ rất chậm.<p>Ngoài ra, bạn nên sử dụng các <a href=http://www.teglor.com/b/deep-learning-libraries-language-cm569/ target=_blank rel="noopener noreferrer">thư viện học sâu</a> có sẵn để thực hiện.
Do các thư viện này đã được tối ưu hóa để đạt được hiệu năng tốt rồi, nên bạn hoàn toàn có thể an tâm sử dụng và tập trung vào nghiệp vụ của chương trình.
Nhiều mô hình nếu tự xây dựng có thể mất vài ngày tới vài tuần để huấn luyện, nhưng chỉ mất vài giờ huấn luyện nếu sử dụng các thư viện có sẵn.
Như vậy thì dại gì mà ta lại đi xây dựng lại nữa.
Tôi thì thích <a href=http://keras.io/ target=_blank rel="noopener noreferrer">Keras</a> hơn cả do nó khá dễ sử dụng và có nhiều ví dụ dễ hiểu cho RNN.<h1 id=5-kết-quả>5. Kết quả</h1><p>Tôi có luyện sẵn một mô hình với lượng từ vựng là 8000, chuỗi véc-tơ có kích cỡ là 48 và 128 tầng GRU.
Cách sài nó tôi cũng đã viết đầy đủ để các bạn tiện sử dụng trên <a href=https://github.com/dennybritz/rnn-tutorial-gru-lstm target=_blank rel="noopener noreferrer">Github</a>,
các bạn có thể tải về và chạy xem sao nhé.<p>Dưới đây là một số kết quả mà tôi chọn lọc ra sau khi chạy chương trình:<ul><li><em>&ldquo;I am a bot , and this action was performed automatically .&rdquo;</em><li><em>&ldquo;I enforce myself ridiculously well enough to just youtube.&rdquo;</em><li><em>&ldquo;I’ve got a good rhythm going !&rdquo;</em><li><em>&ldquo;There is no problem here, but at least still wave !&rdquo;</em><li><em>&ldquo;It depends on how plausible my judgement is .&rdquo;</em><li><em>&rdquo;( with the constitution which makes it impossible )&rdquo;</em></ul><p>Trông khá ngon vì ngữ nghĩa có vẻ ổn hơn lần trước.
Điều đó chứng tỏ mạng của ta đã có thể xử lý được các phụ thuộc xa khá tốt rồi.<p>Tới đây tôi xin dừng vài giới thiệu về RNN của mình, hi vọng là bạn đã có một cái nhìn tổng qua về mô hình mạng hồi quy và có thể áp dụng nó để làm ra nhiều sản phẩm thú vị.
Nếu bạn có thắc mắc hay góp ý gì thì đừng quên bình luận ở bên dưới nhé.</div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/rnn/>RNN</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/change-filename-bat/ data-tooltip="[Windows] Đổi tên file với .bat file"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/ data-tooltip="[RNN] Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-facebook-official"></i></a><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-twitter"></i></a><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-google-plus"></i></a><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)
<option title=English value=en-us>English (en-us)
<option title=日本語 value=ja>日本語 (ja)</select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2017 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/change-filename-bat/ data-tooltip="[Windows] Đổi tên file với .bat file"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/ data-tooltip="[RNN] Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-facebook-official"></i></a><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-twitter"></i></a><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-google-plus"></i></a><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fimplement-gru-lstm%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fimplement-gru-lstm%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fimplement-gru-lstm%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=algolia-search-modal class=modal-container><div class=modal><div class=modal-header><span class=close-button><i class="fa fa-close"></i></span><a href=https://algolia.com target=_blank rel=noopener class="searchby-algolia text-color-light link-unstyled"><span class="searchby-algolia-text text-color-light text-small">by</span>
<img class=searchby-algolia-logo src=https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg></a>
<i class="search-icon fa fa-search"></i><form id=algolia-search-form><input id=algolia-search-input name=search class="form-control input--large search-input" placeholder="Tìm kiếm"></form></div><div class=modal-body><div class="no-result text-color-light text-center">không tìm thấy kết quả</div><div class=results><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/vbnet-oracle-version/><h3 class=media-heading>[.NET] Sài nhiều phiên bản Oracle khi thực thi</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Thông thường khi ta build ứng dụng thì phiên bản Oracle DB ở môi trường phát triển và môi trường thực thi là giống nhau nên không xảy ra vấn đề gì cả. Nhưng nếu ở môi trường phát triển và thực thi khác nhau thì sao?</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/what-is-http2/><h3 class=media-heading>[Web] HTTP2 là gì?</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Nhân tiện bản <code>Node v9x</code> mới ra cho phép ta có thể sử dụng ngay API thử nghiệm <code>HTTP/2</code> nên cũng tò mò tìm hiểu đôi chút xem kiến trúc, đặc điểm và cách sử dụng thế nào.
Sau 2 năm ra chính thức ra lò, phiên bản tiếp theo của <code>HTTP</code> này dần được nhiều máy chủ Web lẫn trình duyệt hỗ trợ bởi tính vượt trội của nó so với phiên bản <code>HTTP/1.1</code>.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/about-git/><h3 class=media-heading>[Git] Mô tả về GIT của Linus Torvalds</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Đây là mô tả về GIT mà chủ nhân của nó - ông Linus Torvalds đã viết khi công khai mã nguồn. Cụ thể bài này được copy lại từ <a href=https://github.com/git/git/tree/e83c5163316f89bfbde7d9ab23ca2e25604af290 target=_blank rel="noopener noreferrer">Github</a>.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/change-filename-bat/><h3 class=media-heading>[Windows] Đổi tên file với .bat file</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather">Gần đây Gmail không cho phép gửi các file có đuôi là mã nguồn ngôn ngữ lập trình như .js, .vb chẳng hạn. Ngay cả việc đổi đuôi của các file nén cũng không có hiệu quả như trước, nên buộc phải tìm cách đổi toàn bộ đuôi 1 phát.
Bài viết này sẽ nói về cách thay đổi toàn bộ đuôi file bằng .bat file của Windows, tuy nhiên hoàn toàn có thể sử dụng để làm những chuyện khác với các file này như đổi tên chẳng hạn.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/><h3 class=media-heading>[RNN] Cài đặt GRU/LSTM</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather"><blockquote><p>Bài giới thiệu RNN cuối cùng này được dịch lại từ trang <a href=http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/ target=_blank rel="noopener noreferrer">blog WILDML</a>.</blockquote><p>Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi <a href=http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf target=_blank rel="noopener noreferrer">Sepp Hochreiter và Jürgen Schmidhuber</a>.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/><h3 class=media-heading>[RNN] Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather"><blockquote><p>Bài giới thiệu RNN thứ 3 này được dịch lại từ trang <a href=http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/ target=_blank rel="noopener noreferrer">blog WILDML</a>.</blockquote><p>Trong phần này tôi sẽ giới thiệu tổng quan về BPTT (Backpropagation Through Time) và giải thích sự khác biệt của nó so với các giải thuật lan truyền ngược truyền thống.
Sau đó ta sẽ cùng tìm hiểu vấn đề mất mát đạo hàm (vanishing gradient problem), nó dẫn ta tới việc phát triển của LSTM và GRU - 2 mô hình phổ biến và mạnh mẽ nhất hiện nay trong các bài toán NLP (và cả các lĩnh vực khác).</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/implement-rnn-with-python/><h3 class=media-heading>[RNN] Cài đặt RNN với Python và Theano</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather"><blockquote><p>Bài giới thiệu RNN thứ 2 này được dịch lại từ trang <a href=http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/ target=_blank rel="noopener noreferrer">blog WILDML</a>.</blockquote><p>Trong phần này chúng ta sẽ cài đặt một mạng nơ-ron hồi quy từ đầu sử dụng Python
và tối ưu với <a href=http://deeplearning.net/software/theano/ target=_blank rel="noopener noreferrer">Theano</a> - một thư viện tính toán trên GPU.
Tôi sẽ chỉ đề cập các thành phần quan trọng để giúp bạn có thể hiểu được RNN,
còn toàn bộ mã nguồn bạn có thể xem trên <a href=https://github.com/dennybritz/rnn-tutorial-rnnlm target=_blank rel="noopener noreferrer">Github</a>.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/what-is-lstm/><h3 class=media-heading>[RNN] LSTM là gì?</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather"><blockquote><p>Bài LSTM này được dịch lại từ trang <a href=https://colah.github.io/posts/2015-08-Understanding-LSTMs/ target=_blank rel="noopener noreferrer">colah&rsquo;s blog</a>.</blockquote><p>LSTM là một mạng cải tiến của RNN nhằm giải quyết vấn đề nhớ các bước dài của RNN.
Có nhiều bài đã viết về LSTM, nhưng được đề cập tới nhiều và dễ hiểu nhất có lẽ là của anh
<a href=https://github.com/colah/ target=_blank rel="noopener noreferrer">Christopher Olah</a>.
Nên mình quyết định dịch lại cho bản thân có thể hiểu thêm và cho cả các bạn đang tìm hiểu.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/what-is-rnn/><h3 class=media-heading>[RNN] RNN là gì?</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather"><blockquote><p>Bài giới thiệu RNN này được dịch lại từ trang <a href=http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/ target=_blank rel="noopener noreferrer">blog WILDML</a>.</blockquote><p>Mạng nơ-ron hồi quy (RNN - Recurrent Neural Network) là một thuật toán được chú ý rất nhiều trong thời gian gần đây bởi các kết quả tốt thu được trong lĩnh vực xử lý ngôn ngữ tự nhiên.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/what-is-prob/><h3 class=media-heading>[Xác Suất] Khái niệm cơ bản</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Lý thuyết xác suất là công cụ cơ bản và là tiền đề cho học máy. Việc nắm được lý thuyết về xác suất là rất cần thiết để có thể dấn thân vào lĩnh vực này. Trong phần này, tôi sẽ viết lại các định nghĩa, lý thuyết cơ bản của xác suất thống kê.</div></div><div style=clear:both></div><hr></div></div></div><div class=modal-footer><p class="results-count text-medium" data-message-zero="không tìm thấy kết quả" data-message-one="tìm thấy 1 kết quả" data-message-other="tìm thấy {n} kết quả">43 posts found</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js integrity="sha256-IFHWFEbU2/+wNycDECKgjIRSirRNIDp2acEB5fvdVRU=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js integrity="sha256-+mpyNVJsNt4rVXCw0F+pAOiB3YxmHgrbJsx4ecPuUaI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js integrity="sha256-vMxgR/7FtLovVA+IPrR7+xTgIgARH7y9VZQnmmi0HDI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.js integrity="sha256-N0qFUh7/9vLvia87dDndewmsgsyYoNkdA212tPc+2NI=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-GR8SEkOO1rBN/jnOcQDFcFmwXAevSLx7/Io9Ps1rkxWp983ZIuUGfxivlF/5f5eJ src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.js></script><script crossorigin=anonymous integrity=sha384-cXpztMJlr2xFXyDSIfRWYSMVCXZ9HeGXvzyKTYrn03rsMAlOtIQVzjty5ULbaP8L src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/10\/implement-gru-lstm\/';this.page.identifier='\/vi\/2017\/10\/implement-gru-lstm\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script>