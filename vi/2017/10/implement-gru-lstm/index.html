<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.54.0 with theme Tranquilpeak 0.4.1-BETA"><title>[RNN] Cài đặt GRU/LSTM</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Mạng RNN,Học Sâu,Deep Learning,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/><meta name=description content="Bài giới thiệu RNN cuối cùng này được dịch lại từ trang blog WILDML.


Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi Sepp Hochreiter và Jürgen Schmidhuber.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM."><link rel=publisher href=https://plus.google.com/115106277658014197977><meta property=fb:app_id content=333198270561466><meta property=og:locale content=vi_VN><meta property=og:type content=article><meta property=article:author content=dominhai><meta property=og:title content="[RNN] Cài đặt GRU/LSTM"><meta property=og:url content=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/><meta property=og:description content="Bài giới thiệu RNN cuối cùng này được dịch lại từ trang blog WILDML.


Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi Sepp Hochreiter và Jürgen Schmidhuber.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM."><meta property=og:site_name content="Hai's Blog"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:creator content=@minhhai3b><meta name=twitter:card content=summary><meta name=twitter:title content="[RNN] Cài đặt GRU/LSTM"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/><meta name=twitter:description content="Bài giới thiệu RNN cuối cùng này được dịch lại từ trang blog WILDML.


Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi Sepp Hochreiter và Jürgen Schmidhuber.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM."><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css integrity=sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/talk/><i class="sidebar-button-icon fa fa-lg fa-child"></i><span class=sidebar-button-desc>Chém gió</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[RNN] Cài đặt GRU/LSTM</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-10-23T00:00:00Z>23 tháng 10, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-s%c3%a2u>Học Sâu</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/rnn>RNN</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><blockquote><p>Bài giới thiệu RNN cuối cùng này được dịch lại từ trang <a href=http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/ target=_blank _ rel="noopener noreferrer">blog WILDML</a>.</p></blockquote><p>Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi <a href=https://github.com/dzitkowskik/StockPredictionRNN/blob/master/docs/Hochreiter97_lstm.pdf target=_blank _ rel="noopener noreferrer">Sepp Hochreiter và Jürgen Schmidhuber</a>.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM.</p><p>Đây là bài cuối trong chuỗi bài giới thiệu về RNN:</p><ul><li>1. <a href=https://dominhhai.github.io/vi/2017/10/what-is-rnn/>Giới thiệu RNN</a></li><li>2. <a href=https://dominhhai.github.io/vi/2017/10/implement-rnn-with-python/>Cài đặt RNN với Python và Theano</a></li><li>3. <a href=https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/>Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm</a></li><li>4. Cài đặt GRU/LSTM (bài này)</li></ul><h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-mạng-lstm>1. Mạng LSTM</a></li><li><a href=#2-mạng-gru>2. Mạng GRU</a></li><li><a href=#3-gru-vs-lstm>3. GRU vs LSTM</a></li><li><a href=#4-cài-đặt>4. Cài đặt</a><ul><li><a href=#4-1-cập-nhập-tham-số-với-rmsprop>4.1. Cập nhập tham số với rmsprop</a></li><li><a href=#4-2-thêm-một-tầng-nhúng>4.2. Thêm một tầng nhúng</a></li><li><a href=#4-3-thêm-tầng-gru-thứ-2>4.3. Thêm tầng GRU thứ 2</a></li><li><a href=#4-4-hiệu-năng>4.4. Hiệu năng</a></li></ul></li><li><a href=#5-kết-quả>5. Kết quả</a></li></ul></nav><h1 id=1-mạng-lstm>1. Mạng LSTM</h1><p>LSTM được thiết kế nhằm tránh cho đạo hàm bị triệt tiêu như đã mô tả <a href=https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/#2-vấn-đề-mất-mát-đạo-hàm target=_blank _ rel="noopener noreferrer">trong phần 3</a> của chuỗi bài viết.
Về cơ bản, LSMT có kiến trúc như mạng RNN thuần nhưng khác nhau ở cách tính toán các trạng thái ẩn ($ \circ $ là kí hiệu của phép nhân poitwise - hay còn gọi là phép nhân Hadamard):</p><p>$$
\begin{aligned}
i &amp;= \sigma(x_t U^i + s_{t-1} W^i) \cr
f &amp;= \sigma(x_t U^f + s_{t-1} W^f) \cr
o &amp;= \sigma(x_t U^o + s_{t-1} W^o) \cr
g &amp;= \tanh(x_t U^g + s_{t-1} W^g) \cr
c_t &amp;= {c_{t-1} \circ f} + {g \circ i} \cr
s_t &amp;= \tanh(c_t) \circ o
\end{aligned}
$$</p><p>Những công thức trên nhìn khá phức tạp, nhưng chúng thực sự không khó.
Với mạng RNN thuần, các trạng thái ẩn được tính toán dựa vào $ s_t = \tanh(U x_t + W s_{t-1}) $
với $ s_t $ là trạng thái ẩn mới, $ s_{t-1} $ là trạng thái ẩn phía trước và $ x_t $ là đầu vào của bước đó. Như vậy, đầu vào và đầu ra của LSTM cũng không khác gì so với RNN thuần, chúng chỉ khác cách tính toán mà thôi.
Chính cách tính toán đặc biệt này giúp cho LSTM tránh được tình trạng đạo hàm bị triệt tiêu ở các bước phụ thuộc xa.</p><div class="figure center"><a class=fancybox href=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/gru-lstm.png data-fancybox-group><img class=fig-img src=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/gru-lstm.png></a></div><p>Chi tiết về cách LSTM tránh được chuyện đó bạn có thể đọc bài viết của anh Chirs Olah tại <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/ target=_blank _ rel="noopener noreferrer">đây</a> (bản dịch tại <a href=https://dominhhai.github.io/vi/2017/10/what-is-lstm/ target=_blank _ rel="noopener noreferrer">đây</a>).
Về cơ bản ta có thể tóm tắt LSTM như sau:</p><ul><li>$ i, f, o $ lần lượt được gọi là cổng vào, cổng quên và cổng ra.
Từ công thức ở trên, ta có thể thấy giống hệt nhau và chỉ khác nhau ở tham số ma trận.
Chúng được gọi là cổng bởi nó dùng để lọc thông tin đi qua đó.
Với đặc điểm của hàm sigmoid nằm trong khoảng $ [0, 1] $ khi nhân với một véc-tơ thì ta có thể quyết định được có bao nhiêu thông tin được giữ lại.
Ví dụ, với $ 0 $ thì phép nhân sẽ làm triệt tiêu véc-tơ tương đương với việc không có thông tin nào đi qua cổng được.
Còn với $ 1 $ thì phép nhân không làm thay đổi véc-tơ đi qua, nên ta nói rằng toàn bộ thông tin qua nó được được bảo đảm.
Cổng vào giúp ta chỉ định được bao nhiêu thông tin của đầu vào sẽ ảnh hưởng tới trạng thái mới.
Cổng quên thì giúp ta bỏ đi bao nhiêu lượng thông tin ở trạng thái trước đó.
Còn cổng ra sẽ điều chỉnh lượng thông tin trạng thái trong có thể ra ngoài và truyền tới các nút mạng tiếp theo.
Ở đây, toàn bộ các cổng có cùng một kích cỡ và bằng số lượng trại thái ẩn của bạn: $ d_s $.</li><li>$ g $ là trạng thái ẩn ứng cử được tính toán dựa trên đầu vào hiện tại và trạng thái trước.
Công thức tính của nó không khác gì so với RNN thuần (ta chỉ đổi tên ở công thức trên: $ U = U_g $ và $ W = W_g $).
Tuy nhiên, thay vì lấy giá trị đó làm trạng thái đầu ra như RNN thuần thì ta sẽ lọc thông tin của nó bằng cổng vào trước khi đưa nó làm trạng thái ẩn mới.</li><li>$ c_t $ là bộ nhớ trong của LSTM. Nhìn vào công thức trên ta có thể thấy rằng nó là tổng của bộ nhớ trước đã được lọc bởi cổng quên và trạng thái ẩn ứng cử được lọc bởi cổng vào.
Nói nôm na là nó là sự kết hợp của bộ nhớ trước và đầu vào hiện tại.</li><li>Sau khi có được $ c_t $ rồi, ta sẽ đưa nó qua cổng ra để lọc thông tin một lần nữa để có được trạng thái mới $ s_t $.</li></ul><div class="figure center"><a class=fancybox href=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.00.55-AM.png title="LSTM Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)" data-fancybox-group><img class=fig-img src=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.00.55-AM.png alt="LSTM Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)"></a>
<span class=caption>LSTM Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)</span></div><p>RNN thuần có thể coi là một trường hợp đặc biệt của LSTM.
Ở sơ đồ trên, nếu ta để giá trị đầu ra của cổng vào luôn là 1 và đầu ra của cổng quên luôn là 0 (không nhớ trạng thái trước), thì ta sẽ được mô hình RNN thuần.
Cơ chế cổng của LSTM chính là chìa khóa giúp cho nó không bị mất mát đạo hàm, hay nói cách khác là có thể học được cả phụ thuộc xa.</p><p>Lưu ý rằng, mô hình LSTM ở trên chỉ là kiến trúc cơ bản của LSTM mà thôi.
Trong thực tế có nhiều kiến trúc LSTM đã được xây dựng để giải quyết từng vấn đề cụ thể.
Nếu bạn cần tìm hiểu sự khác nhau của chúng thì có thể đọc <a href=http://arxiv.org/pdf/1503.04069.pdf target=_blank _ rel="noopener noreferrer">bài này của Odyssey</a>.
Một kiến trúc phổ biến của LSTM là sử dụng các kết nối <em>peephole</em> nhằm giúp các cổng có thể sử dụng được cả trạng thái trong $ c_{t-1} $ để đưa ra phán đoán hợp lý hơn.</p><h1 id=2-mạng-gru>2. Mạng GRU</h1><p>Ý tưởng của GRU cũng khá giống với LSTM:</p><p>$$
\begin{aligned}
z &amp;= \sigma(x_t U^z + s_{t-1} W^z) \cr
r &amp;= \sigma(x_t U^r + s_{t-1} W^r) \cr
h &amp;= \tanh(x_t U^h + (s_{t-1} \circ r) W^h) \cr
s_t &amp;= {(1 - z) \circ h} + {z \circ s_{t-1}}
\end{aligned}
$$</p><p>GRU chỉ có 2 cổng: cổng thiết lập lại $ r $ và cổng cập nhập $ z $.
Cổng thiết lập lại sẽ quyết định cách kết hợp giữa đầu vào hiện tại với bộ nhớ trước,
còn cổng cập nhập sẽ chỉ định có bao nhiêu thông tin về bộ nhớ trước nên giữa lại.
Như vậy RNN thuần cũng là một dạng đặc biệt của GRU, với đầu ra của cổng thiết lập lại là 1 và cổng cập nhập là 0.
Cùng chung ý tưởng sử dụng cơ chế cổng điều chỉnh thông tin, nhưng chúng khác nhau ở mấy điểm sau:</p><ul><li>GRU có 2 cổng, còn LSTM có tới 3 cổng.</li><li>GRU không có bộ nhớ trong $ c_t $ và không có cổng ra như LSTM.</li><li>2 cổng vào và cổng quên được kết hợp lại thành cổng cập nhập $ z $ và cổng thiết lập lại $ r $ sẽ được áp dụng trực tiếp cho trạng thái ẩn trước.</li><li>GRU không sử dụng một hàm phi tuyến tính để tính đầu ra như LSTM.</li></ul><div class="figure center"><a class=fancybox href=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.36.51-AM.png title="GRU Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)" data-fancybox-group><img class=fig-img src=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.36.51-AM.png alt="GRU Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)"></a>
<span class=caption>GRU Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)</span></div><h1 id=3-gru-vs-lstm>3. GRU vs LSTM</h1><p>Cả 2 kiến trúc này đều có thể giải quyết được vấn đề mất mát đạo hàm, nhưng cái nào ngon hơn cái nào?
GRU còn khá trẻ tuổi (2014) so với ông chú LSTM của mình (1997) và tiềm năng của nó vẫn chưa được khám phá hết.
Tuy nhiên thông qua một số đánh giá thì không cái nào thực sự là ăn được hẳn cái nào.
Nhiều bài toán, việc điều chỉnh các siêu tham số (hyperparameters) như số tầng chẳng hạn lại có ý nghĩa hơn là việc chọn kiến trúc LSTM hay GRU.
Nhưng cũng có những bài toán mà GRU được chọn bởi nó nhanh hơn hoặc cần ít dữ liệu hơn do GRU ít tham số hơn.
Cũng có những lúc nếu bạn có đủ dữ liệu thì LSTM lại tỏ ra mạnh mẽ hơn và đạt được kết quả tốt hơn.
Để tìm hiểu thêm về một số đánh giá so sánh giữa 2 mô hình này, bạn có thể tham khảo tại <a href=http://arxiv.org/abs/1412.3555 target=_blank _ rel="noopener noreferrer">đây</a> và cả <a href=http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf target=_blank _ rel="noopener noreferrer">đây</a> nữa.</p><h1 id=4-cài-đặt>4. Cài đặt</h1><p>Ta sẽ dựa vào đoạn mã bữa trước ta đã xây dựng với Theano để cài đặt LSTM/GRU.
Lô-gic chương trình sẽ không thay đổi vì LSTM hay GRU chỉ đơn giản là thay đổi cách tính trạng thái ẩn mà thôi.
Nên ta chỉ cần thay đổi đoạn mã tính toán đó dựa và các công thức phía trên là được.
Đoạn mã bên dưới đây sẽ chỉ mô ta việc tính toán đó, còn toàn bộ mã nguồn đầy đủ các bạn có thể xem trên <a href=https://github.com/dennybritz/rnn-tutorial-gru-lstm target=_blank _ rel="noopener noreferrer">Github</a>.</p><figure class="highlight python language-python"><figcaption><span>gru.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python>def forward_prop_step(x_t, s_t1_prev):
      # This is how we calculated the hidden state in a simple RNN. No longer!
      # s_t = T.tanh(U[:,x_t] &#43; W.dot(s_t1_prev))

      # Get the word vector
      x_e = E[:,x_t]

      # GRU Layer
      z_t1 = T.nnet.hard_sigmoid(U[0].dot(x_e) &#43; W[0].dot(s_t1_prev) &#43; b[0])
      r_t1 = T.nnet.hard_sigmoid(U[1].dot(x_e) &#43; W[1].dot(s_t1_prev) &#43; b[1])
      c_t1 = T.tanh(U[2].dot(x_e) &#43; W[2].dot(s_t1_prev * r_t1) &#43; b[2])
      s_t1 = (T.ones_like(z_t1) - z_t1) * c_t1 &#43; z_t1 * s_t1_prev

      # Final output calculation
      # Theano&#39;s softmax returns a matrix with one row, we only need the row
      o_t = T.nnet.softmax(V.dot(s_t1) &#43; c)[0]

      return [o_t, s_t1]</code></pre></td></tr></tbody></table></figure><p>Nhìn khá đơn giản phải không? Thế còn việc tính đạo hàm thì sao?
Cũng như phần trước ta có thể tính đạo hàm với <code>E</code>, <code>W</code>, <code>U</code>, <code>b</code> và <code>c</code> một cách tương tự bằng quy tắc chuỗi vi phân.
Tuy nhiên, ở đây tôi sử dụng luôn thư viện Theano để tính đạo hàm cho tiện.</p><figure class="highlight python language-python"><figcaption><span>gru.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python># Gradients using Theano
dE = T.grad(cost, E)
dU = T.grad(cost, U)
dW = T.grad(cost, W)
db = T.grad(cost, b)
dV = T.grad(cost, V)
dc = T.grad(cost, c)</code></pre></td></tr></tbody></table></figure><p>Giờ thì chương trình của ta đã khá đẹp rồi, nhưng để đạt được kết quả tốt thì cần một số mẹo nữa.</p><h2 id=4-1-cập-nhập-tham-số-với-rmsprop>4.1. Cập nhập tham số với rmsprop</h2><p>Giải thuật SGD (Stochastic Gradient Descent) thường sẽ không tìm được điểm tối ưu nếu độ học (learning rate) của ta lớn và sẽ rất chậm nếu độ học nhỏ.
Để giải quyết vấn đề đó, hàng loạt các biến thể khác nhau của SGD đã được ra đời như
<a href=http://www.cs.toronto.edu/~fritz/absps/momentum.pdf target=_blank _ rel="noopener noreferrer">Momentum Method</a>,
<a href=http://www.magicbroom.info/Papers/DuchiHaSi10.pdf target=_blank _ rel="noopener noreferrer">AdaGrad</a>,
<a href=http://arxiv.org/abs/1212.5701 target=_blank _ rel="noopener noreferrer">AdaDelta</a>,
<a href=http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf target=_blank _ rel="noopener noreferrer">rmsprop</a>&hellip;
Để tìm hiểu thêm các giải thuật này khác nhau ra sao bạn có thể đọc bài <a href=http://cs231n.github.io/neural-networks-3/#update target=_blank _ rel="noopener noreferrer">so sánh này</a> để có một cái nhìn tổng quan về chúng.
Trong phần này tôi chọn <code>rmsprop</code> để thực hiện việc tối ưu tham số.
Ý tưởng cơ bản của giải thuật này là thay đổi độ học theo từng tham số một dựa vào tổng các đạo hàm trước.
Một cách trừu tượng, ta có thể nói rằng đối với các thuộc tính thường xảy ra hơn thì sẽ có độ học nhỏ hơn do tổng đạo hàm của chúng lớn hơn, còn các thuộc tính ít xảy ra thì sẽ có độ học lớn hơn.</p><p>Việc cài đặt <code>rmsprop</code> khá đơn giản. Với mỗi tham số ta tạo một biến để lưu tạm tham số và sẽ cập nhập dần tham số và biến đó trong quá trình giảm đạo hàm như sau:</p><figure class="highlight python language-python"><figcaption><span>gru.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python># for W parameter
cacheW = decay * cacheW &#43; (1 - decay) * dW ** 2
W = W - learning_rate * dW / np.sqrt(cacheW &#43; 1e-6)</code></pre></td></tr></tbody></table></figure><p><code>decay</code> thường là 0.9 hoặc 0.95, còn 1e-6 được cộng thêm vào để tránh việc chia cho 0 khi <code>cacheW</code> bằng 0.</p><h2 id=4-2-thêm-một-tầng-nhúng>4.2. Thêm một tầng nhúng</h2><p>Sử dụng các từ nhúng như <a href=https://code.google.com/p/word2vec/ target=_blank _ rel="noopener noreferrer">word2vec</a> và <a href=http://nlp.stanford.edu/projects/glove/>GloVe</a>
là một phương pháp phổ biến để cài thiện độ chính xác của mô hình.
Thay vì sử dụng các véc-tơ one-hot để biểu diễn các từ thì ta sử dụng các véc-tơ có kích cỡ nhỏ như word2vec hay GloVe có mang ngữ nghĩa sẽ mang lại hiệu năng tốt hơn.
Sử dụng các véc-tơ này tương đương với việc ta sử dụng các đầu vào đã được <em>huấn luyện trước</em> (pre-training), nên độ chính xác có thể được cải thiện.
Một cách trừu tượng, bạn cho mạng nơ-ron biết được các từ nào là tương tự nhau có thể giúp nó hiểu được ngôn ngữ hơn và việc học sẽ được cắt giảm bớt đi.
Sử dụng các véc-tơ được huấn luyện trước này còn có lợi khi bạn có ít dữ liệu vì nó cho phép mạng có thể sinh ra được nhiều từ mà bạn chưa có trong tập dữ liệu dựa vào các từ đồng nghĩa của véc-tơ.
Ở đây tôi không thêm tầng nhúng vào, nhưng việc thêm này cũng không khó vì chỉ đơn giản là thay thế ma trận <code>E</code> trong đoạn mã của ta là xong.</p><h2 id=4-3-thêm-tầng-gru-thứ-2>4.3. Thêm tầng GRU thứ 2</h2><p>Thêm một tầng thứ 2 có thể giúp mô hình của ta tương tác được ở mức độ cao hơn.
Bạn có thể thêm nhiều tầng hơn nữa, nhưng chắc chắn rằng đừng để mô hình của bạn bị khớp quá (overfitting) khi dữ liệu của bạn không đủ lớn.
Ở đây tôi không có nhiều dữ liệu, nên tôi cũng chỉ muốn mô hình của mình trả ra kết quả ngay sau 2, 3 tầng mạng.</p><div class="figure center"><a class=fancybox href=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/gru-lstm-2-layer.png data-fancybox-group><img class=fig-img src=https://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/gru-lstm-2-layer.png></a></div><p>Việc tính toán ở các tầng là tương tự nhau, nên ta chỉ cần thêm đoạn mã tính cho tầng vừa thêm là được.</p><figure class="highlight python language-python"><figcaption><span>gru.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre></td><td class=code><pre class="code-highlight language-python"><code class=python># GRU Layer 1
z_t1 = T.nnet.hard_sigmoid(U[0].dot(x_e) &#43; W[0].dot(s_t1_prev) &#43; b[0])
r_t1 = T.nnet.hard_sigmoid(U[1].dot(x_e) &#43; W[1].dot(s_t1_prev) &#43; b[1])
c_t1 = T.tanh(U[2].dot(x_e) &#43; W[2].dot(s_t1_prev * r_t1) &#43; b[2])
s_t1 = (T.ones_like(z_t1) - z_t1) * c_t1 &#43; z_t1 * s_t1_prev

# GRU Layer 2
z_t2 = T.nnet.hard_sigmoid(U[3].dot(s_t1) &#43; W[3].dot(s_t2_prev) &#43; b[3])
r_t2 = T.nnet.hard_sigmoid(U[4].dot(s_t1) &#43; W[4].dot(s_t2_prev) &#43; b[4])
c_t2 = T.tanh(U[5].dot(s_t1) &#43; W[5].dot(s_t2_prev * r_t2) &#43; b[5])
s_t2 = (T.ones_like(z_t2) - z_t2) * c_t2 &#43; z_t2 * s_t2_prev</code></pre></td></tr></tbody></table></figure><p>Mã đầy đủ tôi có để trên <a href=https://github.com/dennybritz/rnn-tutorial-gru-lstm/blob/master/gru_theano.py target=_blank _ rel="noopener noreferrer">Github</a>, nếu hứng thú các bạn có thể tham khảo trên đó.</p><h2 id=4-4-hiệu-năng>4.4. Hiệu năng</h2><p>Đoạn mã tôi xây dựng ở đây chỉ dành cho mục đích học tập, không phải dành cho phát triển sản phẩm, bởi vậy hiệu năng thực sự là không tốt.
Để hoàn thiện hơn thì ta cần <a href=http://svail.github.io/ target=_blank _ rel="noopener noreferrer">nhiều mẹo khác</a> để tối ưu hiệu năng của RNN,
nhưng có lẽ quan trọng nhất là cập nhập cùng lúc nhiều tham số.
Thay vì học từng câu một, ta có thể nhóm các câu có cùng độ dài với nhau (thậm chí có thể thêm các kí tự vào để được các câu có cùng độ dài),
sau đó thực hiện phép nhân ma trận và cộng tổng đạo hàm lại cùng lúc.
Vì thực hiện phép nhân một ma trận cỡ lớn có thể thực hiện rất hiệu quả với GPU,
chứ không cần phải chia nhỏ ra để xử lý sẽ rất chậm.</p><p>Ngoài ra, bạn nên sử dụng các <a href=http://www.teglor.com/b/deep-learning-libraries-language-cm569/ target=_blank _ rel="noopener noreferrer">thư viện học sâu</a> có sẵn để thực hiện.
Do các thư viện này đã được tối ưu hóa để đạt được hiệu năng tốt rồi, nên bạn hoàn toàn có thể an tâm sử dụng và tập trung vào nghiệp vụ của chương trình.
Nhiều mô hình nếu tự xây dựng có thể mất vài ngày tới vài tuần để huấn luyện, nhưng chỉ mất vài giờ huấn luyện nếu sử dụng các thư viện có sẵn.
Như vậy thì dại gì mà ta lại đi xây dựng lại nữa.
Tôi thì thích <a href=http://keras.io/ target=_blank _ rel="noopener noreferrer">Keras</a> hơn cả do nó khá dễ sử dụng và có nhiều ví dụ dễ hiểu cho RNN.</p><h1 id=5-kết-quả>5. Kết quả</h1><p>Tôi có luyện sẵn một mô hình với lượng từ vựng là 8000, chuỗi véc-tơ có kích cỡ là 48 và 128 tầng GRU.
Cách sài nó tôi cũng đã viết đầy đủ để các bạn tiện sử dụng trên <a href=https://github.com/dennybritz/rnn-tutorial-gru-lstm target=_blank _ rel="noopener noreferrer">Github</a>,
các bạn có thể tải về và chạy xem sao nhé.</p><p>Dưới đây là một số kết quả mà tôi chọn lọc ra sau khi chạy chương trình:</p><ul><li><em>&ldquo;I am a bot , and this action was performed automatically .&rdquo;</em></li><li><em>&ldquo;I enforce myself ridiculously well enough to just youtube.&rdquo;</em></li><li><em>&ldquo;I’ve got a good rhythm going !&rdquo;</em></li><li><em>&ldquo;There is no problem here, but at least still wave !&rdquo;</em></li><li><em>&ldquo;It depends on how plausible my judgement is .&rdquo;</em></li><li><em>&rdquo;( with the constitution which makes it impossible )&rdquo;</em></li></ul><p>Trông khá ngon vì ngữ nghĩa có vẻ ổn hơn lần trước.
Điều đó chứng tỏ mạng của ta đã có thể xử lý được các phụ thuộc xa khá tốt rồi.</p><p>Tới đây tôi xin dừng vài giới thiệu về RNN của mình, hi vọng là bạn đã có một cái nhìn tổng qua về mô hình mạng hồi quy và có thể áp dụng nó để làm ra nhiều sản phẩm thú vị.
Nếu bạn có thắc mắc hay góp ý gì thì đừng quên bình luận ở bên dưới nhé.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/rnn/>RNN</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/change-filename-bat/ data-tooltip="[Windows] Đổi tên file với .bat file"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/ data-tooltip="[RNN] Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=fb-root></div><script>(function(d,s,id){if(window.location.hostname=='localhost')return;var js,fjs=d.getElementsByTagName(s)[0];if(d.getElementById(id))return;js=d.createElement(s);js.id=id;js.src='https://connect.facebook.net/vi_VN/sdk.js#xfbml=1&version=v3.1&appId=333198270561466&autoLogAppEvents=1';fjs.parentNode.insertBefore(js,fjs);}(document,'script','facebook-jssdk'));</script><div id=fb-cmt-thread class=fb-comments data-href=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/ data-width=100%></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2021 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/change-filename-bat/ data-tooltip="[Windows] Đổi tên file với .bat file"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/ data-tooltip="[RNN] Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#fb-cmt-thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fimplement-gru-lstm%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fimplement-gru-lstm%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F10%2Fimplement-gru-lstm%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js></script><script crossorigin=anonymous integrity=sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/10\/implement-gru-lstm\/';this.page.identifier='\/vi\/2017\/10\/implement-gru-lstm\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>