<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Học Máy on Hai&#39;s Blog</title><link>https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/</link><description>Recent content in Học Máy on Hai&#39;s Blog</description><generator>Hugo -- gohugo.io</generator><language>vi</language><lastBuildDate>Mon, 25 Dec 2017 08:45:04 +0900</lastBuildDate><atom:link href="https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/index.xml" rel="self" type="application/rss+xml"/><item><title>[ML] Mô hình quá khớp (Overfitting)</title><link>https://dominhhai.github.io/vi/2017/12/ml-overfitting/</link><pubDate>Mon, 25 Dec 2017 08:45:04 +0900</pubDate><guid>https://dominhhai.github.io/vi/2017/12/ml-overfitting/</guid><description>&lt;p&gt;Lỗi ước lượng tham số có thể được chia thành 2 loại là &lt;strong&gt;khớp quá&lt;/strong&gt; (&lt;em&gt;over-fitting&lt;/em&gt;) và &lt;strong&gt;chưa khớp&lt;/strong&gt; (&lt;em&gt;under-fitting&lt;/em&gt;) với tập huấn luyện. Trong bài này sẽ nói về cách theo dõi và hạn chế các lỗi này ra sao. Trọng tâm của bài này sẽ tập trung chủ yếu vào kĩ thuật &lt;strong&gt;chính quy hoá&lt;/strong&gt; (&lt;em&gt;regularization&lt;/em&gt;) để giải quyết vấn đề khớp quá của tham số.
&lt;!--toc--&gt;&lt;/p&gt;</description></item><item><title>[ML] Tối ưu hàm lỗi với Gradient Descent</title><link>https://dominhhai.github.io/vi/2017/12/ml-gd/</link><pubDate>Fri, 22 Dec 2017 08:45:04 +0900</pubDate><guid>https://dominhhai.github.io/vi/2017/12/ml-gd/</guid><description>&lt;p&gt;Mặc dù sử dụng công thức chuẩn để tìm tham số là có thể thực hiện được, nhưng với tập dữ liệu lớn nhiều chiều trong thực tế thì với máy tính lại không thể thực hiện được do các ràng buộc của bộ nhớ cũng như khả năng tính toán. Chưa kể với nhiều bài toán việc giải được đạo hàm để tìm ra công thức chuẩn là rất khó khăn. Nên trong thực tế giải thuật thay thế là &lt;strong&gt;Gradient Descent&lt;/strong&gt; thường được sử dụng.
&lt;/p&gt;</description></item><item><title>[ML] MLE của hồi quy tuyến tính</title><link>https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/</link><pubDate>Thu, 21 Dec 2017 12:28:26 +0900</pubDate><guid>https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/</guid><description>&lt;p&gt;Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (&lt;em&gt;mean squared error&lt;/em&gt;). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng &lt;a href=&#34;https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/#2-2-mle&#34;&gt;MLE (&lt;em&gt;Maximum Likelihood Esitmation&lt;/em&gt;)&lt;/a&gt; xem sao.
&lt;!--toc--&gt;&lt;/p&gt;</description></item><item><title>[ML] Hồi quy tuyến tính (Linear Regression)</title><link>https://dominhhai.github.io/vi/2017/12/ml-linear-regression/</link><pubDate>Tue, 19 Dec 2017 12:54:51 +0900</pubDate><guid>https://dominhhai.github.io/vi/2017/12/ml-linear-regression/</guid><description>&lt;p&gt;Học có giám sát (&lt;em&gt;Supervised Learning&lt;/em&gt;) được chia ra làm 2 dạng lớn là &lt;strong&gt;hồi quy&lt;/strong&gt; (&lt;em&gt;regression&lt;/em&gt;) và &lt;strong&gt;phân loại&lt;/strong&gt; (&lt;em&gt;classification&lt;/em&gt;) dựa trên tập dữ liệu mẫu - tập huấn luyện (&lt;em&gt;training data&lt;/em&gt;). Với bài đầu tiên này ta sẽ bắt đầu bằng bài toán hồi quy mà cụ thể là hồi quy tuyến tính (&lt;em&gt;linear regression&lt;/em&gt;).
&lt;/p&gt;</description></item><item><title>Pattern Recognition and Machine Learning</title><link>https://dominhhai.github.io/vi/2017/12/ml-prml/</link><pubDate>Tue, 12 Dec 2017 16:36:56 +0900</pubDate><guid>https://dominhhai.github.io/vi/2017/12/ml-prml/</guid><description>&lt;p&gt;Được coi là sách giáo khoa cho những người làm học máy, cuốn sách này viết về các giải thuật và lý thuyết xây dựng các giải thuật nhận dạng mẫu và học máy. Tuy nhiên lúc mới đọc thì thấy khá khó nhằn nên tôi đã tìm hiểu độ khó các phần đề biết đường mà đọc.
&lt;/p&gt;</description></item><item><title>[ML] Học máy là gì?</title><link>https://dominhhai.github.io/vi/2017/12/ml-intro/</link><pubDate>Fri, 08 Dec 2017 16:31:37 +0900</pubDate><guid>https://dominhhai.github.io/vi/2017/12/ml-intro/</guid><description>&lt;p&gt;Thời gian gần đây AI nổi lên mạnh mẽ xâm nhập vào rất nhiều lĩnh vực trong cuộc sống như tự động dịch thuật, nhận dạng giọng nói, điều khiển tự động, v.v. Nó giờ được coi là xu hướng công nghệ thế giới và nhiều người cho rằng đó là cuộc cách mạng công nghiệp lần thứ 4.
&lt;/p&gt;</description></item><item><title>[RNN] Cài đặt GRU/LSTM</title><link>https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/</link><pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate><guid>https://dominhhai.github.io/vi/2017/10/implement-gru-lstm/</guid><description>&lt;blockquote&gt;
&lt;p&gt;Bài giới thiệu RNN cuối cùng này được dịch lại từ trang &lt;a href=&#34;http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/&#34; target=&#34;_blank&#34;_ rel=&#34;noopener noreferrer&#34;&gt;blog WILDML&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Trong phần này ta sẽ tìm hiểu về LSTM (Long Short-Term Memory) và GRU (Gated Recurrent Units).
LSTM lần đầu được giới thiệu vào năm 1997 bởi &lt;a href=&#34;https://github.com/dzitkowskik/StockPredictionRNN/blob/master/docs/Hochreiter97_lstm.pdf&#34; target=&#34;_blank&#34;_ rel=&#34;noopener noreferrer&#34;&gt;Sepp Hochreiter và Jürgen Schmidhuber&lt;/a&gt;.
Nó giờ hiện diện trên hầu hết các mô hình có sử dụng học sâu cho NPL.
Còn GRU mới được đề xuất vào năm 2014 là một phiên bản đơn giản hơn của LSTM nhưng vẫn giữ được các tính chất của LSTM.
&lt;/p&gt;</description></item><item><title>[RNN] Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm</title><link>https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/</link><pubDate>Sun, 22 Oct 2017 00:00:00 +0000</pubDate><guid>https://dominhhai.github.io/vi/2017/10/understand-rnn-bptt/</guid><description>&lt;blockquote&gt;
&lt;p&gt;Bài giới thiệu RNN thứ 3 này được dịch lại từ trang &lt;a href=&#34;http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/&#34; target=&#34;_blank&#34;_ rel=&#34;noopener noreferrer&#34;&gt;blog WILDML&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Trong phần này tôi sẽ giới thiệu tổng quan về BPTT (Backpropagation Through Time) và giải thích sự khác biệt của nó so với các giải thuật lan truyền ngược truyền thống.
Sau đó ta sẽ cùng tìm hiểu vấn đề mất mát đạo hàm (vanishing gradient problem), nó dẫn ta tới việc phát triển của LSTM và GRU - 2 mô hình phổ biến và mạnh mẽ nhất hiện nay trong các bài toán NLP (và cả các lĩnh vực khác).
&lt;/p&gt;</description></item><item><title>[RNN] Cài đặt RNN với Python và Theano</title><link>https://dominhhai.github.io/vi/2017/10/implement-rnn-with-python/</link><pubDate>Sat, 21 Oct 2017 00:00:00 +0000</pubDate><guid>https://dominhhai.github.io/vi/2017/10/implement-rnn-with-python/</guid><description>&lt;blockquote&gt;
&lt;p&gt;Bài giới thiệu RNN thứ 2 này được dịch lại từ trang &lt;a href=&#34;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/&#34; target=&#34;_blank&#34;_ rel=&#34;noopener noreferrer&#34;&gt;blog WILDML&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Trong phần này chúng ta sẽ cài đặt một mạng nơ-ron hồi quy từ đầu sử dụng Python
và tối ưu với &lt;a href=&#34;http://deeplearning.net/software/theano/&#34; target=&#34;_blank&#34;_ rel=&#34;noopener noreferrer&#34;&gt;Theano&lt;/a&gt; - một thư viện tính toán trên GPU.
Tôi sẽ chỉ đề cập các thành phần quan trọng để giúp bạn có thể hiểu được RNN,
còn toàn bộ mã nguồn bạn có thể xem trên &lt;a href=&#34;https://github.com/dennybritz/rnn-tutorial-rnnlm&#34; target=&#34;_blank&#34;_ rel=&#34;noopener noreferrer&#34;&gt;Github&lt;/a&gt;.
&lt;/p&gt;</description></item><item><title>[RNN] LSTM là gì?</title><link>https://dominhhai.github.io/vi/2017/10/what-is-lstm/</link><pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate><guid>https://dominhhai.github.io/vi/2017/10/what-is-lstm/</guid><description>&lt;blockquote&gt;
&lt;p&gt;Bài LSTM này được dịch lại từ trang &lt;a href=&#34;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34; target=&#34;_blank&#34;_ rel=&#34;noopener noreferrer&#34;&gt;colah&amp;rsquo;s blog&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LSTM là một mạng cải tiến của RNN nhằm giải quyết vấn đề nhớ các bước dài của RNN.
Có nhiều bài đã viết về LSTM, nhưng được đề cập tới nhiều và dễ hiểu nhất có lẽ là của anh
&lt;a href=&#34;https://github.com/colah/&#34; target=&#34;_blank&#34;_ rel=&#34;noopener noreferrer&#34;&gt;Christopher Olah&lt;/a&gt;.
Nên mình quyết định dịch lại cho bản thân có thể hiểu thêm và cho cả các bạn đang tìm hiểu.
&lt;/p&gt;</description></item><item><title>[RNN] RNN là gì?</title><link>https://dominhhai.github.io/vi/2017/10/what-is-rnn/</link><pubDate>Thu, 19 Oct 2017 00:00:00 +0000</pubDate><guid>https://dominhhai.github.io/vi/2017/10/what-is-rnn/</guid><description>&lt;blockquote&gt;
&lt;p&gt;Bài giới thiệu RNN này được dịch lại từ trang &lt;a href=&#34;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/&#34; target=&#34;_blank&#34;_ rel=&#34;noopener noreferrer&#34;&gt;blog WILDML&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Mạng nơ-ron hồi quy (RNN - Recurrent Neural Network) là một thuật toán được chú ý rất nhiều trong thời gian gần đây bởi các kết quả tốt thu được trong lĩnh vực xử lý ngôn ngữ tự nhiên.
&lt;/p&gt;</description></item></channel></rss>